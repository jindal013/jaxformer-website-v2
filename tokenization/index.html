<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Tokenization at Scale | Jaxformer: Scaling Modern Transformers </title> <meta name="author" content=" "> <meta name="description" content="This section describes how to efficiently tokenize large amounts of text via distributed computing on CloudTPUs and Python multiprocessing. We also expose an interface for shard checkpointing to handle unexpected interruptions in data uploading to GCP buckets. The script is adapted from Andrej Karpathy's NanoGPT project with optimizations to process data at a larger scale."> <meta name="keywords" content="jscaling, jax, llms, transformers, tpus, google, cloud, parallelism, distributed"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/jaxformer-icon.png?7001ddef15419e25335b33b49c6ce725"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jaxformer.com/tokenization/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Tokenization at Scale",
            "description": "This section describes how to efficiently tokenize large amounts of text via distributed computing on CloudTPUs and Python multiprocessing. We also expose an interface for shard checkpointing to handle unexpected interruptions in data uploading to GCP buckets. The script is adapted from Andrej Karpathy's NanoGPT project with optimizations to process data at a larger scale.",
            "published": "September 05, 2025",
            "authors": [
              
              {
                "author": "Aditya Makkar",
                "authorURL": "https://x.com/AdityaMakkar000"
              },
              
              {
                "author": "Divya Makkar",
                "authorURL": "https://x.com/_DivyaMakkar"
              },
              
              {
                "author": "Chinmay Jindal",
                "authorURL": "https://x.com/chinmayjindal_"
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <script>
    function goToTop() {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    }

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
      // Get the button:
      let mybutton = document.getElementById("top-button");

      if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
  }
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Jaxformer: Scaling Modern Transformers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="left-button section-button"><a href=".."><svg viewbox="-78.5 0 512 512"><path d="M257 64L291 98 128 262 291 426 257 460 61 262 257 64Z"></path></svg></a></div> <div class="right-button section-button"><a href="../base_model"><svg viewbox="-78.5 0 512 512"><path d="M98 460L64 426 227 262 64 98 98 64 294 262 98 460Z"></path></svg></a></div> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item nav-hidden"><a class="nav-link" onclick="goToTop()" id="top-button" style="display: none;">Back to Top</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item nav-hidden"><a class="nav-link" href="..">Previous Part</a></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../base_model">Next Part</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Sections </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/">Part 0. Introduction</a> <a class="dropdown-item " href="/tokenization/">Part 1. Tokenization</a> <a class="dropdown-item " href="/base_model/">Part 2. Base Model</a> <a class="dropdown-item " href="/sharded/">Part 3. Sharded Model</a> <a class="dropdown-item " href="/dataset/">Part 4. Dataset &amp; Config</a> <a class="dropdown-item " href="/distributed_training/">Part 5. Distributed Training</a> <a class="dropdown-item " href="/moe/">Part 6. Mixture of Experts</a> <a class="dropdown-item " href="/training/">Part 7. Training Results</a> <a class="dropdown-item " href="/conclusion/">Part 8. Conclusion</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Tokenization at Scale</h1> <p>Part 1 of <a href="">Jaxformer</a> (<a href="..">Part 0: Introduction</a> | <a href="../base_model">Part 2: Base Model</a>)</p> <p>This section describes how to efficiently tokenize large amounts of text via distributed computing on CloudTPUs and Python multiprocessing. We also expose an interface for shard checkpointing to handle unexpected interruptions in data uploading to GCP buckets. The script is adapted from Andrej Karpathy's NanoGPT project with optimizations to process data at a larger scale.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#libraries-tokenizer-dataset">Libraries, Tokenizer &amp; Dataset</a> </li> <li> <a href="#splits-streaming">Splits &amp; Streaming</a> </li> </ul> <div> <a href="#multiprocessing-on-a-single-vm">Multiprocessing on a Single VM</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#core-concepts">Core concepts</a> </li> <li> <a href="#pool-multiprocessing">Pool() Multiprocessing</a> </li> </ul> <div> <a href="#integration-with-gcp">Integration with GCP</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#single-use-folder-script">Single-use Folder Script</a> </li> <li> <a href="#uploading-to-gcp-buckets">Uploading to GCP Buckets</a> </li> </ul> <div> <a href="#shard-checkpointing">Shard Checkpointing</a> </div> <div> <a href="#distributed-multiprocessing-using-ray">Distributed Multiprocessing using Ray</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#cluster-setup">Cluster Setup</a> </li> <li> <a href="#adding-ray">Adding Ray</a> </li> <li> <a href="#final-script">Final Script</a> </li> </ul> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>The tokenization script was built off of Andrej Karpathy’s <a href="https://github.com/karpathy/build-nanogpt" rel="external nofollow noopener" target="_blank">Build-NanoGPT</a> architecture with quite a few major changes. Let’s first briefly discuss the basics of the original script before moving on to our additions which significantly speed up the process.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tokenization/1-480.webp 480w,/assets/img/tokenization/1-800.webp 800w,/assets/img/tokenization/1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/tokenization/1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Tokenization of a sentence into 'tokens'. Example with GPT-2 tokenizer</figcaption> </figure> <p>Tokenization is the process of breaking text (in our case, UTF-8 encoding) into smaller chunks that can be used to form a finitely sized vocabulary for an LLM. The exact process for deciding between the tradeoffs of vocab size and average character length of a token (eg. splitting text into individual characters yields a smaller vocab, but loses more information vs individual words) is not done manually. The tokenizer uses the <a href="https://huggingface.co/learn/llm-course/en/chapter6/5" rel="external nofollow noopener" target="_blank">Byte-Pair Encoding</a> (BPE) algorithm, which is tested and optimized differently for various models.</p> <h3 id="libraries-tokenizer--dataset">Libraries, Tokenizer &amp; Dataset</h3> <p>The tokenizer is pre-trained and loaded through OpenAI’s <a href="https://github.com/openai/tiktoken" rel="external nofollow noopener" target="_blank">tiktoken</a> library. Tiktoken is a fast BPE tokenizer that is used with OpenAI’s models. We use the GPT-4 tokenizer (<code class="language-plaintext highlighter-rouge">cl100k_base</code>) with a vocab size of <code class="language-plaintext highlighter-rouge">100,277</code> and thus the <code class="language-plaintext highlighter-rouge">uint32</code> data type is used. The tokenize function grabs the <code class="language-plaintext highlighter-rouge">"text"</code> value of each dataset row and converts it into a <code class="language-plaintext highlighter-rouge">numpy</code> array. The <code class="language-plaintext highlighter-rouge">doc_id</code> is returned for checkpointing purposes, which will be explained in more detail below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">enc</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="p">.</span><span class="nf">encoding_for_model</span><span class="p">(</span><span class="sh">"</span><span class="s">gpt-4</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># 'cl100k_base'
</span><span class="n">eot</span> <span class="o">=</span> <span class="n">enc</span><span class="p">.</span><span class="n">_special_tokens</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;|endoftext|&gt;</span><span class="sh">'</span><span class="p">]</span> <span class="c1"># end of text token
</span>
<span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
  <span class="n">doc_id</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">]</span>
  <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">eot</span><span class="p">]</span> <span class="c1"># the special &lt;|endoftext|&gt; token delimits all documents
</span>  <span class="n">tokens</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">enc</span><span class="p">.</span><span class="nf">encode_ordinary</span><span class="p">(</span><span class="n">doc</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">]))</span>
  <span class="n">tokens_np</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
  <span class="nf">assert </span><span class="p">(</span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">tokens_np</span><span class="p">).</span><span class="nf">all</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span><span class="n">tokens_np</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="p">).</span><span class="nf">all</span><span class="p">(),</span> <span class="sh">"</span><span class="s">token dictionary too large for uint32</span><span class="sh">"</span>
  <span class="n">tokens_np_uint32</span> <span class="o">=</span> <span class="n">tokens_np</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">uint32</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tokens_np_uint32</span><span class="p">,</span> <span class="n">doc_id</span>
</code></pre></div></div> <p>Python’s native <a href="https://docs.python.org/3/library/multiprocessing.html" rel="external nofollow noopener" target="_blank">Multiprocessing</a> module was used for spawning multiple worker processes that each call the tokenize function. This was implemented for a single machine, meaning that the script could only utilize the max CPUs provided by a single instance. We also utilize this for single VM multiprocessing. However, for a distributed implementation, <a href="https://docs.ray.io/en/latest/index.html" rel="external nofollow noopener" target="_blank">Ray</a> was utilized to create a cluster across multiple machines. The code snippet below starts the Pooling processing with the <code class="language-plaintext highlighter-rouge">mp.Pool()</code> context manager. The parameter, <code class="language-plaintext highlighter-rouge">nprocs = int(os.cpu_count()) // 2</code>, denotes the number of worker processes to independently start. Floor division by 2 prevents over-saturating CPUs with too many workers, ensuring smoother performance and less contention.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">mp</span><span class="p">.</span><span class="nc">Pool</span><span class="p">(</span><span class="n">nprocs</span><span class="p">)</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span>
  <span class="bp">...</span>
  <span class="c1"># preallocate buffer to hold current shard
</span>  <span class="n">all_tokens_np</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">empty</span><span class="p">((</span><span class="n">shard_size</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">uint32</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="n">pool</span><span class="p">.</span><span class="nf">imap</span><span class="p">(</span><span class="n">tokenize</span><span class="p">,</span> <span class="n">fw</span><span class="p">,</span> <span class="n">chunksize</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>

    <span class="c1"># check if current shard can accomodate new tokens
</span>    <span class="c1"># if yes --&gt; simply append
</span>    <span class="c1"># if not --&gt; write current shard to file, checkpoint, start new
</span>
    <span class="c1"># at the end --&gt; fill last shard and write remaining to new file
</span>    <span class="bp">...</span>
</code></pre></div></div> <p>HuggingFace’s <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu" rel="external nofollow noopener" target="_blank">FineWeb-EDU</a> dataset was used. The original script used the Sample-10BT bucket, a subset randomly sampled from the whole dataset of around 10B gpt2 tokens. Our modified script uses the Sample-350BT bucket as we aimed to launch much larger training runs. The <code class="language-plaintext highlighter-rouge">load_dataset()</code> data loader from HuggingFace <a href="https://huggingface.co/docs/datasets/en/index" rel="external nofollow noopener" target="_blank">datasets</a> API was utilized.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">remote_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">sample-350BT</span><span class="sh">"</span>
<span class="n">fw</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">HuggingFaceFW/fineweb-edu</span><span class="sh">"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">remote_name</span><span class="p">)</span>
</code></pre></div></div> <h3 id="splits--streaming">Splits &amp; Streaming</h3> <p>A more robust method for changing test and train splits was added. This was done simply by keeping a global variable, <code class="language-plaintext highlighter-rouge">TEST_SPLIT</code> which would indicate the shard at which you wish to stop each split, assuming the order is 1) test and 2) train for the remaining shards. Then, during tokenization, the <code class="language-plaintext highlighter-rouge">shard_index</code> variable was used to track which shard the script was on. Simple conditional logic was added to then redirect the shard to the appropriate GCP bucket, update it’s naming convention and also the uploaded <code class="language-plaintext highlighter-rouge">shard_index_number</code> (different from <code class="language-plaintext highlighter-rouge">shard_index</code>) so that it resets every split.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 90:10 train, test split
</span><span class="n">TEST_SPLIT</span> <span class="o">=</span> <span class="mi">350</span> <span class="c1"># 0 (inclusive) to 350 (exclusive) shards are test
# rest are train
</span>
<span class="bp">...</span>
<span class="k">for</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="n">pool</span><span class="p">.</span><span class="nf">imap</span><span class="p">(</span><span class="n">tokenize</span><span class="p">,</span> <span class="n">fw</span><span class="p">,</span> <span class="n">chunksize</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
    <span class="bp">...</span>

  <span class="k">if</span> <span class="n">shard_index</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">shard_index</span> <span class="o">&lt;</span> <span class="n">TEST_SPLIT</span><span class="p">:</span>
        <span class="n">split</span> <span class="o">=</span> <span class="sh">'</span><span class="s">test/</span><span class="sh">'</span>
        <span class="n">shard_index_number</span> <span class="o">=</span> <span class="n">shard_index</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">split</span> <span class="o">=</span> <span class="sh">'</span><span class="s">train/</span><span class="sh">'</span>
      <span class="n">shard_index_number</span> <span class="o">=</span> <span class="n">shard_index</span> <span class="o">-</span> <span class="n">TEST_SPLIT</span>
    <span class="n">split_name</span> <span class="o">=</span> <span class="n">split</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="bp">...</span>
</code></pre></div></div> <p>Another design decision was to stream the Hugging Face (HF) dataset. Streaming a HF dataset means progressively loading and processing data as you iterate, without downloading the entire dataset to disk. This is ideal for our use case as we can start tokenizing shards right away.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fw</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">HuggingFaceFW/fineweb-edu</span><span class="sh">"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">remote_name</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">,</span> <span class="n">streaming</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <h2 id="multiprocessing-on-a-single-vm">Multiprocessing on a Single VM</h2> <p>A single VM can support multiprocessing as it has multiple CPU cores. We can utilize each of them by spawning identical tokenization processes on each so that it can be done in parallel.</p> <h3 id="core-concepts">Core concepts</h3> <p>Tokenization is a CPU-bound task, which makes Python’s normal threading ineffective because of the <a href="https://en.wikipedia.org/wiki/Global_interpreter_lock" rel="external nofollow noopener" target="_blank">Global Interpreter Lock (GIL)</a>.<d-footnote>Note: Starting in Python 3.13, this limitation has been lifted via an experimental free-threaded build that disables the GIL.</d-footnote> GIL makes it so that only one thread can execute Python bytecode at a time. For I/O tasks or API requests, threading is fine, but for heavy computation it gives almost no speedup as it provides concurrency and not true parallelism.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tokenization/2-480.webp 480w,/assets/img/tokenization/2-800.webp 800w,/assets/img/tokenization/2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/tokenization/2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Multiprocessing uses separate processes whereas threading handles concurrency (<a href="https://datanoon.com/blog/multiprocessing_in_python/" rel="external nofollow noopener" target="_blank">source</a>)</figcaption> </figure> <p>This is why we switch to multiprocessing: each worker runs in its own process, bypassing the GIL and truly using multiple CPU cores in parallel. Each worker independently runs <code class="language-plaintext highlighter-rouge">tokenize()</code>, while the main process orchestrates shard writing and uploads.</p> <h3 id="pool-multiprocessing">Pool() Multiprocessing</h3> <p>A <code class="language-plaintext highlighter-rouge">Pool</code> is just a convenience wrapper in Python’s <code class="language-plaintext highlighter-rouge">multiprocessing</code> module that manages a group of worker processes for you. Instead of manually creating and tracking processes, you create a pool, give it a function (like <code class="language-plaintext highlighter-rouge">tokenize()</code>), and it will distribute work across the workers.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">mp</span><span class="p">.</span><span class="nc">Pool</span><span class="p">(</span><span class="n">nprocs</span><span class="p">)</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="n">pool</span><span class="p">.</span><span class="nf">imap</span><span class="p">(</span><span class="n">tokenize</span><span class="p">,</span> <span class="n">fw</span><span class="p">,</span> <span class="n">chunksize</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
        <span class="bp">...</span>
</code></pre></div></div> <p>In the code, the <code class="language-plaintext highlighter-rouge">Pool</code> spins up <code class="language-plaintext highlighter-rouge">nprocs</code> worker processes and each worker runs independently on a CPU core. The <code class="language-plaintext highlighter-rouge">pool.imap()</code> function is similar to <code class="language-plaintext highlighter-rouge">map()</code> except that it returns back an iterator so the main process can keep writing shards while workers continue tokenizing. In essence, you can start receiving results from workers as soon as they’re ready with <code class="language-plaintext highlighter-rouge">imap</code>, rather than having to wait for all of them to be finished. The <code class="language-plaintext highlighter-rouge">chunksize</code> parameter will cause the iterable to be split into pieces of approximately that size, and each piece is submitted as a separate task. Other aspects of the script include writing the shards to the file, and then a for loop to append shards until the desired size (<code class="language-plaintext highlighter-rouge">100M</code>) is reached, after which it is stored and a new shard/file is started. Progress bar tracking has been taken out of the code snippet below to improve readability.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># function to save sharded file to local disk
</span><span class="k">def</span> <span class="nf">write_datafile</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">tokens_np</span><span class="p">):</span>
    <span class="n">np</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">tokens_np</span><span class="p">)</span>

<span class="bp">...</span>

<span class="k">with</span> <span class="n">mp</span><span class="p">.</span><span class="nc">Pool</span><span class="p">(</span><span class="n">nprocs</span><span class="p">)</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span>
    <span class="n">shard_index</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># current shard index
</span>
    <span class="n">all_tokens_np</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">empty</span><span class="p">((</span><span class="n">shard_size</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">uint16</span><span class="p">)</span>
    <span class="n">token_count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">tokens</span> <span class="ow">in</span> <span class="n">pool</span><span class="p">.</span><span class="nf">imap</span><span class="p">(</span><span class="n">tokenize</span><span class="p">,</span> <span class="n">fw</span><span class="p">,</span> <span class="n">chunksize</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>

        <span class="c1"># if there is enough space in the current shard
</span>        <span class="k">if</span> <span class="n">token_count</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">shard_size</span><span class="p">:</span>
            <span class="c1"># append tokens to current shard
</span>            <span class="n">all_tokens_np</span><span class="p">[</span><span class="n">token_count</span> <span class="p">:</span> <span class="n">token_count</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)]</span> <span class="o">=</span> <span class="n">tokens</span>
            <span class="n">token_count</span> <span class="o">+=</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># write the current shard and start a new one
</span>            <span class="n">split</span> <span class="o">=</span> <span class="sh">"</span><span class="s">val</span><span class="sh">"</span> <span class="k">if</span> <span class="n">shard_index</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="sh">"</span><span class="s">train</span><span class="sh">"</span>
            <span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span>
                <span class="n">DATA_CACHE_DIR</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">shard_index</span><span class="si">:</span><span class="mi">06</span><span class="n">d</span><span class="si">}</span><span class="sh">"</span>
            <span class="p">)</span>
            <span class="c1"># fill the remaining document, then start new shard
</span>            <span class="n">remainder</span> <span class="o">=</span> <span class="n">shard_size</span> <span class="o">-</span> <span class="n">token_count</span>
            <span class="n">all_tokens_np</span><span class="p">[</span><span class="n">token_count</span><span class="p">:</span><span class="n">token_count</span> <span class="o">+</span> <span class="n">remainder</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[:</span><span class="n">remainder</span><span class="p">]</span>
            <span class="nf">write_datafile</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">all_tokens_np</span><span class="p">)</span>
            <span class="n">shard_index</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># populate the next shard with the leftovers of the current doc
</span>            <span class="n">all_tokens_np</span><span class="p">[</span><span class="mi">0</span> <span class="p">:</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">-</span> <span class="n">remainder</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="n">remainder</span><span class="p">:]</span>
            <span class="n">token_count</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">-</span> <span class="n">remainder</span>

    <span class="c1"># write any remaining tokens as the last shard
</span>    <span class="k">if</span> <span class="n">token_count</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">split</span> <span class="o">=</span> <span class="sh">"</span><span class="s">val</span><span class="sh">"</span> <span class="k">if</span> <span class="n">shard_index</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="sh">"</span><span class="s">train</span><span class="sh">"</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">DATA_CACHE_DIR</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">shard_index</span><span class="si">:</span><span class="mi">06</span><span class="n">d</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">write_datafile</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">all_tokens_np</span><span class="p">[:</span><span class="n">token_count</span><span class="p">])</span>
</code></pre></div></div> <h2 id="integration-with-gcp">Integration with GCP</h2> <p>For this project, Google Cloud Storage (GCS) was used due to it’s strong integration in the JAX ecosystem. In order to create a bucket with support for folders, the <code class="language-plaintext highlighter-rouge">Hierarchical namespace</code> was enabled in the GC Console after starting a new project.</p> <h3 id="single-use-folder-script">Single-use Folder Script</h3> <p>After creation, <a href="https://github.com/jindal013/gcp_tokenizer/blob/main/README.md" rel="external nofollow noopener" target="_blank">the TPU can be authenticated</a> so that it can read/write to the bucket. Now, we run the following script to create the checkpoints, train, and test folders. We use with the the Python Client API for GCS.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">google.cloud</span> <span class="kn">import</span> <span class="n">storage_control_v2</span>

<span class="k">def</span> <span class="nf">create_folder</span><span class="p">(</span><span class="n">bucket_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">folder_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">storage_control_client</span> <span class="o">=</span> <span class="n">storage_control_v2</span><span class="p">.</span><span class="nc">StorageControlClient</span><span class="p">()</span>
    <span class="n">project_path</span> <span class="o">=</span> <span class="n">storage_control_client</span><span class="p">.</span><span class="nf">common_project_path</span><span class="p">(</span><span class="sh">"</span><span class="s">_</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">bucket_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">project_path</span><span class="si">}</span><span class="s">/buckets/</span><span class="si">{</span><span class="n">bucket_name</span><span class="si">}</span><span class="sh">"</span>

    <span class="n">request</span> <span class="o">=</span> <span class="n">storage_control_v2</span><span class="p">.</span><span class="nc">CreateFolderRequest</span><span class="p">(</span>
        <span class="n">parent</span><span class="o">=</span><span class="n">bucket_path</span><span class="p">,</span>
        <span class="n">folder_id</span><span class="o">=</span><span class="n">folder_name</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">storage_control_client</span><span class="p">.</span><span class="nf">create_folder</span><span class="p">(</span><span class="n">request</span><span class="o">=</span><span class="n">request</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Created folder: </span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">name</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
   <span class="c1"># The ID of your GCS bucket goes here
</span>  <span class="n">bucket_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">NAME_HERE</span><span class="sh">"</span>

  <span class="k">for</span> <span class="n">folder_name</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">checkpoints</span><span class="sh">'</span><span class="p">]:</span>
    <span class="nf">create_folder</span><span class="p">(</span><span class="n">bucket_name</span><span class="p">,</span> <span class="n">folder_name</span><span class="p">)</span>
</code></pre></div></div> <h3 id="uploading-to-gcp-buckets">Uploading to GCP Buckets</h3> <p>Uploading a given shard and checkpoint to a GCP bucket is done with many helper functions. In order to direct each given shard to the appropriate dataset split, we first save the shard locally to the <code class="language-plaintext highlighter-rouge">data_dir</code> folder (which is included in our <code class="language-plaintext highlighter-rouge">.gitignore</code>).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">upload_file</span><span class="p">(</span><span class="n">split</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">upload_many_blobs_with_transfer_manager</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="n">filenames</span><span class="p">,</span> <span class="n">source_directory</span><span class="o">=</span><span class="sh">""</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>

    <span class="c1"># split gives access to folders within GCP, ie "test/"
</span>    <span class="n">blob_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">split</span> <span class="o">+</span> <span class="n">name</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">filenames</span><span class="p">]</span>

    <span class="c1"># matches blob_name splits with their respective files in local memory
</span>    <span class="n">blob_file_pairs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">source_directory</span><span class="p">,</span> <span class="n">f</span><span class="p">),</span> <span class="n">bucket</span><span class="p">.</span><span class="nf">blob</span><span class="p">(</span><span class="n">b</span><span class="p">))</span> <span class="k">for</span> <span class="n">f</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">filenames</span><span class="p">,</span> <span class="n">blob_names</span><span class="p">)]</span>

    <span class="c1"># uploading the blob_file_pairs onto GCP, utilizes threading
</span>    <span class="n">results</span> <span class="o">=</span> <span class="n">transfer_manager</span><span class="p">.</span><span class="nf">upload_many</span><span class="p">(</span>
      <span class="n">blob_file_pairs</span><span class="p">,</span> <span class="n">skip_if_exists</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_workers</span><span class="o">=</span><span class="n">workers</span><span class="p">,</span> <span class="n">worker_type</span><span class="o">=</span><span class="n">transfer_manager</span><span class="p">.</span><span class="n">THREAD</span>
    <span class="p">)</span>

  <span class="n">FILE_NAMES</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">DATA_CACHE_DIR</span><span class="p">)</span>
  <span class="nf">upload_many_blobs_with_transfer_manager</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="n">FILE_NAMES</span><span class="p">,</span> <span class="n">DATA_CACHE_DIR</span><span class="p">,</span> <span class="n">WORKERS</span><span class="p">)</span>

  <span class="c1"># cleanup
</span>  <span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="n">FILE_NAMES</span><span class="p">:</span>
    <span class="n">full_path</span> <span class="o">=</span> <span class="n">DATA_CACHE_DIR</span> <span class="o">+</span> <span class="sh">'</span><span class="s">/</span><span class="sh">'</span> <span class="o">+</span> <span class="nb">file</span>
    <span class="n">os</span><span class="p">.</span><span class="nf">remove</span><span class="p">(</span><span class="n">full_path</span><span class="p">)</span>
</code></pre></div></div> <h2 id="shard-checkpointing">Shard Checkpointing</h2> <p>We introduce a method to checkpoint uploaded shards to the GCP bucket to avoid losing progress during tokenization, as the process often takes hours even on distributed systems. In our script, if passed the <code class="language-plaintext highlighter-rouge">--continue</code> argument, the script will look for the last uploaded checkpoint in the bucket’s <code class="language-plaintext highlighter-rouge">checkpoints/</code> folder and use the HuggingFace datasets <code class="language-plaintext highlighter-rouge">.skip()</code> method to continue from the next required shard. This is done by keeping track of the number of documents processed in each checkpoint file alongside the document’s unique ID as provided by FineWeb already.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">upload_checkpoint</span><span class="p">():</span>
  <span class="n">checkpoint_files</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">checkpoint_files</span><span class="p">:</span>
    <span class="n">blob</span> <span class="o">=</span> <span class="n">bucket</span><span class="p">.</span><span class="nf">blob</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">checkpoints/</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">blob</span><span class="p">.</span><span class="nf">upload_from_filename</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">checkpoint_files</span><span class="p">:</span>
    <span class="n">os</span><span class="p">.</span><span class="nf">remove</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">upload_checkpoint</code> function checks the local checkpointing dir and simply scrapes its files to redirect them to the GCP bucket. This is akin to the data directory and each checkpointing file is fully self-contained in terms of the information we need to upload. The only caveat is that we need the latest shard as we sort by time when reading checkpoints (explained later). Thus, we make sure to remove all files in the local directory after upload, which ensures that only one checkpoint is present at a given time.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fw</span><span class="p">.</span><span class="nf">skip</span><span class="p">(</span><span class="n">skip_number</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">total docs processed so far: </span><span class="sh">'</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">skip_number</span><span class="p">))</span>
<span class="k">if</span> <span class="n">continue_processing</span><span class="p">:</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">skipped to the previous checkpoint</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>The default value for <code class="language-plaintext highlighter-rouge">skip_number</code> is 0. The <a href="https://huggingface.co/docs/datasets/v1.11.0/dataset_streaming.html#split-your-dataset-with-take-and-skip" rel="external nofollow noopener" target="_blank">datasets library</a> provides the <code class="language-plaintext highlighter-rouge">skip(n)</code> function which skips over the first <code class="language-plaintext highlighter-rouge">n</code> examples/rows in the given dataset. In the actual script, checkpointing is done only when it is ready to upload a shard. This ensures a guarantee that no previous progress has been lost, and the downtime for progress lost in between is little (&lt;1 min for a single v4 TPU, &lt;20s for newer versions TPU generations).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">mp</span><span class="p">.</span><span class="nc">Pool</span><span class="p">(</span><span class="n">nprocs</span><span class="p">)</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span>
  <span class="k">if</span> <span class="n">continue_processing</span><span class="p">:</span>
    <span class="n">shard_index</span> <span class="o">=</span> <span class="n">shard_to_resume</span> <span class="o">+</span> <span class="mi">1</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">shard_index</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="bp">...</span>
  <span class="k">for</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="n">pool</span><span class="p">.</span><span class="nf">imap</span><span class="p">(</span><span class="n">tokenize</span><span class="p">,</span> <span class="n">fw</span><span class="p">,</span> <span class="n">chunksize</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
    <span class="n">skip_number</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">token_count</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">shard_size</span><span class="p">:</span>
      <span class="bp">...</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># checkpoint the shard
</span>      <span class="n">checkpoint_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">doc_id</span><span class="si">}</span><span class="s">.txt</span><span class="sh">"</span><span class="p">)</span>
      <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">checkpoint_filename</span><span class="p">,</span> <span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
          <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="n">shard_index</span><span class="p">)</span> <span class="o">+</span> <span class="sh">'</span><span class="s">:</span><span class="sh">'</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">skip_number</span><span class="p">))</span>
      <span class="bp">...</span>
      <span class="c1"># upload file and checkpointing functions
</span>      <span class="nf">upload_file</span><span class="p">(</span><span class="n">split</span><span class="p">)</span>
      <span class="nf">upload_checkpoint</span><span class="p">()</span>
</code></pre></div></div> <p>Finally, in order to load the checkpoints at startup (only true if the <code class="language-plaintext highlighter-rouge">--continue</code> flag is provided), we check the GCP folder for the latest checkpoint sorted by time. Then, the <code class="language-plaintext highlighter-rouge">shard_to_resume</code> (number of shards already processed for future naming) and <code class="language-plaintext highlighter-rouge">skip_number</code> (number of document rows already processed) variables are pulled from the file data, which were determined at upload time.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">continue_processing</span><span class="p">:</span>
  <span class="c1"># pull latest checkpoint name from gcp bucket called checkpoints
</span>  <span class="n">blobs</span> <span class="o">=</span> <span class="n">bucket</span><span class="p">.</span><span class="nf">list_blobs</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="sh">"</span><span class="s">checkpoints/</span><span class="sh">"</span><span class="p">)</span>
  <span class="n">checkpoint_blobs</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">blobs</span> <span class="k">if</span> <span class="nf">str</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">name</span><span class="p">).</span><span class="nf">endswith</span><span class="p">(</span><span class="sh">"</span><span class="s">.txt</span><span class="sh">"</span><span class="p">)]</span>

  <span class="c1"># if no checkpoints are found
</span>  <span class="k">if</span> <span class="ow">not</span> <span class="n">checkpoint_blobs</span><span class="p">:</span>
    <span class="n">continue_processing</span> <span class="o">=</span> <span class="bp">False</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">latest_checkpoint</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">checkpoint_blobs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">b</span><span class="p">:</span> <span class="n">b</span><span class="p">.</span><span class="n">updated</span><span class="p">)</span>
    <span class="c1"># grab shard id (in checkpoint name upon upload)
</span>    <span class="n">checkpoint_to_resume</span> <span class="o">=</span> <span class="n">latest_checkpoint</span><span class="p">.</span><span class="n">name</span><span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="sh">"</span><span class="s">checkpoints/</span><span class="sh">"</span><span class="p">):</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span>
    <span class="c1"># parse file to get shard number and # of documents skipped
</span>    <span class="n">shard_to_resume</span><span class="p">,</span> <span class="n">skip_number</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="p">(</span><span class="n">latest_checkpoint</span><span class="p">.</span><span class="nf">download_as_bytes</span><span class="p">().</span><span class="nf">decode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)).</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">:</span><span class="sh">'</span><span class="p">))</span>
</code></pre></div></div> <h2 id="distributed-multiprocessing-using-ray">Distributed Multiprocessing using Ray</h2> <p><a href="https://docs.ray.io/en/latest/index.html" rel="external nofollow noopener" target="_blank">Ray</a> is an open-source framework for distributed machine learning applications. It provides an interface to connect multiple machines on the same network (for example, 32 v4 TPUs) into a “cluster” that can utilize all of the shared resources together.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tokenization/3-480.webp 480w,/assets/img/tokenization/3-800.webp 800w,/assets/img/tokenization/3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/tokenization/3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Example Ray use-case to perform model parallelism (<a href="https://docs.ray.io/en/latest/ray-overview/use-cases.html" rel="external nofollow noopener" target="_blank">source</a>)</figcaption> </figure> <p>The Ray library exposes a multiprocessing API that is intended to directly replace Python’s <code class="language-plaintext highlighter-rouge">multiprocessing</code> module. However before using Ray, we have to edit a few parts of the tokenization function so that it can support distributed operations. This is to ensure that data and information are shared correctly across different TPUs on different machines. For example, we are not able to use the <code class="language-plaintext highlighter-rouge">pool.imap</code> function anymore. This was better before as<code class="language-plaintext highlighter-rouge">pool.imap</code> streams results from workers incrementally (instead of waiting for all tasks like <code class="language-plaintext highlighter-rouge">map</code>), which saves memory and lets us shard, checkpoint, and upload on the go.</p> <h3 id="cluster-setup">Cluster Setup</h3> <p>Ray’s key idea is that any Python function can be turned into a remote task and run on any node in the cluster. You mark a function with <code class="language-plaintext highlighter-rouge">@ray.remote</code> decorator, call it with <code class="language-plaintext highlighter-rouge">.remote()</code>, and Ray takes care of process scheduling, inter-node communication, and result collection. Likewise, <a href="https://docs.ray.io/en/latest/ray-core/starting-ray.html" rel="external nofollow noopener" target="_blank">cluster setup</a> is straightforward: one arbitrary machine acts as the head node, and others join as worker nodes. Once Ray is initialized, all nodes form a single logical pool of resources (CPUs, GPUs, memory). The following commands can also be found on the SSH startup script in our repo under the tokenization section.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># on the head node
</span><span class="n">ray</span> <span class="n">start</span> <span class="o">--</span><span class="n">head</span> <span class="o">--</span><span class="n">port</span><span class="o">=</span><span class="mi">6379</span>

<span class="c1"># on each worker node (replace head-node-ip with what the prev. command outputs)
</span><span class="n">ray</span> <span class="n">start</span> <span class="o">--</span><span class="n">address</span><span class="o">=</span><span class="sh">'</span><span class="s">head-node-ip:6379</span><span class="sh">'</span>
</code></pre></div></div> <p>After starting the cluster in the terminal, the follow code needs to be added to the tokenization file after downloading the required packages (guides found on <a href="https://docs.ray.io/en/latest/cluster/vms/getting-started.html" rel="external nofollow noopener" target="_blank">Ray docs</a>)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">ray</span>
<span class="n">ray</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">address</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># connect to the cluster
</span></code></pre></div></div> <h3 id="adding-ray">Adding Ray</h3> <p>In our original script, tokenization was done with <code class="language-plaintext highlighter-rouge">multiprocessing.Pool</code>. To move this to Ray, we convert the <code class="language-plaintext highlighter-rouge">tokenize()</code> function into a Ray task using the <code class="language-plaintext highlighter-rouge">ray.remote</code> decorator:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@ray.remote</span>
<span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
  <span class="n">doc_id</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">]</span>
  <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">eot</span><span class="p">]</span> <span class="c1"># the special &lt;|endoftext|&gt; token delimits all documents
</span>  <span class="n">tokens</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">enc</span><span class="p">.</span><span class="nf">encode_ordinary</span><span class="p">(</span><span class="n">doc</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">]))</span>
  <span class="n">tokens_np</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
  <span class="nf">assert </span><span class="p">(</span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">tokens_np</span><span class="p">).</span><span class="nf">all</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span><span class="n">tokens_np</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="p">).</span><span class="nf">all</span><span class="p">(),</span> <span class="sh">"</span><span class="s">token dictionary too large for uint32</span><span class="sh">"</span>
  <span class="n">tokens_np_uint32</span> <span class="o">=</span> <span class="n">tokens_np</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">uint32</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tokens_np_uint32</span><span class="p">,</span> <span class="n">doc_id</span>
</code></pre></div></div> <p>Now, instead of running locally, each call to <code class="language-plaintext highlighter-rouge">tokenize_remote.remote(doc)</code> will be dispatched to any available worker across the cluster. Results are collected with <code class="language-plaintext highlighter-rouge">ray.get()</code>. Additionally, as we are no longer using <code class="language-plaintext highlighter-rouge">pool.imap()</code>, we have to create our own batches for the worker processes. This is done with a simple python list and a <code class="language-plaintext highlighter-rouge">while True</code> loop is added to maintain similar structure to previous script:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
  <span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
      <span class="n">batch</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">next</span><span class="p">(</span><span class="n">doc_iter</span><span class="p">))</span>
  <span class="k">except</span> <span class="nb">StopIteration</span><span class="p">:</span>
    <span class="k">pass</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">batch</span><span class="p">:</span>
    <span class="k">break</span>

  <span class="n">futures</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenize</span><span class="p">.</span><span class="nf">remote</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
  <span class="n">results</span> <span class="o">=</span> <span class="n">ray</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">futures</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">BATCH_SIZE</code> is a hyperparamater that must be optimized depending on the cluster configuration. The bash script for finding the optimal <code class="language-plaintext highlighter-rouge">BATCH_SIZE</code> value for your particular cluster is provided in the <code class="language-plaintext highlighter-rouge">tokenization/scripts/</code> folder.</p> <h3 id="final-script">Final Script</h3> <p>With the cluster initialized, the tokenization function adapted for distributed execution, and batching logic added, we can now combine everything into the full Ray-enabled pipeline. The final script ties together streaming from Hugging Face, distributed tokenization across nodes, shard writing, checkpointing, and GCP uploads. Below is the complete version, with inline comments explaining each major step.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">shutil</span>
<span class="kn">import</span> <span class="n">multiprocessing</span> <span class="k">as</span> <span class="n">mp</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">tiktoken</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="n">google.cloud.storage</span> <span class="kn">import</span> <span class="n">Client</span><span class="p">,</span> <span class="n">transfer_manager</span>
<span class="kn">import</span> <span class="n">argparse</span>
<span class="kn">import</span> <span class="n">ray</span>

<span class="c1"># init ray in the cluster mode
</span><span class="n">ray</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">address</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># constants for splits and multiprocessing
</span><span class="n">TEST_SPLIT</span> <span class="o">=</span> <span class="mi">350</span>
<span class="n">BUCKET_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">ray_jaxformer</span><span class="sh">"</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">WORKERS</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="nf">cpu_count</span><span class="p">())</span>
<span class="n">nprocs</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="nf">cpu_count</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1.5</span><span class="p">))</span>

<span class="c1"># other constants for dataset processing
</span><span class="n">local_dir</span> <span class="o">=</span> <span class="sh">"</span><span class="s">data_dir</span><span class="sh">"</span>
<span class="n">remote_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">sample-350BT</span><span class="sh">"</span>
<span class="n">shard_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="mf">1e8</span><span class="p">)</span>

<span class="c1"># gcp storage client and bucket
</span><span class="n">storage_client</span> <span class="o">=</span> <span class="nc">Client</span><span class="p">()</span>
<span class="n">bucket</span> <span class="o">=</span> <span class="n">storage_client</span><span class="p">.</span><span class="nf">bucket</span><span class="p">(</span><span class="n">BUCKET_NAME</span><span class="p">)</span>

<span class="c1"># create the cache the local directory if it doesn't exist yet
</span><span class="n">DATA_CACHE_DIR</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">dirname</span><span class="p">(</span><span class="n">__file__</span><span class="p">),</span> <span class="n">local_dir</span><span class="p">)</span>
<span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">dirname</span><span class="p">(</span><span class="n">__file__</span><span class="p">),</span> <span class="sh">'</span><span class="s">checkpoints</span><span class="sh">'</span><span class="p">)</span>
<span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">DATA_CACHE_DIR</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># set up argument parser to check if --continue flag is given
</span><span class="k">def</span> <span class="nf">setup_argument_parser</span><span class="p">():</span>
  <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="p">.</span><span class="nc">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">'</span><span class="s">Process the 350BT dataset</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">'</span><span class="s">--continue</span><span class="sh">'</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="sh">'</span><span class="s">continue_processing</span><span class="sh">'</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="sh">'</span><span class="s">store_true</span><span class="sh">'</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="sh">'</span><span class="s">Continue processing from a checkpoint</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">parser</span><span class="p">.</span><span class="nf">set_defaults</span><span class="p">(</span><span class="n">continue_processing</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">parser</span>

<span class="n">parser</span> <span class="o">=</span> <span class="nf">setup_argument_parser</span><span class="p">()</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse_args</span><span class="p">()</span>
<span class="n">continue_processing</span> <span class="o">=</span> <span class="n">args</span><span class="p">.</span><span class="n">continue_processing</span>
<span class="n">checkpoint_to_resume</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">shard_to_resume</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">skip_number</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># if a --continue flag is given, pull latest checkpoint name from gcp bucket called checkpoints
</span><span class="k">if</span> <span class="n">continue_processing</span><span class="p">:</span>
  <span class="c1"># pull latest checkpoint name from gcp bucket called checkpoints
</span>  <span class="n">blobs</span> <span class="o">=</span> <span class="n">bucket</span><span class="p">.</span><span class="nf">list_blobs</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="sh">"</span><span class="s">checkpoints/</span><span class="sh">"</span><span class="p">)</span>
  <span class="n">checkpoint_blobs</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">blobs</span> <span class="k">if</span> <span class="nf">str</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">name</span><span class="p">).</span><span class="nf">endswith</span><span class="p">(</span><span class="sh">"</span><span class="s">.txt</span><span class="sh">"</span><span class="p">)]</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">checkpoint_blobs</span><span class="p">:</span>
    <span class="n">continue_processing</span> <span class="o">=</span> <span class="bp">False</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">latest_checkpoint</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">checkpoint_blobs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">b</span><span class="p">:</span> <span class="n">b</span><span class="p">.</span><span class="n">updated</span><span class="p">)</span>
    <span class="n">checkpoint_to_resume</span> <span class="o">=</span> <span class="n">latest_checkpoint</span><span class="p">.</span><span class="n">name</span><span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="sh">"</span><span class="s">checkpoints/</span><span class="sh">"</span><span class="p">):</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span>  <span class="c1"># remove 'checkpoints/' prefix and '.txt' suffix
</span>    <span class="n">shard_to_resume</span><span class="p">,</span> <span class="n">skip_number</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="p">(</span><span class="n">latest_checkpoint</span><span class="p">.</span><span class="nf">download_as_bytes</span><span class="p">().</span><span class="nf">decode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)).</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">:</span><span class="sh">'</span><span class="p">))</span>

<span class="c1"># ------------------------------------------
</span>
<span class="n">fw</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">HuggingFaceFW/fineweb-edu</span><span class="sh">"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">remote_name</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">,</span> <span class="n">streaming</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># init the tokenizer
</span><span class="n">enc</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="p">.</span><span class="nf">encoding_for_model</span><span class="p">(</span><span class="sh">"</span><span class="s">gpt-4</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># 'cl100k_base'
</span><span class="n">eot</span> <span class="o">=</span> <span class="n">enc</span><span class="p">.</span><span class="n">_special_tokens</span><span class="p">[</span><span class="sh">'</span><span class="s">&lt;|endoftext|&gt;</span><span class="sh">'</span><span class="p">]</span> <span class="c1"># end of text token
</span>
<span class="c1"># tokenize function with ray remote decorator
</span><span class="nd">@ray.remote</span>
<span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
  <span class="n">doc_id_return</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">]</span>
  <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">eot</span><span class="p">]</span>
  <span class="n">tokens</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">enc</span><span class="p">.</span><span class="nf">encode_ordinary</span><span class="p">(</span><span class="n">doc</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">]))</span>
  <span class="n">tokens_np</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
  <span class="nf">assert </span><span class="p">(</span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">tokens_np</span><span class="p">).</span><span class="nf">all</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span><span class="n">tokens_np</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="p">).</span><span class="nf">all</span><span class="p">(),</span> <span class="sh">"</span><span class="s">token dictionary too large for uint32</span><span class="sh">"</span>
  <span class="n">tokens_np_uint32</span> <span class="o">=</span> <span class="n">tokens_np</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">uint32</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tokens_np_uint32</span><span class="p">,</span> <span class="n">doc_id_return</span>

<span class="k">def</span> <span class="nf">write_datafile</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">tokens_np</span><span class="p">):</span>
  <span class="n">np</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">tokens_np</span><span class="p">)</span>

<span class="c1"># function to upload files to gcp bucket using transfer manager
</span><span class="k">def</span> <span class="nf">upload_file</span><span class="p">(</span><span class="n">split</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">upload_many_blobs_with_transfer_manager</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="n">filenames</span><span class="p">,</span> <span class="n">source_directory</span><span class="o">=</span><span class="sh">""</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>

    <span class="n">blob_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">split</span> <span class="o">+</span> <span class="n">name</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">filenames</span><span class="p">]</span>

    <span class="n">blob_file_pairs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">source_directory</span><span class="p">,</span> <span class="n">f</span><span class="p">),</span> <span class="n">bucket</span><span class="p">.</span><span class="nf">blob</span><span class="p">(</span><span class="n">b</span><span class="p">))</span> <span class="k">for</span> <span class="n">f</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">filenames</span><span class="p">,</span> <span class="n">blob_names</span><span class="p">)]</span>

    <span class="n">results</span> <span class="o">=</span> <span class="n">transfer_manager</span><span class="p">.</span><span class="nf">upload_many</span><span class="p">(</span>
      <span class="n">blob_file_pairs</span><span class="p">,</span> <span class="n">skip_if_exists</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_workers</span><span class="o">=</span><span class="n">workers</span><span class="p">,</span> <span class="n">worker_type</span><span class="o">=</span><span class="n">transfer_manager</span><span class="p">.</span><span class="n">THREAD</span>
    <span class="p">)</span>

  <span class="n">FILE_NAMES</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">DATA_CACHE_DIR</span><span class="p">)</span>
  <span class="nf">upload_many_blobs_with_transfer_manager</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="n">FILE_NAMES</span><span class="p">,</span> <span class="n">DATA_CACHE_DIR</span><span class="p">,</span> <span class="n">WORKERS</span><span class="p">)</span>
  <span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="n">FILE_NAMES</span><span class="p">:</span>
    <span class="n">full_path</span> <span class="o">=</span> <span class="n">DATA_CACHE_DIR</span> <span class="o">+</span> <span class="sh">'</span><span class="s">/</span><span class="sh">'</span> <span class="o">+</span> <span class="nb">file</span>
    <span class="n">os</span><span class="p">.</span><span class="nf">remove</span><span class="p">(</span><span class="n">full_path</span><span class="p">)</span>

<span class="c1"># function to upload checkpoints to gcp bucket and remove local copies
</span><span class="k">def</span> <span class="nf">upload_checkpoint</span><span class="p">():</span>
  <span class="n">checkpoint_files</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">checkpoint_files</span><span class="p">:</span>
    <span class="n">blob</span> <span class="o">=</span> <span class="n">bucket</span><span class="p">.</span><span class="nf">blob</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">checkpoints/</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">blob</span><span class="p">.</span><span class="nf">upload_from_filename</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">checkpoint_files</span><span class="p">:</span>
    <span class="n">os</span><span class="p">.</span><span class="nf">remove</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span>

<span class="c1"># skip to the previous checkpoint (zero by default)
</span><span class="n">fw</span><span class="p">.</span><span class="nf">skip</span><span class="p">(</span><span class="n">skip_number</span><span class="p">)</span>
<span class="n">shard_index</span> <span class="o">=</span> <span class="n">shard_to_resume</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">continue_processing</span> <span class="k">else</span> <span class="mi">0</span>

<span class="c1"># variables to keep track of tokens in the current shard
</span><span class="n">all_tokens_np</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">empty</span><span class="p">((</span><span class="n">shard_size</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">uint32</span><span class="p">)</span>
<span class="n">token_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">progress_bar</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">doc_iter</span> <span class="o">=</span> <span class="nf">iter</span><span class="p">(</span><span class="n">fw</span><span class="p">)</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">try</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
        <span class="n">batch</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">next</span><span class="p">(</span><span class="n">doc_iter</span><span class="p">))</span>
    <span class="k">except</span> <span class="nb">StopIteration</span><span class="p">:</span>
      <span class="k">pass</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">batch</span><span class="p">:</span>
      <span class="k">break</span>

    <span class="c1"># get the tokenized results from ray
</span>    <span class="n">futures</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenize</span><span class="p">.</span><span class="nf">remote</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">ray</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">futures</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
      <span class="n">skip_number</span> <span class="o">+=</span> <span class="mi">1</span>

      <span class="c1"># if the current document fits in the current shard
</span>      <span class="k">if</span> <span class="n">token_count</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">shard_size</span><span class="p">:</span>

        <span class="c1"># simply append tokens to current shard
</span>        <span class="n">all_tokens_np</span><span class="p">[</span><span class="n">token_count</span><span class="p">:</span><span class="n">token_count</span><span class="o">+</span><span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)]</span> <span class="o">=</span> <span class="n">tokens</span>
        <span class="n">token_count</span> <span class="o">+=</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

        <span class="c1"># update progress bar
</span>        <span class="k">if</span> <span class="n">progress_bar</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
          <span class="n">progress_bar</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">shard_size</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="sh">"</span><span class="s">tokens</span><span class="sh">"</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Shard </span><span class="si">{</span><span class="n">shard_index</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">dynamic_ncols</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">progress_bar</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>

      <span class="k">else</span><span class="p">:</span>

        <span class="c1"># save a checkpoint for resuming later
</span>        <span class="n">checkpoint_filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">doc_id</span><span class="si">}</span><span class="s">.txt</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">checkpoint_filename</span><span class="p">,</span> <span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
          <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="n">shard_index</span><span class="p">)</span> <span class="o">+</span> <span class="sh">'</span><span class="s">:</span><span class="sh">'</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">skip_number</span><span class="p">))</span>

        <span class="c1"># write the current shard and start a new one
</span>        <span class="k">if</span> <span class="n">shard_index</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">shard_index</span> <span class="o">&lt;</span> <span class="n">TEST_SPLIT</span><span class="p">:</span>
          <span class="n">split</span> <span class="o">=</span> <span class="sh">'</span><span class="s">test/</span><span class="sh">'</span>
          <span class="n">shard_index_number</span> <span class="o">=</span> <span class="n">shard_index</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">split</span> <span class="o">=</span> <span class="sh">'</span><span class="s">train/</span><span class="sh">'</span>
          <span class="n">shard_index_number</span> <span class="o">=</span> <span class="n">shard_index</span> <span class="o">-</span> <span class="n">TEST_SPLIT</span>
        <span class="n">split_name</span> <span class="o">=</span> <span class="n">split</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">DATA_CACHE_DIR</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">split_name</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">shard_index_number</span><span class="si">:</span><span class="mi">04</span><span class="n">d</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

        <span class="c1"># split the document into whatever fits in this shard; the remainder goes to next one
</span>        <span class="n">remainder</span> <span class="o">=</span> <span class="n">shard_size</span> <span class="o">-</span> <span class="n">token_count</span>
        <span class="n">progress_bar</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">remainder</span><span class="p">)</span>
        <span class="n">all_tokens_np</span><span class="p">[</span><span class="n">token_count</span><span class="p">:</span><span class="n">token_count</span><span class="o">+</span><span class="n">remainder</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[:</span><span class="n">remainder</span><span class="p">]</span>

        <span class="nf">write_datafile</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">all_tokens_np</span><span class="p">)</span>
        <span class="nf">upload_file</span><span class="p">(</span><span class="n">split</span><span class="p">)</span>
        <span class="nf">upload_checkpoint</span><span class="p">()</span>

        <span class="c1"># update shard index and reset progress bar
</span>        <span class="n">shard_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">progress_bar</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="c1"># populate the next shard with the leftovers of the current doc
</span>        <span class="n">all_tokens_np</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="o">-</span><span class="n">remainder</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="n">remainder</span><span class="p">:]</span>
        <span class="n">token_count</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="o">-</span><span class="n">remainder</span>

<span class="c1"># write any remaining tokens as the last shard
</span><span class="k">if</span> <span class="n">token_count</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
  <span class="k">if</span> <span class="n">shard_index</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">shard_index</span> <span class="o">&lt;</span> <span class="n">TEST_SPLIT</span><span class="p">:</span>
    <span class="n">split</span> <span class="o">=</span> <span class="sh">'</span><span class="s">test/</span><span class="sh">'</span>
    <span class="n">shard_index_number</span> <span class="o">=</span> <span class="n">shard_index</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">split</span> <span class="o">=</span> <span class="sh">'</span><span class="s">train/</span><span class="sh">'</span>
    <span class="n">shard_index_number</span> <span class="o">=</span> <span class="n">shard_index</span> <span class="o">-</span> <span class="n">TEST_SPLIT</span>
  <span class="n">split_name</span> <span class="o">=</span> <span class="n">split</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

  <span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">DATA_CACHE_DIR</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">split_name</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">shard_index_number</span><span class="si">:</span><span class="mi">04</span><span class="n">d</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

  <span class="nf">write_datafile</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">all_tokens_np</span><span class="p">[:</span><span class="n">token_count</span><span class="p">])</span>
  <span class="nf">upload_file</span><span class="p">(</span><span class="n">split</span><span class="p">)</span>
  <span class="nf">upload_checkpoint</span><span class="p">()</span>


<span class="c1"># clean up directory after function terminates
</span><span class="k">if</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">DATA_CACHE_DIR</span><span class="p">):</span>
  <span class="n">shutil</span><span class="p">.</span><span class="nf">rmtree</span><span class="p">(</span><span class="n">DATA_CACHE_DIR</span><span class="p">)</span>
</code></pre></div></div> <p>Now that we are done with tokenization, we can move onto model architecture, starting with learning how to write the base model.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/main.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'jindal013/jaxformer-website-v2',
        'data-repo-id': 'R_kgDOPoEVEA',
        'data-category': 'General',
        'data-category-id': 'DIC_kwDOPoEVEM4Cu2n4',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-loading': '1',
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>