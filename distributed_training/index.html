<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Distributed Training: Scaling Transformers in Practice | Jaxformer: Scaling Modern Transformers </title> <meta name="author" content=" "> <meta name="description" content="We now introduce the main training script that will be used to launch the training. This covers the infrastructure, distributed functions and training loops that will sync all devices together."> <meta name="keywords" content="jscaling, jax, llms, transformers, tpus, google, cloud, parallelism, distributed"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/jaxformer-icon.png?7001ddef15419e25335b33b49c6ce725"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jaxformer.com/distributed_training/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> <script async src="https://www.googletagmanager.com/gtag/js?id=G-G878LK8JDJ"></script> <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-G878LK8JDJ');
</script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Distributed Training: Scaling Transformers in Practice",
            "description": "We now introduce the main training script that will be used to launch the training. This covers the infrastructure, distributed functions and training loops that will sync all devices together.",
            "published": "September 05, 2025",
            "authors": [
              
              {
                "author": "Aditya Makkar",
                "authorURL": "https://x.com/AdityaMakkar000"
              },
              
              {
                "author": "Divya Makkar",
                "authorURL": "https://x.com/_DivyaMakkar"
              },
              
              {
                "author": "Chinmay Jindal",
                "authorURL": "https://x.com/chinmayjindal_"
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <script>
    function goToTop() {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    }

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
      // Get the button:
      let mybutton = document.getElementById("top-button");

      if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
  }
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="d-none d-sm-inline">Jaxformer: Scaling Modern Transformers</span> <span class="d-inline d-sm-none"> Jaxformer </span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="left-button section-button"><a href="../dataset"><svg viewbox="-78.5 0 512 512"><path d="M257 64L291 98 128 262 291 426 257 460 61 262 257 64Z"></path></svg></a></div> <div class="right-button section-button"><a href="../moe"><svg viewbox="-78.5 0 512 512"><path d="M98 460L64 426 227 262 64 98 98 64 294 262 98 460Z"></path></svg></a></div> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item nav-hidden"><a class="nav-link" onclick="goToTop()" id="top-button" style="display: none;">Back to Top</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../dataset">Previous Part</a></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../moe">Next Part</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Sections </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/">Part 0. Introduction</a> <a class="dropdown-item " href="/tokenization/">Part 1. Tokenization</a> <a class="dropdown-item " href="/base_model/">Part 2. Base Model</a> <a class="dropdown-item " href="/sharded/">Part 3. Sharded Model</a> <a class="dropdown-item " href="/dataset/">Part 4. Dataset &amp; Config</a> <a class="dropdown-item " href="/distributed_training/">Part 5. Distributed Training</a> <a class="dropdown-item " href="/moe/">Part 6. Mixture of Experts</a> <a class="dropdown-item " href="/training/">Part 7. Training Results</a> <a class="dropdown-item " href="/conclusion/">Part 8. Conclusion</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Distributed Training: Scaling Transformers in Practice</h1> <p>Part 5 of <a href="">Jaxformer</a> (<a href="../dataset">Part 4: Dataset &amp; Config</a> | <a href="../moe">Part 6: Mixture of Experts</a>)</p> <p>We now introduce the main training script that will be used to launch the training. This covers the infrastructure, distributed functions and training loops that will sync all devices together.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#setting-up-the-distributed-environment">Setting Up the Distributed Environment</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#configuring-jax-xla-flags">Configuring JAX &amp; XLA Flags</a> </li> <li> <a href="#device-mesh-initialization">Device Mesh Initialization</a> </li> </ul> <div> <a href="#training-infrastructure">Training Infrastructure</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#checkpointing-and-state-management">Checkpointing and State Management</a> </li> <li> <a href="#data-partitioning-and-model-initialization">Data Partitioning and Model Initialization</a> </li> </ul> <div> <a href="#training-and-evaluation-loops">Training and Evaluation Loops</a> </div> </nav> </d-contents> <h2 id="setting-up-the-distributed-environment">Setting Up the Distributed Environment</h2> <h3 id="configuring-jax--xla-flags">Configuring JAX &amp; XLA Flags</h3> <p>We will begin by configuring JAX. In JAX, XLA flags optimize performance and are related to communications that occur between GPUs . We can follow general practice to enable flags allowing for faster performance. Note we train on TPUs but the flags do not hurt performance in general.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>

<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">XLA_FLAGS</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">--xla_gpu_triton_gemm_any=True --xla_gpu_enable_latency_hiding_scheduler=true </span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div> <p>We can also use JAX’s optional disk cache which enables JAX to store copies of complied programs on disk, saving recompilation time when running the same or similar tasks repeatedly. We use a remote file storage to sync cache across multi-controller nodes.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">jax</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="sh">"</span><span class="s">jax_compilation_cache_dir</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">gs://jaxformer-cache/</span><span class="sh">"</span><span class="p">)</span>
<span class="n">jax</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="sh">"</span><span class="s">jax_persistent_cache_min_entry_size_bytes</span><span class="sh">"</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">jax</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="sh">"</span><span class="s">jax_persistent_cache_min_compile_time_secs</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">jax</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">jax_persistent_cache_enable_xla_caches</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">xla_gpu_per_fusion_autotune_cache_dir</span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div> <p>We can begin by writing a helper function to print values on multi-controller JAX since we only want to print once on the main node’s process.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="n">msg</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">jax</span><span class="p">.</span><span class="nf">process_index</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
</code></pre></div></div> <h3 id="device-mesh-initialization">Device Mesh Initialization</h3> <p>We can now create a function to setup the mesh for the given TPU topology. The function takes in the number of devices per axes, the name of each axis and returns the <code class="language-plaintext highlighter-rouge">JAX</code> mesh. It first makes the devices into an np array and ensures we have the right number of devices in the desired mesh as the device count.</p> <p>We try to use the <code class="language-plaintext highlighter-rouge">jax.make_mesh</code> function as that makes the most optimized mesh given the topology of TPUs; however, if it cannot, it throws an exception hence we wrap it in a try-catch and make the mesh ourself. The mesh is then returned.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">init_devices</span><span class="p">(</span>
    <span class="n">axes</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="p">...],</span> <span class="n">axes_name</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="p">...]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">Mesh</span><span class="p">:</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">devices</span><span class="p">())</span>
    <span class="c1"># print for convenience
</span>    <span class="c1"># Assumes you are on TPU
</span>    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">ndindex</span><span class="p">(</span><span class="n">devices</span><span class="p">.</span><span class="n">shape</span><span class="p">):</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">devices</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="nf">log</span><span class="p">(</span>
            <span class="sa">f</span><span class="sh">"</span><span class="s">  </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s"> ID: </span><span class="si">{</span><span class="n">d</span><span class="p">.</span><span class="nb">id</span><span class="si">}</span><span class="s">, Process: </span><span class="si">{</span><span class="n">d</span><span class="p">.</span><span class="n">process_index</span><span class="si">}</span><span class="s">, </span><span class="sh">"</span>
            <span class="sa">f</span><span class="sh">"</span><span class="s">Coords: </span><span class="si">{</span><span class="n">d</span><span class="p">.</span><span class="n">coords</span><span class="si">}</span><span class="s">, Core: </span><span class="si">{</span><span class="n">d</span><span class="p">.</span><span class="n">core_on_chip</span><span class="si">}</span><span class="sh">"</span>
        <span class="p">)</span>

    <span class="k">assert</span> <span class="n">devices</span><span class="p">.</span><span class="n">size</span> <span class="o">==</span> <span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">(</span><span class="n">axes</span><span class="p">),</span> <span class="p">(</span>
        <span class="sa">f</span><span class="sh">"</span><span class="s">Expected </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span><span class="si">}</span><span class="s"> devices, got </span><span class="si">{</span><span class="n">devices</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span>
    <span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">make_mesh</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">axes_name</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="nf">log</span><span class="p">(</span><span class="sh">"</span><span class="s">Failed to create mesh with make_mesh, falling back to sharding.Mesh</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="nc">Mesh</span><span class="p">(</span><span class="n">devices</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">axes</span><span class="p">),</span> <span class="n">axes_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mesh</span>
</code></pre></div></div> <h2 id="training-infrastructure">Training Infrastructure</h2> <p>With the helper functions established, we can now begin the <code class="language-plaintext highlighter-rouge">main</code> training loop function. Our main function will take in the <code class="language-plaintext highlighter-rouge">config</code> we described earlier. Since we are assuming this script is for <code class="language-plaintext highlighter-rouge">3-D</code> parallelism, we can assign variables to the device size for each axis and setup the key with the initial seed.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="n">config</span><span class="p">):</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">LAYER_PARALLEL</span><span class="p">,</span> <span class="n">TENSOR_PARALLEL</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">device_config</span><span class="p">.</span><span class="n">n_device_axis</span>
</code></pre></div></div> <p>We can now initialize and log our mesh.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="n">config</span><span class="p">):</span>
  <span class="bp">...</span>
  <span class="n">axes</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">cfg</span><span class="p">.</span><span class="n">device_config</span><span class="p">.</span><span class="n">n_device_axis</span><span class="p">,)</span>
  <span class="n">axes_name</span> <span class="o">=</span> <span class="p">(</span><span class="sh">"</span><span class="s">dp</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">)</span>

  <span class="n">mesh</span> <span class="o">=</span> <span class="nf">init_devices</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">axes_name</span><span class="p">)</span>
  <span class="nf">log</span><span class="p">(</span><span class="n">mesh</span><span class="p">)</span>
</code></pre></div></div> <h3 id="checkpointing-and-state-management">Checkpointing and State Management</h3> <p>The next step is to setup checkpointing for our model using the <code class="language-plaintext highlighter-rouge">orbax</code> library and the <code class="language-plaintext highlighter-rouge">orbax.checkpointing</code> (ocp) module which conveniently handles checkpointing on multiprocess and remote storage for us. All we need to do is give it the google storage url and make sure to run it on every process. We first make the directory from the config by combining the GCS url with the unique name of the run and setup the orbax checkpoint manager. We can then use this to see if a latest step exists in which case we are loading from a previous run.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">output_dir</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">name</span>
<span class="n">options</span> <span class="o">=</span> <span class="n">ocp</span><span class="p">.</span><span class="nc">CheckpointManagerOptions</span><span class="p">(</span><span class="n">max_to_keep</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">checkpoint_manager</span> <span class="o">=</span> <span class="n">ocp</span><span class="p">.</span><span class="nc">CheckpointManager</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">)</span>
<span class="n">load</span> <span class="o">=</span> <span class="n">checkpoint_manager</span><span class="p">.</span><span class="nf">latest_step</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
</code></pre></div></div> <h3 id="data-partitioning-and-model-initialization">Data Partitioning and Model Initialization</h3> <p>We begin setting up our dataset. We first make the data partition. Every data shard that is loaded will be of the form <code class="language-plaintext highlighter-rouge">(G, M , B, T)</code> where <code class="language-plaintext highlighter-rouge">G</code> is the total batches in a shard, <code class="language-plaintext highlighter-rouge">M</code> are the microbatches in the batch (for pipelining), <code class="language-plaintext highlighter-rouge">B</code> is the batch size per microbatch and <code class="language-plaintext highlighter-rouge">T</code> is the sequence length. Thus we want the <code class="language-plaintext highlighter-rouge">M</code> to be split amongst the pipeline, <code class="language-plaintext highlighter-rouge">B</code> to be split amongst the data and <code class="language-plaintext highlighter-rouge">T</code> to be spilt initially along <code class="language-plaintext highlighter-rouge">Tensor</code> as discussed previously. Thus we obtain the following <code class="language-plaintext highlighter-rouge">PartitionSpec</code> and <code class="language-plaintext highlighter-rouge">NamedSharding</code> and can use it to initialize our dataset class written previously.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_spec</span> <span class="o">=</span> <span class="nc">P</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">dp</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">)</span>
<span class="n">data_partition</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">data_spec</span><span class="p">)</span>

<span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">.</span><span class="nf">getDataset</span><span class="p">(</span>
    <span class="n">cfg</span><span class="p">.</span><span class="n">data_config</span><span class="p">,</span>
    <span class="n">partition</span><span class="o">=</span><span class="n">data_partition</span><span class="p">,</span>
    <span class="n">dp</span><span class="o">=</span><span class="n">DATA_PARALLEL</span><span class="p">,</span>
    <span class="n">pp</span><span class="o">=</span><span class="n">LAYER_PARALLEL</span><span class="p">,</span>
    <span class="n">tp</span><span class="o">=</span><span class="n">TENSOR_PARALLEL</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <p>We can now create our model using the <code class="language-plaintext highlighter-rouge">ShardedModel</code>, creating our init key and and initializing our params.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nf">shardedModel</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_config</span><span class="p">)</span>

<span class="nf">log</span><span class="p">(</span><span class="sh">"</span><span class="s">creating sharded model ...</span><span class="sh">"</span><span class="p">)</span>
<span class="n">key</span><span class="p">,</span> <span class="n">init_key</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">init_weights</span><span class="p">(</span><span class="n">init_key</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</code></pre></div></div> <p>We can now use these params to initialize our optimizer. Since the params are sharded, the optimizer states will be sharded as well.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">optax</span><span class="p">.</span><span class="nf">warmup_cosine_decay_schedule</span><span class="p">(</span>
    <span class="n">init_value</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">lr</span><span class="p">.</span><span class="n">min_lr</span><span class="p">,</span>
    <span class="n">peak_value</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">lr</span><span class="p">.</span><span class="n">max_lr</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">lr</span><span class="p">.</span><span class="n">warmup_steps</span><span class="p">,</span>
    <span class="n">decay_steps</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">lr</span><span class="p">.</span><span class="n">end_steps</span><span class="p">,</span>
    <span class="n">end_value</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">lr</span><span class="p">.</span><span class="n">end_lr</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">tx</span> <span class="o">=</span> <span class="n">optax</span><span class="p">.</span><span class="nf">chain</span><span class="p">(</span>
    <span class="n">optax</span><span class="p">.</span><span class="nf">clip_by_global_norm</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">grad_clip_norm</span><span class="p">),</span>
    <span class="n">optax</span><span class="p">.</span><span class="nf">inject_hyperparams</span><span class="p">(</span><span class="n">optax</span><span class="p">.</span><span class="n">adamw</span><span class="p">)(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">lr_scheduler</span><span class="p">),</span>
<span class="p">)</span>
</code></pre></div></div> <p>One bug that we observed in Optax is that the params with no dims (i.e scalar values) are not replicated across devices leading to errors when trying to reload from checkpoints and use them in distributed functions calls (i.e train step which is written below). Hence we can write a simple map function that says if the value has no dimensions, replicate it across each device.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">default_sharding</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="nc">P</span><span class="p">())</span>
<span class="n">opt_state</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="k">if</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_put</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">default_sharding</span><span class="p">),</span>
    <span class="n">tx</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">params</span><span class="p">),</span>
<span class="p">)</span>
</code></pre></div></div> <p>We can now setup our misc variables such as our starting step, whether to use wandb (has to be enabled in config and process 0), and a placeholder for the id.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">init_step</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">use_wandb</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">wandb</span> <span class="ow">is</span> <span class="bp">True</span> <span class="ow">and</span> <span class="n">jax</span><span class="p">.</span><span class="nf">process_index</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
<span class="n">wandb_id</span> <span class="o">=</span> <span class="bp">None</span>
</code></pre></div></div> <p>We can also write a save-checkpoint function to ensure the PyTree saves. Note we decrement the <code class="language-plaintext highlighter-rouge">shard_idx</code> for the train/val dataset because when loading a shard, we increment by 1, so we want to revert that change.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_save_tree</span><span class="p">(</span><span class="n">step</span><span class="p">):</span>
  <span class="n">model_state</span> <span class="o">=</span> <span class="p">{</span>
      <span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">:</span> <span class="n">params</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">opt_state</span><span class="sh">"</span><span class="p">:</span> <span class="n">opt_state</span><span class="p">,</span>
  <span class="p">}</span>
  <span class="n">save_tree</span> <span class="o">=</span> <span class="p">{</span>
      <span class="sh">"</span><span class="s">state</span><span class="sh">"</span><span class="p">:</span> <span class="n">model_state</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">key</span><span class="sh">"</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_get</span><span class="p">(</span><span class="n">key</span><span class="p">),</span>
      <span class="sh">"</span><span class="s">train_step_idx</span><span class="sh">"</span><span class="p">:</span> <span class="n">train_dataset</span><span class="p">.</span><span class="n">step_idx</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">train_shard_idx</span><span class="sh">"</span><span class="p">:</span> <span class="p">(</span><span class="n">train_dataset</span><span class="p">.</span><span class="n">shard_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">.</span><span class="n">data</span><span class="p">),</span>
      <span class="sh">"</span><span class="s">val_step_idx</span><span class="sh">"</span><span class="p">:</span> <span class="n">val_dataset</span><span class="p">.</span><span class="n">step_idx</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">val_shard_idx</span><span class="sh">"</span><span class="p">:</span> <span class="p">(</span><span class="n">val_dataset</span><span class="p">.</span><span class="n">shard_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nf">len</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">.</span><span class="n">data</span><span class="p">),</span>
      <span class="sh">"</span><span class="s">step</span><span class="sh">"</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span>
  <span class="p">}</span>
  <span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span>
      <span class="sh">"</span><span class="s">wandb_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">wandb_id</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">save_tree</span><span class="p">,</span> <span class="n">metadata</span>
</code></pre></div></div> <p>Our <code class="language-plaintext highlighter-rouge">save_checkpoint</code> function can now just take the step and call the checkpoint manager.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span>
    <span class="n">step</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">save_tree</span><span class="p">,</span> <span class="n">metadata</span> <span class="o">=</span> <span class="nf">make_save_tree</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
    <span class="n">checkpoint_manager</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">ocp</span><span class="p">.</span><span class="n">args</span><span class="p">.</span><span class="nc">Composite</span><span class="p">(</span>
      <span class="n">state</span><span class="o">=</span><span class="n">ocp</span><span class="p">.</span><span class="n">args</span><span class="p">.</span><span class="nc">StandardSave</span><span class="p">(</span><span class="n">save_tree</span><span class="p">),</span>
      <span class="n">metadata</span><span class="o">=</span><span class="n">ocp</span><span class="p">.</span><span class="n">args</span><span class="p">.</span><span class="nc">JsonSave</span><span class="p">(</span><span class="n">metadata</span><span class="p">)</span>
    <span class="p">))</span>
</code></pre></div></div> <p>Before the main training functions or training loop, we should add model-loading logic if we want to resume from a checkpoint. Since we always initialize, we can pass Orbax the sharding and array metadata from the current parameters and use that to load with the correct sharding.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="n">config</span><span class="p">):</span>
    <span class="bp">...</span>
    <span class="k">if</span> <span class="n">load</span><span class="p">:</span>

        <span class="c1"># get PyTree metadata
</span>        <span class="n">abstract_tree_map</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
          <span class="n">ocp</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">to_shape_dtype_struct</span><span class="p">,</span> <span class="nf">make_save_tree</span><span class="p">(</span><span class="n">init_step</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># load checkpoint
</span>        <span class="n">tree</span> <span class="o">=</span> <span class="n">checkpoint_manager</span><span class="p">.</span><span class="nf">restore</span><span class="p">(</span>
            <span class="n">checkpoint_manager</span><span class="p">.</span><span class="nf">latest_step</span><span class="p">(),</span>
            <span class="n">args</span><span class="o">=</span><span class="n">ocp</span><span class="p">.</span><span class="n">args</span><span class="p">.</span><span class="nc">Composite</span><span class="p">(</span>
                <span class="n">state</span><span class="o">=</span><span class="n">ocp</span><span class="p">.</span><span class="n">args</span><span class="p">.</span><span class="nc">StandardRestore</span><span class="p">(</span><span class="n">abstract_tree_state</span><span class="p">),</span>
                <span class="n">metadata</span><span class="o">=</span><span class="n">ocp</span><span class="p">.</span><span class="n">args</span><span class="p">.</span><span class="nc">JsonRestore</span><span class="p">(),</span>
        <span class="p">))</span>

        <span class="c1"># assign all variables
</span>        <span class="n">tree_state</span><span class="p">,</span> <span class="n">tree_metadata</span> <span class="o">=</span> <span class="n">tree</span><span class="p">.</span><span class="n">state</span><span class="p">,</span> <span class="n">tree</span><span class="p">.</span><span class="n">metadata</span>

        <span class="n">init_step</span> <span class="o">=</span> <span class="n">tree_state</span><span class="p">[</span><span class="sh">"</span><span class="s">step</span><span class="sh">"</span><span class="p">]</span>
        <span class="nf">log</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">loading checkpoint @ step </span><span class="si">{</span><span class="n">init_step</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

        <span class="n">key</span><span class="p">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">tree_state</span><span class="p">[</span><span class="sh">"</span><span class="s">key</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">tree_state</span><span class="p">[</span><span class="sh">"</span><span class="s">state</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">opt_state</span> <span class="o">=</span> <span class="n">tree_state</span><span class="p">[</span><span class="sh">"</span><span class="s">state</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">opt_state</span><span class="sh">"</span><span class="p">]</span>

        <span class="n">train_dataset</span><span class="p">.</span><span class="n">step_idx</span> <span class="o">=</span> <span class="n">tree_state</span><span class="p">[</span><span class="sh">"</span><span class="s">train_step_idx</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">train_dataset</span><span class="p">.</span><span class="n">shard_idx</span> <span class="o">=</span> <span class="n">tree_state</span><span class="p">[</span><span class="sh">"</span><span class="s">train_shard_idx</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">train_dataset</span><span class="p">.</span><span class="nf">load_next_shard</span><span class="p">()</span>

        <span class="n">val_dataset</span><span class="p">.</span><span class="n">step_idx</span> <span class="o">=</span> <span class="n">tree_state</span><span class="p">[</span><span class="sh">"</span><span class="s">val_step_idx</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">val_dataset</span><span class="p">.</span><span class="n">shard_idx</span> <span class="o">=</span> <span class="n">tree_state</span><span class="p">[</span><span class="sh">"</span><span class="s">val_shard_idx</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">val_dataset</span><span class="p">.</span><span class="nf">load_next_shard</span><span class="p">()</span>

        <span class="n">wandb_id</span> <span class="o">=</span> <span class="n">tree_metadata</span><span class="p">[</span><span class="sh">"</span><span class="s">wandb_id</span><span class="sh">"</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">use_wandb</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">wandb_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">,</span> <span class="sh">"</span><span class="s">wandb_id is None</span><span class="sh">"</span>
            <span class="n">wandb</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span>
                <span class="n">entity</span><span class="o">=</span><span class="sh">"</span><span class="s">waterloo2</span><span class="sh">"</span><span class="p">,</span>
                <span class="n">project</span><span class="o">=</span><span class="sh">"</span><span class="s">jaxformer</span><span class="sh">"</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">name</span><span class="p">,</span>
                <span class="n">resume</span><span class="o">=</span><span class="sh">"</span><span class="s">must</span><span class="sh">"</span><span class="p">,</span>
                <span class="nb">id</span><span class="o">=</span><span class="n">wandb_id</span><span class="p">,</span>
                <span class="n">config</span><span class="o">=</span><span class="nf">asdict</span><span class="p">(</span><span class="n">cfg</span><span class="p">),</span>
            <span class="p">)</span>
</code></pre></div></div> <p>Otherwise, if we are not loading, we can save the first checkpoint and initialize the wandb run if needed.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="n">config</span><span class="p">):</span>
  <span class="bp">...</span>
  <span class="k">if</span> <span class="n">load</span><span class="p">:</span>
      <span class="bp">...</span>
  <span class="k">else</span><span class="p">:</span>
      <span class="nf">log</span><span class="p">(</span><span class="sh">"</span><span class="s">no checkpoint found, saving init copy</span><span class="sh">"</span><span class="p">)</span>
      <span class="nf">save_checkpoint</span><span class="p">(</span><span class="n">init_step</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">use_wandb</span><span class="p">:</span>
          <span class="n">wandb</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span>
              <span class="n">entity</span><span class="o">=</span><span class="sh">"</span><span class="s">waterloo2</span><span class="sh">"</span><span class="p">,</span>
              <span class="n">project</span><span class="o">=</span><span class="sh">"</span><span class="s">jaxformer</span><span class="sh">"</span><span class="p">,</span>
              <span class="n">name</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">name</span><span class="p">,</span>
              <span class="n">resume</span><span class="o">=</span><span class="sh">"</span><span class="s">allow</span><span class="sh">"</span><span class="p">,</span>
              <span class="n">config</span><span class="o">=</span><span class="nf">asdict</span><span class="p">(</span><span class="n">cfg</span><span class="p">),</span>
          <span class="p">)</span>
          <span class="n">wandb_id</span> <span class="o">=</span> <span class="n">wandb</span><span class="p">.</span><span class="n">run</span><span class="p">.</span><span class="nb">id</span>

</code></pre></div></div> <p>Finally, we can print our parameter count for convenience.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">param_count</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">reduce</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">,</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>
<span class="nf">log</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total parameters: </span><span class="si">{</span><span class="n">param_count</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="training-and-evaluation-loops">Training and Evaluation Loops</h2> <p>Now we can introduce the step functions that call our model. We begin by writing a general step that runs the model forward and returns the loss along with other metrics. Note that we will use communication operations (e.g., <code class="language-plaintext highlighter-rouge">pmean</code>), but since this will ultimately be wrapped under a <code class="language-plaintext highlighter-rouge">shard_map</code>, this is allowed (you cannot call <code class="language-plaintext highlighter-rouge">pmean</code> unless you are under a mesh context, as there is otherwise no information about the distributed setting). Our step function is defined by wrapping <code class="language-plaintext highlighter-rouge">loss_fn</code> in a closure under the training variable.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">train</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
      <span class="bp">...</span>
  <span class="k">return</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
</code></pre></div></div> <p>We can first get the logits from the model by calling <code class="language-plaintext highlighter-rouge">pipe_step</code>, discarding the cache output.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(...):</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">pipe_step</span><span class="p">(</span>
        <span class="n">params</span><span class="p">,</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
        <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div> <p>We can first begin by stepping through. We can use the JAX built in function to turn logits into log-probs and reshape it into a 2D tensor combining all dims other then the distribution into a batch.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(...):</span>
    <span class="bp">...</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">M</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_probs</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">M</span> <span class="o">*</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
</code></pre></div></div> <p>Note that logits is <code class="language-plaintext highlighter-rouge">4D</code> originally since it is a tensor with dimensions defined as microbatch, batches per microbatch, sequence and vocab. We can get our cross-entropy loss by applying a vmap over and selecting the index that using a dynamic slice method, negating and then meaning it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(...):</span>
    <span class="n">loss_idx</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">dynamic_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">idx</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
    <span class="n">loss_cross</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">loss_idx</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">y</span><span class="p">)).</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div> <p>To perform FSDP, we average the loss over the <code class="language-plaintext highlighter-rouge">dp</code> axis. We do the same for the <code class="language-plaintext highlighter-rouge">pp</code> and <code class="language-plaintext highlighter-rouge">tp</code> axes as well, since batches span multiple devices. The <code class="language-plaintext highlighter-rouge">jax.grad</code> function will then handle the reverse communication operations needed to propagate the gradients.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss_cross</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">pmean</span><span class="p">(</span><span class="n">loss_cross</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">"</span><span class="s">dp</span><span class="sh">"</span><span class="p">)</span>
<span class="n">loss_cross</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">pmean</span><span class="p">(</span><span class="n">loss_cross</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">)</span>
<span class="n">loss_cross</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">pmean</span><span class="p">(</span><span class="n">loss_cross</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>We can make a dict of metrics and return that as well. This will be useful when we have to log stats on MoE.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">loss_cross</span><span class="sh">"</span><span class="p">:</span> <span class="n">loss_cross</span><span class="p">,</span>
<span class="p">}</span>
<span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span>
</code></pre></div></div> <p>Now we can get the partition spec of the params, model, key and write the distributed step function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">param_spec</span> <span class="o">=</span> <span class="n">shardedModel</span><span class="p">.</span><span class="nf">get_p_spec</span><span class="p">(</span>
    <span class="p">[</span><span class="n">model</span><span class="p">.</span><span class="n">embedding</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">block</span><span class="p">],</span> <span class="n">mesh</span><span class="p">,</span> <span class="n">cfg</span><span class="p">.</span><span class="n">model_config</span>
<span class="p">)</span>
<span class="n">opt_spec</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">spec</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">)</span>
<span class="n">key_spec</span> <span class="o">=</span> <span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">dp</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>Note each device gets a unique key since we want every operation done on every device to be unique otherwise there is no reason to use more then 1 device. We start by writing the train step. In here, our step function will be the value and grad of step functions previously written since we also want to compute the gradients.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="n">step_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">value_and_grad</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p>Then to allow for gradient accumulation we write a single step function. We take in the past gradients and the batch and then accumulate the grads.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="n">step_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">value_and_grad</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">single_step</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">metrics</span><span class="p">),</span> <span class="n">grads_current</span> <span class="o">=</span> <span class="nf">step_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">batch</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">grads_current</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grads</span><span class="p">,</span> <span class="n">metrics</span>
</code></pre></div></div> <p>We can then initialize the grads and reshape the keys to be PRNG keys again (get rid of leading dims) and then use the <code class="language-plaintext highlighter-rouge">jax.lax.scan</code> function to sequentially loop over the leading dim of the <code class="language-plaintext highlighter-rouge">(x,y,key)</code> batch.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="n">step_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">value_and_grad</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">single_step</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="bp">...</span>
        <span class="k">return</span> <span class="n">grads</span><span class="p">,</span> <span class="n">metrics</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">grad_step</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">grads</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">scan</span><span class="p">(</span>
        <span class="n">single_step</span><span class="p">,</span>
        <span class="n">grads</span><span class="p">,</span>
        <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">key</span><span class="p">),</span>
  <span class="p">)</span>

</code></pre></div></div> <p>We then average the gradients and metrics, apply the updates to the parameters, and return the updated parameters, optimizer state, and metrics. Thus, our final function is as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="n">step_fn</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">value_and_grad</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">single_step</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">metrics</span><span class="p">),</span> <span class="n">grads_current</span> <span class="o">=</span> <span class="nf">step_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">batch</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">grads_current</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grads</span><span class="p">,</span> <span class="n">metrics</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">grad_step</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">grads</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">scan</span><span class="p">(</span>
        <span class="n">single_step</span><span class="p">,</span>
        <span class="n">grads</span><span class="p">,</span>
        <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">key</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">/</span> <span class="n">cfg</span><span class="p">.</span><span class="n">grad_step</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">metrics</span><span class="p">)</span>

    <span class="n">updates</span><span class="p">,</span> <span class="n">opt_state</span> <span class="o">=</span> <span class="n">tx</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">optax</span><span class="p">.</span><span class="nf">apply_updates</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">metrics</span>
</code></pre></div></div> <p>We now can wrap this function under a shard map to allow for the distributed training to occur. For the arguments, we use the spec we have defined throughout the script and the outputs follow the same way. Metrics are replicated across every device.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@jax.jit</span>
<span class="nd">@partial</span><span class="p">(</span>
    <span class="n">jax</span><span class="p">.</span><span class="n">shard_map</span><span class="p">,</span>
    <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">,</span>
    <span class="n">in_specs</span><span class="o">=</span><span class="p">(</span><span class="n">param_spec</span><span class="p">,</span> <span class="n">opt_spec</span><span class="p">,</span> <span class="n">data_spec</span><span class="p">,</span> <span class="n">data_spec</span><span class="p">,</span> <span class="n">key_spec</span><span class="p">),</span>
    <span class="n">out_specs</span><span class="o">=</span><span class="p">(</span><span class="n">param_spec</span><span class="p">,</span> <span class="n">opt_spec</span><span class="p">,</span> <span class="nc">P</span><span class="p">()),</span>
    <span class="n">check_vma</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="bp">...</span>
    <span class="k">return</span> <span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">metrics</span>
</code></pre></div></div> <p>We can similarly write the <code class="language-plaintext highlighter-rouge">eval_step</code> function. The only difference is we don’t have to keep the grads, thus the carry argument of the <code class="language-plaintext highlighter-rouge">jax.lax.scan</code> can be ignored in the <code class="language-plaintext highlighter-rouge">single_step</code> function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@jax.jit</span>
<span class="nd">@partial</span><span class="p">(</span>
    <span class="n">jax</span><span class="p">.</span><span class="n">shard_map</span><span class="p">,</span>
    <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">,</span>
    <span class="n">in_specs</span><span class="o">=</span><span class="p">(</span><span class="n">param_spec</span><span class="p">,</span> <span class="n">data_spec</span><span class="p">,</span> <span class="n">data_spec</span><span class="p">),</span>
    <span class="n">out_specs</span><span class="o">=</span><span class="nc">P</span><span class="p">(),</span>
    <span class="n">check_vma</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">eval_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">single_step</span><span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="nf">step</span><span class="p">(</span>
          <span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">batch</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span>
        <span class="p">)</span>  <span class="c1"># Key does not matter
</span>        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">scan</span><span class="p">(</span><span class="n">single_step</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">metrics</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metrics</span>
</code></pre></div></div> <p>We now define our final variables and sync devices. Note we split the sample key before the loop since we want to have the same random key for each inference to see how the model evolves. We also keep an array to append the training loss and average when we need to print for each eval step.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="n">config</span><span class="p">):</span>
    <span class="bp">...</span>
    <span class="n">total_steps</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">training_steps</span>
    <span class="n">total_tokens</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">.</span><span class="n">tokens_per_step</span>

    <span class="n">jax</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">multihost_utils</span><span class="p">.</span><span class="nf">sync_global_devices</span><span class="p">(</span><span class="sh">"</span><span class="s">sync</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">log</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total steps: </span><span class="si">{</span><span class="n">total_steps</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">log</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total tokens per step: </span><span class="si">{</span><span class="n">total_tokens</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">key</span><span class="p">,</span> <span class="n">sample_key</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># used to keep track of loss and averaged when printing
</span></code></pre></div></div> <p>Our last helper function is to make the keys. Essentially we want to create new keys for each device for a total of our grad steps. Therefore, we can make this a param and re-jit the function for each new value since it must be a static parameter. This doesn’t slow us down since this function is called and compiled once.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnames</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">steps</span><span class="sh">"</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">make_sharded_key</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span>
      <span class="n">key</span><span class="p">,</span> <span class="n">DATA_PARALLEL</span> <span class="o">*</span> <span class="n">LAYER_PARALLEL</span> <span class="o">*</span> <span class="n">TENSOR_PARALLEL</span> <span class="o">*</span> <span class="n">steps</span>
    <span class="p">)</span> <span class="c1"># python array currently make it into a jax array
</span>    <span class="n">key</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">key</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span>
      <span class="p">(</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">LAYER_PARALLEL</span><span class="p">,</span> <span class="n">TENSOR_PARALLEL</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">key</span>
</code></pre></div></div> <p>Then, the final training loop can be written. We start by splitting our key and then making our train keys, getting our data and finally calling the train step.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="n">config</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">current_step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">init_step</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">):</span>
        <span class="n">key</span><span class="p">,</span> <span class="n">train_key</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">train_key</span> <span class="o">=</span> <span class="nf">make_sharded_key</span><span class="p">(</span><span class="n">train_key</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">grad_step</span><span class="p">)</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">train_dataset</span><span class="p">(</span><span class="n">step</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">grad_step</span><span class="p">)</span>

        <span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_key</span><span class="p">)</span>
        <span class="n">train_loss</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div> <p>We then add wandb logging metrics and add in our eval step.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="n">config</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">current_step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">init_step</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">):</span>
        <span class="bp">...</span>
        <span class="k">if</span> <span class="n">use_wandb</span><span class="p">:</span>
            <span class="n">wandb_log</span> <span class="o">=</span> <span class="p">{</span>
                <span class="sh">"</span><span class="s">step</span><span class="sh">"</span><span class="p">:</span> <span class="n">current_step</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">loss/train_loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">],</span>
                <span class="sh">"</span><span class="s">loss/train_cross_entropy_loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss_cross</span><span class="sh">"</span><span class="p">],</span>
                <span class="sh">"</span><span class="s">lr</span><span class="sh">"</span><span class="p">:</span> <span class="n">opt_state</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">hyperparams</span><span class="p">[</span><span class="sh">"</span><span class="s">learning_rate</span><span class="sh">"</span><span class="p">],</span>
            <span class="p">}</span>

            <span class="k">if</span> <span class="n">current_step</span> <span class="o">%</span> <span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">time_per_batch</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
                <span class="n">eval_x</span><span class="p">,</span> <span class="n">eval_y</span> <span class="o">=</span> <span class="nf">val_dataset</span><span class="p">(</span><span class="n">step</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">eval_steps</span><span class="p">)</span>
                <span class="n">val_metrics</span> <span class="o">=</span> <span class="nf">eval_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">eval_x</span><span class="p">,</span> <span class="n">eval_y</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">use_wandb</span><span class="p">:</span>
                    <span class="n">wandb_log</span><span class="p">[</span><span class="sh">"</span><span class="s">loss/val_loss</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">val_metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">]</span>
                    <span class="n">wandb_log</span><span class="p">[</span><span class="sh">"</span><span class="s">loss/val_cross_entropy_loss</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">val_metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss_cross</span><span class="sh">"</span><span class="p">]</span>

                <span class="n">jax</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">multihost_utils</span><span class="p">.</span><span class="nf">sync_global_devices</span><span class="p">(</span><span class="sh">"</span><span class="s">sync</span><span class="sh">"</span><span class="p">)</span>

                <span class="n">tokens_per_second</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_steps</span> <span class="o">*</span> <span class="n">total_tokens</span> <span class="o">/</span> <span class="n">time_per_batch</span>
                <span class="n">train_loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">train_loss</span><span class="p">).</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
                <span class="n">eval_loss</span> <span class="o">=</span> <span class="n">val_metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span>
                <span class="n">log_string</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Step </span><span class="si">{</span><span class="n">current_step</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, Loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, Eval Loss: </span><span class="si">{</span><span class="n">eval_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, tk/s: </span><span class="si">{</span><span class="n">tokens_per_second</span><span class="si">:</span><span class="p">,.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span>
                <span class="nf">log</span><span class="p">(</span><span class="n">log_string</span><span class="p">)</span>
</code></pre></div></div> <p>To avoid slowdown we can checkpoint every 10 eval steps. We can also include checkpointing to get the final training loop.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="n">config</span><span class="p">):</span>
    <span class="bp">...</span>
    <span class="k">for</span> <span class="n">current_step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">init_step</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">):</span>
        <span class="n">key</span><span class="p">,</span> <span class="n">train_key</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">train_key</span> <span class="o">=</span> <span class="nf">make_sharded_key</span><span class="p">(</span><span class="n">train_key</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">grad_step</span><span class="p">)</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">train_dataset</span><span class="p">(</span><span class="n">step</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">grad_step</span><span class="p">)</span>

        <span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_key</span><span class="p">)</span>
        <span class="n">train_loss</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">use_wandb</span><span class="p">:</span>
            <span class="n">wandb_log</span> <span class="o">=</span> <span class="p">{</span>
                <span class="sh">"</span><span class="s">step</span><span class="sh">"</span><span class="p">:</span> <span class="n">current_step</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">loss/train_loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">],</span>
                <span class="sh">"</span><span class="s">loss/train_cross_entropy_loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss_cross</span><span class="sh">"</span><span class="p">],</span>
                <span class="sh">"</span><span class="s">lr</span><span class="sh">"</span><span class="p">:</span> <span class="n">opt_state</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">hyperparams</span><span class="p">[</span><span class="sh">"</span><span class="s">learning_rate</span><span class="sh">"</span><span class="p">],</span>
            <span class="p">}</span>

        <span class="k">if</span> <span class="n">current_step</span> <span class="o">%</span> <span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">time_per_batch</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
            <span class="n">eval_x</span><span class="p">,</span> <span class="n">eval_y</span> <span class="o">=</span> <span class="nf">val_dataset</span><span class="p">(</span><span class="n">step</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">eval_steps</span><span class="p">)</span>
            <span class="n">val_metrics</span> <span class="o">=</span> <span class="nf">eval_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">eval_x</span><span class="p">,</span> <span class="n">eval_y</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">use_wandb</span><span class="p">:</span>
                <span class="n">wandb_log</span><span class="p">[</span><span class="sh">"</span><span class="s">loss/val_loss</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">val_metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">]</span>
                <span class="n">wandb_log</span><span class="p">[</span><span class="sh">"</span><span class="s">loss/val_cross_entropy_loss</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">val_metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss_cross</span><span class="sh">"</span><span class="p">]</span>

            <span class="n">jax</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">multihost_utils</span><span class="p">.</span><span class="nf">sync_global_devices</span><span class="p">(</span><span class="sh">"</span><span class="s">sync</span><span class="sh">"</span><span class="p">)</span>

            <span class="n">tokens_per_second</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_steps</span> <span class="o">*</span> <span class="n">total_tokens</span> <span class="o">/</span> <span class="n">time_per_batch</span>
            <span class="n">train_loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">train_loss</span><span class="p">).</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
            <span class="n">eval_loss</span> <span class="o">=</span> <span class="n">val_metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span>
            <span class="n">log_string</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Step </span><span class="si">{</span><span class="n">current_step</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, Loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, Eval Loss: </span><span class="si">{</span><span class="n">eval_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, tk/s: </span><span class="si">{</span><span class="n">tokens_per_second</span><span class="si">:</span><span class="p">,.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span>
            <span class="nf">log</span><span class="p">(</span><span class="n">log_string</span><span class="p">)</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
            <span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="n">current_step</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">save_checkpoint</span><span class="p">(</span><span class="n">current_step</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_wandb</span><span class="p">:</span>
            <span class="n">wandb</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">wandb_log</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">current_step</span><span class="p">)</span>
</code></pre></div></div> <p>Finally, we end the main function by calling <code class="language-plaintext highlighter-rouge">wandb.finish()</code> if we are using wandb. To kick off training, we can add a main guard that called <code class="language-plaintext highlighter-rouge">jax.distrbuted.intialize()</code> to sync the multi-controller processes and print the <code class="language-plaintext highlighter-rouge">cfg</code> from the <code class="language-plaintext highlighter-rouge">parse_args()</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">jax</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="nf">initialize</span><span class="p">()</span>
    <span class="n">cfg</span> <span class="o">=</span> <span class="nf">parse_args</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">__dict__</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="k">lambda</span> <span class="n">o</span><span class="p">:</span> <span class="n">o</span><span class="p">.</span><span class="n">__dict__</span><span class="p">))</span>
    <span class="nf">main</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
</code></pre></div></div> <p>We now look at how to scale this model further with MoE.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/main.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'jindal013/jaxformer-website-v2',
        'data-repo-id': 'R_kgDOPoEVEA',
        'data-category': 'General',
        'data-category-id': 'DIC_kwDOPoEVEM4Cu2n4',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-loading': '1',
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>