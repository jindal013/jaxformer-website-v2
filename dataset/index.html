<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Dataset Class and Config Files | Jaxformer: Scaling Modern Transformers </title> <meta name="author" content=" "> <meta name="description" content="When training large-scale models on TPU or GPU clusters, memory-efficient data loading is needed to avoid bottlenecks. Below is a walkthrough of a custom Dataset class designed to stream and preprocess data shards from a Google Cloud Storage Bucket, supported for data, pipeline and tensor parallelism."> <meta name="keywords" content="jscaling, jax, llms, transformers, tpus, google, cloud, parallelism, distributed"> <meta property="og:title" content="JAXformer: Scaling Modern Transformers"> <meta property="og:description" content="A zero-to-one guide on scaling modern transformers with n-dimensional parallelism."> <meta property="og:image" content="https://jaxformer.com/assets/img/banner.png"> <meta property="og:url" content="https://jaxformer.com"> <meta property="og:type" content="website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/jaxformer-icon.png?7001ddef15419e25335b33b49c6ce725"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jaxformer.com/dataset/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> <script async src="https://www.googletagmanager.com/gtag/js?id=G-G878LK8JDJ"></script> <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-G878LK8JDJ');
</script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Dataset Class and Config Files",
            "description": "When training large-scale models on TPU or GPU clusters, memory-efficient data loading is needed to avoid bottlenecks. Below is a walkthrough of a custom Dataset class designed to stream and preprocess data shards from a Google Cloud Storage Bucket, supported for data, pipeline and tensor parallelism.",
            "published": "September 06, 2025",
            "authors": [
              
              {
                "author": "Aditya Makkar",
                "authorURL": "https://x.com/AdityaMakkar000"
              },
              
              {
                "author": "Divya Makkar",
                "authorURL": "https://x.com/_DivyaMakkar"
              },
              
              {
                "author": "Chinmay Jindal",
                "authorURL": "https://x.com/chinmayjindal_"
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <script>
    function goToTop() {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    }

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
      // Get the button:
      let mybutton = document.getElementById("top-button");

      if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
  }
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="d-none d-sm-inline">Jaxformer: Scaling Modern Transformers</span> <span class="d-inline d-sm-none"> Jaxformer </span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="left-button section-button"><a href="../sharded"><svg viewbox="-78.5 0 512 512"><path d="M257 64L291 98 128 262 291 426 257 460 61 262 257 64Z"></path></svg></a></div> <div class="right-button section-button"><a href="../distributed_training"><svg viewbox="-78.5 0 512 512"><path d="M98 460L64 426 227 262 64 98 98 64 294 262 98 460Z"></path></svg></a></div> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item nav-hidden"><a class="nav-link" onclick="goToTop()" id="top-button" style="display: none;">Back to Top</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../sharded">Previous Part</a></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../distributed_training">Next Part</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Sections </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/">Part 0. Introduction</a> <a class="dropdown-item " href="/tokenization/">Part 1. Tokenization</a> <a class="dropdown-item " href="/base_model/">Part 2. Base Model</a> <a class="dropdown-item " href="/sharded/">Part 3. Sharded Model</a> <a class="dropdown-item " href="/dataset/">Part 4. Dataset &amp; Config</a> <a class="dropdown-item " href="/distributed_training/">Part 5. Distributed Training</a> <a class="dropdown-item " href="/moe/">Part 6. Mixture of Experts</a> <a class="dropdown-item " href="/training/">Part 7. Training Results</a> <a class="dropdown-item " href="/conclusion/">Part 8. Conclusion</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Dataset Class and Config Files</h1> <p>Part 4 of <a href="">Jaxformer</a> (<a href="../sharded">Part 3: Sharded Model</a> | <a href="../distributed_training">Part 5: Distributed Training</a>)</p> <p>When training large-scale models on TPU or GPU clusters, memory-efficient data loading is needed to avoid bottlenecks. Below is a walkthrough of a custom Dataset class designed to stream and preprocess data shards from a Google Cloud Storage Bucket, supported for data, pipeline and tensor parallelism.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-dataset-class">The Dataset Class</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#initialization-and-setup">Initialization and Setup</a> </li> <li> <a href="#downloading-shards">Downloading Shards</a> </li> <li> <a href="#processing-shards">Processing Shards</a> </li> <li> <a href="#iteration-and-utilities">Iteration and Utilities</a> </li> </ul> <div> <a href="#configs">Configs</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#config-classes">Config Classes</a> </li> <li> <a href="#argument-parsing-and-wrapper">Argument Parsing and Wrapper</a> </li> </ul> </nav> </d-contents> <h2 id="the-dataset-class">The Dataset Class</h2> <h3 id="initialization-and-setup">Initialization and Setup</h3> <p>Beginning with the <code class="language-plaintext highlighter-rouge">Dataset</code> constructor, a <code class="language-plaintext highlighter-rouge">process_path</code> variable is declared as it will store the location of a shardâ€™s download from the GC Bucket.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dataset</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">process_path</span> <span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">T</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">microbatch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dp</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">pp</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">bucket_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="nb">id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">partition</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">NamedSharding</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">):</span>
</code></pre></div></div> <p>Then, the following assert statements are declared to ensure a reshaping can occur. For pipeline parallelism, the <code class="language-plaintext highlighter-rouge">batch_size</code> must divide into the <code class="language-plaintext highlighter-rouge">micro_batch</code> size and pipeline parallelism dimension must divide the <code class="language-plaintext highlighter-rouge">micro_batch</code> size.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">assert </span><span class="p">(</span><span class="n">batch_size</span> <span class="o">%</span> <span class="n">microbatch</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
<span class="nf">assert </span><span class="p">(</span><span class="n">microbatch</span> <span class="o">%</span> <span class="n">pp</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
</code></pre></div></div> <p>Other properties are also initialized, some noteworthy ones include <code class="language-plaintext highlighter-rouge">self.shard_idx</code>, <code class="language-plaintext highlighter-rouge">self.step_idx</code> and <code class="language-plaintext highlighter-rouge">self.id</code> which track the GCP shard to be streamed, the current training step index and the current data splitâ€™s folder name (eg. <code class="language-plaintext highlighter-rouge">train</code>) respectively.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dataset</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init_</span><span class="p">(...):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">T</span>
        <span class="n">self</span><span class="p">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dp</span> <span class="o">=</span> <span class="n">dp</span>
        <span class="n">self</span><span class="p">.</span><span class="n">microbatch</span> <span class="o">=</span> <span class="n">microbatch</span>

        <span class="n">self</span><span class="p">.</span><span class="n">step_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">shard_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">partition</span> <span class="o">=</span> <span class="n">partition</span>

        <span class="n">self</span><span class="p">.</span><span class="n">bucket_name</span> <span class="o">=</span> <span class="n">bucket_name</span>
        <span class="n">self</span><span class="p">.</span><span class="n">base_process_path</span> <span class="o">=</span> <span class="n">process_path</span>
        <span class="n">self</span><span class="p">.</span><span class="n">process_path</span> <span class="o">=</span> <span class="n">process_path</span>
        <span class="n">self</span><span class="p">.</span><span class="nb">id</span> <span class="o">=</span> <span class="nb">id</span>
        <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">return_blobs</span><span class="p">(</span><span class="n">bucket_name</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="nb">id</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dir_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">bucket_downloads</span><span class="sh">"</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">os</span><span class="p">.</span><span class="nf">mkdir</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">dir_name</span><span class="p">)</span>
        <span class="k">except</span> <span class="nb">OSError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">dir_name</span><span class="si">}</span><span class="s"> already exists</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="downloading-shards">Downloading Shards</h3> <p>Another important instantiation is the <code class="language-plaintext highlighter-rouge">self.data</code> variable which holds a list of names containing the shards to be downloaded. The <code class="language-plaintext highlighter-rouge">bucket_name</code> and <code class="language-plaintext highlighter-rouge">self.id</code> (folder name) are taken as parameters and return a list containing all the names in the GCP bucket with the prefix identifier. Due to this, the folder name is also included which is why the first index in the resulting list is excluded.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dataset</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(...):</span>
        <span class="bp">...</span>

    <span class="k">def</span> <span class="nf">return_blobs</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">bucket_name</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">storage_client</span> <span class="o">=</span> <span class="n">storage</span><span class="p">.</span><span class="nc">Client</span><span class="p">()</span>
        <span class="n">blobs</span> <span class="o">=</span> <span class="n">storage_client</span><span class="p">.</span><span class="nf">list_blobs</span><span class="p">(</span><span class="n">bucket_name</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="n">delimiter</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">blob</span> <span class="ow">in</span> <span class="n">blobs</span><span class="p">:</span>
            <span class="n">res</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">blob</span><span class="p">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
</code></pre></div></div> <p>Then, the process for downloading begins by calling the <code class="language-plaintext highlighter-rouge">load_next_shard()</code> function, which operates using the following 3 functions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dataset</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(...):</span>
        <span class="bp">...</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">load_next_shard</span><span class="p">()</span>
</code></pre></div></div> <p>There are three functions that download a shard of data from the GCP bucket. The first is shown below and streams a file with a specific name from the GCP bucket.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dataset</span><span class="p">:</span>
    <span class="bp">...</span>
    <span class="k">def</span> <span class="nf">download_blob_to_stream</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">bucket_name</span><span class="p">,</span> <span class="n">source_blob_name</span><span class="p">,</span> <span class="n">file_obj</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Downloads a blob to a stream or other file-like object.</span><span class="sh">"""</span>
        <span class="n">storage_client</span> <span class="o">=</span> <span class="n">storage</span><span class="p">.</span><span class="nc">Client</span><span class="p">()</span>
        <span class="n">bucket</span> <span class="o">=</span> <span class="n">storage_client</span><span class="p">.</span><span class="nf">bucket</span><span class="p">(</span><span class="n">bucket_name</span><span class="p">)</span>

        <span class="n">blob</span> <span class="o">=</span> <span class="n">bucket</span><span class="p">.</span><span class="nf">blob</span><span class="p">(</span><span class="n">source_blob_name</span><span class="p">)</span>
        <span class="n">blob</span><span class="p">.</span><span class="nf">download_to_file</span><span class="p">(</span><span class="n">file_obj</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Downloaded blob </span><span class="si">{</span><span class="n">source_blob_name</span><span class="si">}</span><span class="s"> to file-like object.</span><span class="sh">"</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">file_obj</span>
</code></pre></div></div> <p>The second function is wrapper around the first function. If the call to <code class="language-plaintext highlighter-rouge">download_blob_to_stream</code> is successful, then the result is returned, else the function is re-called after a 5 second wait.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dataset</span><span class="p">:</span>
    <span class="bp">...</span>
    <span class="k">def</span> <span class="nf">download_bucket</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">bucket_name</span><span class="p">,</span> <span class="n">source_name</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">download_blob_to_stream</span><span class="p">(</span><span class="n">bucket_name</span><span class="p">,</span> <span class="n">source_name</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">result</span>
            <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nf">log</span><span class="p">(</span><span class="sh">"</span><span class="s">Failed to download due to exception</span><span class="sh">"</span><span class="p">)</span>
                <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div> <p>Note the <code class="language-plaintext highlighter-rouge">log</code> function is a simple way to ensure only one device is logging the download as stated by <code class="language-plaintext highlighter-rouge">jax.process_index() == 0</code>, instead of all devices printing the same message.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="n">out</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">jax</span><span class="p">.</span><span class="nf">process_index</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Dataset</span><span class="p">:</span>
    <span class="bp">...</span>
</code></pre></div></div> <p>Lastly, <code class="language-plaintext highlighter-rouge">download_next</code> is the main function that executes the downloading. It creates a <code class="language-plaintext highlighter-rouge">source_name</code> by iterating through the <code class="language-plaintext highlighter-rouge">self.data</code> array with all the names of the files in the GCP bucket using the <code class="language-plaintext highlighter-rouge">shard_idx</code>. Then, a unique process path is created using the <code class="language-plaintext highlighter-rouge">shard_idx</code> and the file with <code class="language-plaintext highlighter-rouge">source_name</code> is downloaded.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dataset</span><span class="p">:</span>
    <span class="bp">...</span>
    <span class="k">def</span> <span class="nf">download_next</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">log</span><span class="p">(</span><span class="sh">"</span><span class="s">Started downloading</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">source_name</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">shard_idx</span> <span class="o">%</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">)]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">shard_idx</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nf">log</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s"> Downloading: </span><span class="si">{</span><span class="n">source_name</span><span class="si">}</span><span class="s"> | Shard_idx: </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">shard_idx</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">process_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">base_process_path</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="nb">id</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">shard_idx</span><span class="si">}</span><span class="sh">"</span>
        <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">process_path</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">download_bucket</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bucket_name</span><span class="p">,</span> <span class="n">source_name</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
            <span class="nf">log</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Done downloading </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="processing-shards">Processing Shards</h3> <p>When the <code class="language-plaintext highlighter-rouge">load_next_shard()</code> function is called, it calls <code class="language-plaintext highlighter-rouge">self.download_next()</code> which was explained above. Once the shard has been downloaded, it must be processed - rearranged to accommodate the batch size and mini batch sizes for data/pipeline parallelism, and reshaped into the x and y datasets. This is done with the <code class="language-plaintext highlighter-rouge">process_prev</code> function which begins by using <code class="language-plaintext highlighter-rouge">np.load(self.process_path)</code> to load the <code class="language-plaintext highlighter-rouge">.npy</code> shard that was downloaded to the <code class="language-plaintext highlighter-rouge">self.process_path</code> to a numpy array called <code class="language-plaintext highlighter-rouge">data</code>. The features for the dataset are loaded started from the beginning of the data array, leaving out the last index. The labels start from the first index (note the data is 0-indexed) till the end of the array. The reason why the labels is shifted one value is due the nature of predicting the next token. For the 0th token of data, the next token to be predicted is the 1st index, hence the reason why the features stop at the <code class="language-plaintext highlighter-rouge">[:-1]</code> index as the last token is the predictor for the second last token.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dataset</span><span class="p">:</span>
    <span class="bp">...</span>
    <span class="k">def</span> <span class="nf">load_next_shard</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">download_next</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">process_prev</span><span class="p">():</span>
            <span class="nf">log</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Processing shard at </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">process_path</span><span class="si">}</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">)</span>

            <span class="k">try</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">process_path</span><span class="p">)</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="nf">log</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">couldn</span><span class="sh">'</span><span class="s">t load data</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">self</span><span class="p">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

</code></pre></div></div> <p>Now, at this stage, both the dataset and labels are reshaped to align with distributed training. The process begins by determining the total number of usable training samples(<code class="language-plaintext highlighter-rouge">len_dataset</code>) and calculating the maximum number of complete batches that can be formed. The dataset and corresponding labels are then trimmed and reshaped into a four-dimensional tensor of shape: $(\text{max_batches},\; \text{microbatch},\; \tfrac{dp \times \text{batch_size}}{\text{microbatch}},\; T)$ where <code class="language-plaintext highlighter-rouge">dp</code> is the number of data parallel instances, and <code class="language-plaintext highlighter-rouge">microbatch</code> is the number of microbatches per instance, the next term is the number of samples per microbatch and T is the sequence length. This structure ensures the data can be cleanly partitioned across multiple devices and supports microbatch based grad accumulation allowing for efficient JAX sharding and device transfer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_next_shard</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="bp">...</span>
    <span class="k">def</span> <span class="nf">process_prev</span><span class="p">():</span>
        <span class="bp">...</span>
        <span class="n">len_dataset</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">max_batches</span> <span class="o">=</span> <span class="n">len_dataset</span> <span class="o">//</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">[:</span><span class="n">max_batches</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">dp</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span>
                <span class="n">max_batches</span><span class="p">,</span>
                <span class="n">self</span><span class="p">.</span><span class="n">microbatch</span><span class="p">,</span>
                <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">dp</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">microbatch</span><span class="p">,</span>
                <span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">labels</span><span class="p">[</span>
            <span class="p">:</span> <span class="n">max_batches</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">dp</span>
        <span class="p">].</span><span class="nf">reshape</span><span class="p">(</span>
            <span class="n">max_batches</span><span class="p">,</span>
            <span class="n">self</span><span class="p">.</span><span class="n">microbatch</span><span class="p">,</span>
            <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">dp</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">microbatch</span><span class="p">,</span>
            <span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></div> <p>In JAX, sharding refers to dividing an array across multiple devices, typically described using a <code class="language-plaintext highlighter-rouge">NamedSharding</code> object. This specifies how array dimensions should be partitioned across a device mesh (e.g., along data, pipeline, or tensor axes). In the code, the dataset and labels are placed on devices using <code class="language-plaintext highlighter-rouge">jax.device_put</code> with the given sharding specification. This ensures that each device receives only the portion of the data it is responsible for, rather than creating one large array and letting JAX scatter it afterward, saving memory and communication costs in the process. The process function is called, and the path is removed after.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dataset</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">load_next_shard</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">process_prev</span><span class="p">():</span>
            <span class="bp">...</span>
            <span class="n">self</span><span class="p">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_put</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">partition</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_put</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">labels</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">partition</span><span class="p">)</span>

        <span class="nf">process_prev</span><span class="p">()</span>

        <span class="n">os</span><span class="p">.</span><span class="nf">remove</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">process_path</span><span class="p">)</span>
</code></pre></div></div> <h3 id="iteration-and-utilities">Iteration and Utilities</h3> <p>Additionally, within the <code class="language-plaintext highlighter-rouge">Dataset</code> class, we have the length function which returns the number of batches available in the current loaded shard (0th dimension). Additionally, the <code class="language-plaintext highlighter-rouge">__call__</code> method is used to fetch the next batch of inputs and labels sequentially. The <code class="language-plaintext highlighter-rouge">step_idx</code> variable increments each call and if the index exceeds all the batches in the current shard, it means we have exceeded all the batches and we can reset the idx to 0 and load the next shard. A batch is extracted by slicing the dataset labels from <code class="language-plaintext highlighter-rouge">step_idx : step_idx + step</code>. This provides exactly step samples, which aids in the implementation of gradient accumulation. Finally, <code class="language-plaintext highlighter-rouge">step_idx</code> is incremented by <code class="language-plaintext highlighter-rouge">step</code>, so that the next call fetches the following batch. This creates a continuous stream of batches across shards.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dataset</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">step_idx</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">step_idx</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">load_next_shard</span><span class="p">()</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">step_idx</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">step_idx</span> <span class="o">+</span> <span class="n">step</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">labels</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">step_idx</span> <span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">step_idx</span> <span class="o">+</span> <span class="n">step</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">step_idx</span> <span class="o">+=</span> <span class="n">step</span>

        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
</code></pre></div></div> <p>We can add a few more utility functions to create dataset from a dataset config as well as some properties listed below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dataset</span>
    <span class="bp">...</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">getDataset</span><span class="p">(</span>
        <span class="n">cls</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">dataConfig</span><span class="p">,</span>
        <span class="n">partition</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">NamedSharding</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">dp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">pp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">tp</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="sh">"</span><span class="s">Dataset</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Dataset</span><span class="sh">"</span><span class="p">]:</span>
        <span class="nf">assert </span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">T</span> <span class="o">%</span> <span class="n">tp</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">T should be divisible by tensor parallelism</span><span class="sh">"</span>
        <span class="n">train_dataset</span> <span class="o">=</span> <span class="nf">cls</span><span class="p">(</span>
            <span class="n">cfg</span><span class="p">.</span><span class="n">process_path</span><span class="p">,</span>
            <span class="n">cfg</span><span class="p">.</span><span class="n">T</span><span class="p">,</span>
            <span class="n">cfg</span><span class="p">.</span><span class="n">train_batch_size</span><span class="p">,</span>
            <span class="n">cfg</span><span class="p">.</span><span class="n">micro_batch_size</span><span class="p">,</span>
            <span class="n">partition</span><span class="o">=</span><span class="n">partition</span><span class="p">,</span>
            <span class="n">dp</span><span class="o">=</span><span class="n">dp</span><span class="p">,</span>
            <span class="n">pp</span><span class="o">=</span><span class="n">pp</span><span class="p">,</span>
            <span class="n">bucket_name</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">bucket_name</span><span class="p">,</span>
            <span class="nb">id</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">train_folder_name</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">val_dataset</span> <span class="o">=</span> <span class="nf">cls</span><span class="p">(</span>
            <span class="n">cfg</span><span class="p">.</span><span class="n">process_path</span><span class="p">,</span>
            <span class="n">cfg</span><span class="p">.</span><span class="n">T</span><span class="p">,</span>
            <span class="n">cfg</span><span class="p">.</span><span class="n">val_batch_size</span><span class="p">,</span>
            <span class="n">cfg</span><span class="p">.</span><span class="n">micro_batch_size</span><span class="p">,</span>
            <span class="n">partition</span><span class="o">=</span><span class="n">partition</span><span class="p">,</span>
            <span class="n">dp</span><span class="o">=</span><span class="n">dp</span><span class="p">,</span>
            <span class="n">pp</span><span class="o">=</span><span class="n">pp</span><span class="p">,</span>
            <span class="n">bucket_name</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">bucket_name</span><span class="p">,</span>
            <span class="nb">id</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">val_folder_name</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">tokens_per_step</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">dp</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">T</span>
</code></pre></div></div> <p>More advanced data loading techniques can be used such as disturbed data loading however we are able to bypass this and use this dataloader on a multi-node setting since the data chunks are in shards and thus it is still efficient for every process to download duplicate data.</p> <h2 id="configs">Configs</h2> <h3 id="config-classes">Config Classes</h3> <p>Here are the configs found in the <code class="language-plaintext highlighter-rouge">utils.py</code>. Beginning with the different config classes, they are configured for the model, dataset processing, learning rate/optimizer configs, device config for distributed training and inference config respectively.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">modelConfig</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">model config class</span><span class="sh">"""</span>

    <span class="n">model_dimension</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">blocks</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">layers_per_block</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">T</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dhR</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">bfloat16</span><span class="sh">"</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">dataConfig</span><span class="p">:</span>
    <span class="n">bucket_name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">process_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./bucket_downloads/processShard</span><span class="sh">"</span>
    <span class="n">train_folder_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">train</span><span class="sh">"</span>
    <span class="n">val_folder_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">val</span><span class="sh">"</span>
    <span class="n">T</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span>
    <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">val_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">micro_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">LRConfig</span><span class="p">:</span>
    <span class="n">max_lr</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">min_lr</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">end_lr</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">end_steps</span><span class="p">:</span> <span class="nb">int</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">deviceConfig</span><span class="p">:</span>
    <span class="n">n_device_axis</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">inferenceConfig</span><span class="p">:</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">n_devices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="n">use_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">config</span><span class="p">:</span>
    <span class="n">model_config</span><span class="p">:</span> <span class="n">modelConfig</span>
    <span class="n">data_config</span><span class="p">:</span> <span class="n">dataConfig</span>
    <span class="n">lr</span><span class="p">:</span> <span class="n">LRConfig</span>
    <span class="n">device_config</span><span class="p">:</span> <span class="n">deviceConfig</span>
    <span class="n">inference_config</span><span class="p">:</span> <span class="n">inferenceConfig</span>
    <span class="n">output_dir</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">training_steps</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">grad_step</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span>
    <span class="n">checkpoint_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">eval_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">25</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">wandb</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">grad_clip_norm</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
</code></pre></div></div> <h3 id="argument-parsing-and-wrapper">Argument Parsing and Wrapper</h3> <p>Then, the <code class="language-plaintext highlighter-rouge">parse_args()</code> function is designed to parse command line arguments in regards to the model call.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">parse_args</span><span class="p">():</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="p">.</span><span class="nc">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">model training</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--model_dimension</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">768</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--vocab_size</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">50304</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--n_head</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--blocks</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--layers_per_block</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--T</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--latent_dim</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--dhR</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--dropout_rate</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--model_dtype</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="sh">"</span><span class="s">bfloat16</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--k</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--n_experts</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--n_shared</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--capacity_factor</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--bucket_name</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="sh">"</span><span class="s">10bt_gpt2</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">--process_path</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="sh">"</span><span class="s">./bucket_downloads/processShard</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--train_folder_name</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--val_folder_name</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="sh">"</span><span class="s">val</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--train_batch_size</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--val_batch_size</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--micro_batch_size</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--max_lr</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">6e-4</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--min_lr</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--end_lr</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">6e-5</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--warmup_steps</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">715</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--end_steps</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">19073</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--alpha</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--name</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--output_dir</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="sh">"</span><span class="s">gs://results_jaxformer/</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--checkpoint_steps</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--seed</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--wandb</span><span class="sh">"</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="sh">"</span><span class="s">store_true</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--training_steps</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">20000</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--grad_step</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--eval_steps</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--grad_clip_norm</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--n_device_axis</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">nargs</span><span class="o">=</span><span class="sh">"</span><span class="s">*</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--inference_batch</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--top_k</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--temperature</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--use_cache</span><span class="sh">"</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="sh">"</span><span class="s">store_true</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--max_tokens</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--prompt</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="sh">"</span><span class="s">hello world</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--n_devices</span><span class="sh">"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse_args</span><span class="p">()</span>
</code></pre></div></div> <p>Then all the individual config classes are instantiated.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">parse_args</span><span class="p">():</span>
    <span class="bp">...</span>
    <span class="n">model_cfg</span> <span class="o">=</span> <span class="nf">modelConfig</span><span class="p">(</span>
        <span class="n">model_dimension</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span>
        <span class="n">n_head</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span>
        <span class="n">blocks</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">blocks</span><span class="p">,</span>
        <span class="n">layers_per_block</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">layers_per_block</span><span class="p">,</span>
        <span class="n">T</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">T</span><span class="p">,</span>
        <span class="n">latent_dim</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">,</span>
        <span class="n">dhR</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">dhR</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
        <span class="n">model_dtype</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">,</span>
        <span class="n">k</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">k</span><span class="p">,</span>
        <span class="n">n_experts</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">n_experts</span><span class="p">,</span>
        <span class="n">n_shared</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">n_shared</span><span class="p">,</span>
        <span class="n">capacity_factor</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">capacity_factor</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">data_cfg</span> <span class="o">=</span> <span class="nf">dataConfig</span><span class="p">(</span>
        <span class="n">bucket_name</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">bucket_name</span><span class="p">,</span>
        <span class="n">process_path</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">process_path</span><span class="p">,</span>
        <span class="n">train_folder_name</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">train_folder_name</span><span class="p">,</span>
        <span class="n">val_folder_name</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">val_folder_name</span><span class="p">,</span>
        <span class="n">T</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">T</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">train_batch_size</span><span class="p">,</span>
        <span class="n">val_batch_size</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">val_batch_size</span><span class="p">,</span>
        <span class="n">micro_batch_size</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">micro_batch_size</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">lr_cfg</span> <span class="o">=</span> <span class="nc">LRConfig</span><span class="p">(</span>
        <span class="n">max_lr</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">max_lr</span><span class="p">,</span>
        <span class="n">min_lr</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">min_lr</span><span class="p">,</span>
        <span class="n">end_lr</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">end_lr</span><span class="p">,</span>
        <span class="n">warmup_steps</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">warmup_steps</span><span class="p">,</span>
        <span class="n">end_steps</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">end_steps</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">device_cfg</span> <span class="o">=</span> <span class="nf">deviceConfig</span><span class="p">(</span>
        <span class="n">n_device_axis</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">n_device_axis</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">inference_cfg</span> <span class="o">=</span> <span class="nf">inferenceConfig</span><span class="p">(</span>
        <span class="n">prompt</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">prompt</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">inference_batch</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">top_k</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">n_devices</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">n_devices</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">max_tokens</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">use_cache</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div> <p>Finally one wrapper <code class="language-plaintext highlighter-rouge">config</code> class containing all these instantiated subclasses is returned as the final config for the model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">parse_args</span><span class="p">():</span>
    <span class="bp">...</span>
    <span class="n">cfg</span> <span class="o">=</span> <span class="nf">config</span><span class="p">(</span>
        <span class="n">model_config</span><span class="o">=</span><span class="n">model_cfg</span><span class="p">,</span>
        <span class="n">data_config</span><span class="o">=</span><span class="n">data_cfg</span><span class="p">,</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr_cfg</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">name</span><span class="p">,</span>
        <span class="n">output_dir</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">output_dir</span><span class="p">,</span>
        <span class="n">device_config</span><span class="o">=</span><span class="n">device_cfg</span><span class="p">,</span>
        <span class="n">checkpoint_steps</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">checkpoint_steps</span><span class="p">,</span>
        <span class="n">inference_config</span><span class="o">=</span><span class="n">inference_cfg</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">seed</span><span class="p">,</span>
        <span class="n">training_steps</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">training_steps</span><span class="p">,</span>
        <span class="n">grad_step</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">grad_step</span><span class="p">,</span>
        <span class="n">eval_steps</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">eval_steps</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">alpha</span><span class="p">,</span>
        <span class="n">wandb</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">wandb</span><span class="p">,</span>
        <span class="n">grad_clip_norm</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">grad_clip_norm</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">cfg</span>
</code></pre></div></div> <p>With this covered, we can now move on to one of the most fundamental topics of this guide: distributed training.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/main.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'jindal013/jaxformer-website-v2',
        'data-repo-id': 'R_kgDOPoEVEA',
        'data-category': 'General',
        'data-category-id': 'DIC_kwDOPoEVEM4Cu2n4',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-loading': '1',
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> Â© Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>