<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How to Write the Base Model | Jaxformer: Scaling Modern Transformers </title> <meta name="author" content=" "> <meta name="description" content="We begin by writing the single-TPU base model including modern day modules such as RMSNorm, Multi-latent Attention, RoPE, decoupled RoPE embeddings, interleaved attention, KV-cache and more. This serves as a working foundation, from which we can later scale to multi-TPU and distributed training setups."> <meta name="keywords" content="jscaling, jax, llms, transformers, tpus, google, cloud, parallelism, distributed"> <meta property="og:title" content="JAXformer: Scaling Modern Transformers"> <meta property="og:description" content="A zero-to-one guide on scaling modern transformers with n-dimensional parallelism."> <meta property="og:image" content="https://jaxformer.com/assets/img/banner.png"> <meta property="og:url" content="https://jaxformer.com"> <meta property="og:type" content="website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/jaxformer-icon.png?7001ddef15419e25335b33b49c6ce725"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jaxformer.com/base_model/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> <script async src="https://www.googletagmanager.com/gtag/js?id=G-G878LK8JDJ"></script> <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-G878LK8JDJ');
</script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "How to Write the Base Model",
            "description": "We begin by writing the single-TPU base model including modern day modules such as RMSNorm, Multi-latent Attention, RoPE, decoupled RoPE embeddings, interleaved attention, KV-cache and more. This serves as a working foundation, from which we can later scale to multi-TPU and distributed training setups.",
            "published": "September 05, 2025",
            "authors": [
              
              {
                "author": "Aditya Makkar",
                "authorURL": "https://x.com/AdityaMakkar000"
              },
              
              {
                "author": "Divya Makkar",
                "authorURL": "https://x.com/_DivyaMakkar"
              },
              
              {
                "author": "Chinmay Jindal",
                "authorURL": "https://x.com/chinmayjindal_"
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <script>
    function goToTop() {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    }

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
      // Get the button:
      let mybutton = document.getElementById("top-button");

      if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
  }
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="d-none d-sm-inline">Jaxformer: Scaling Modern Transformers</span> <span class="d-inline d-sm-none"> Jaxformer </span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="left-button section-button"><a href="../tokenization"><svg viewbox="-78.5 0 512 512"><path d="M257 64L291 98 128 262 291 426 257 460 61 262 257 64Z"></path></svg></a></div> <div class="right-button section-button"><a href="../sharded"><svg viewbox="-78.5 0 512 512"><path d="M98 460L64 426 227 262 64 98 98 64 294 262 98 460Z"></path></svg></a></div> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item nav-hidden"><a class="nav-link" onclick="goToTop()" id="top-button" style="display: none;">Back to Top</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../tokenization">Previous Part</a></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../sharded">Next Part</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Sections </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/">Part 0. Introduction</a> <a class="dropdown-item " href="/tokenization/">Part 1. Tokenization</a> <a class="dropdown-item " href="/base_model/">Part 2. Base Model</a> <a class="dropdown-item " href="/sharded/">Part 3. Sharded Model</a> <a class="dropdown-item " href="/dataset/">Part 4. Dataset &amp; Config</a> <a class="dropdown-item " href="/distributed_training/">Part 5. Distributed Training</a> <a class="dropdown-item " href="/moe/">Part 6. Mixture of Experts</a> <a class="dropdown-item " href="/training/">Part 7. Training Results</a> <a class="dropdown-item " href="/conclusion/">Part 8. Conclusion</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>How to Write the Base Model</h1> <p>Part 2 of <a href="">Jaxformer</a> (<a href="../tokenization">Part 1: Tokenization</a> | <a href="../sharded">Part 3: Sharded Model</a>)</p> <p>We begin by writing the single-TPU base model including modern day modules such as RMSNorm, Multi-latent Attention, RoPE, decoupled RoPE embeddings, interleaved attention, KV-cache and more. This serves as a working foundation, from which we can later scale to multi-TPU and distributed training setups.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#root-mean-squared-norm">Root-Mean Squared Norm</a> </div> <div> <a href="#embedding">Embedding</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#embedding-lookup">Embedding Lookup</a> </li> <li> <a href="#weight-tying">Weight Tying</a> </li> </ul> <div> <a href="#feedforward">FeedForward</a> </div> <div> <a href="#rope">RoPE</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#mathematical-intuition">Mathematical Intuition</a> </li> <li> <a href="#implementation-in-jax">Implementation in JAX</a> </li> </ul> <div> <a href="#multi-latent-attention">Multi-Latent Attention</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#low-rank-attention-motivation">Low-Rank Attention Motivation</a> </li> <li> <a href="#implementation-and-caching">Implementation and Caching</a> </li> </ul> <div> <a href="#interleaved-attention-layers">Interleaved Attention Layers</a> </div> <div> <a href="#transformer-model">Transformer Model</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#forward-pass-and-caching">Forward Pass and Caching</a> </li> <li> <a href="#configuration-and-initialization">Configuration and Initialization</a> </li> </ul> </nav> </d-contents> <p>Here is the base architecture we will building and reflects many of the components in modern-day transformers.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/base_model/base_img-480.webp 480w,/assets/img/base_model/base_img-800.webp 800w,/assets/img/base_model/base_img-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/base_model/base_img.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="root-mean-squared-norm">Root-Mean Squared Norm</h2> <p>In our model, we utilize root-mean squared norm (<a href="https://arxiv.org/abs/1910.07467" rel="external nofollow noopener" target="_blank">RMS Norm</a>) as the normalization between layers. In JAX we can initialize models directly within the forward pass, similar to TensorFlow. Since this is lazy initialization, we only need to specify the input dtype at the start.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">flax.linen.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">RMSNorm</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span>
</code></pre></div></div> <p>Our <code class="language-plaintext highlighter-rouge">init/forward</code> pass is defined under the <code class="language-plaintext highlighter-rouge">nn.compact</code> decorator. We begin by computing the mean of the squared values and scale by its square root. Next, we initialize the shift/scale parameters and apply them to obtain the output (note: Some RMSNorm implementations do not use shift parameters). These parameters are broadcasted across the <code class="language-plaintext highlighter-rouge">(B, T)</code> dimensions, while the last axis of the input <code class="language-plaintext highlighter-rouge">x</code> is treated as the channel dim.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RMSNorm</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span>

    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
        <span class="n">rms</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">rms</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>

        <span class="n">gamma</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">param</span><span class="p">(</span>
          <span class="sh">"</span><span class="s">gamma</span><span class="sh">"</span><span class="p">,</span>
          <span class="n">nn</span><span class="p">.</span><span class="n">initializers</span><span class="p">.</span><span class="n">ones</span><span class="p">,</span>
          <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
          <span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span>
        <span class="p">)</span>

        <span class="n">beta</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">param</span><span class="p">(</span>
          <span class="sh">"</span><span class="s">beta</span><span class="sh">"</span><span class="p">,</span>
          <span class="n">nn</span><span class="p">.</span><span class="n">initializers</span><span class="p">.</span><span class="n">zeros</span><span class="p">,</span>
          <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
          <span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span>
        <span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">beta</span>

        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="embedding">Embedding</h2> <h3 id="embedding-lookup">Embedding Lookup</h3> <p>The embedding layer helps convert tokens into continuous $n$ dimensional vectors. For this class, we can use JAXâ€™s flexibility when initializing our class using a PyTorch init pattern in the <code class="language-plaintext highlighter-rouge">setup</code> method.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Embeddings</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">model_dimension</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embed</span><span class="p">(</span>
            <span class="n">num_embeddings</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span>
            <span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="nc">RMSNorm</span><span class="p">(</span><span class="n">model_dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">nn.Embed</code> is a built-in class capable of performing an embedding lookup as well as weight-typing which can be used to convert vectors to a distribution over tokens. Additionally, the norm can be used in this final layer. Thus, we separate our call function into two control-flows, one for the start of the forward pass and one for the end based on a param <code class="language-plaintext highlighter-rouge">out</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">out</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">out</span><span class="p">:</span>
        <span class="c1"># perform embedding loop
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># perform dot-product with transpose of the embedding params
</span>    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p>For the embedding lookup we can pass our input <code class="language-plaintext highlighter-rouge">x = self.embedding(x)</code> through the layer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">out</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">out</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># perform embedding loop
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># perform dot-product with transpose of the embedding params
</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p>A caveat of JAX is that the first forward-pass must initialize all the params, so we can pass our input through the norm as a dummy pass to initialize it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">out</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">out</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># perform embedding loop
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="nf">is_mutable_collection</span><span class="p">(</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">):</span>
            <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># perform dot-product with transpose of the embedding params
</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h3 id="weight-tying">Weight Tying</h3> <p>For the second-pass (weight-tying) we first pass <code class="language-plaintext highlighter-rouge">x</code> through the norm and then use the built in <code class="language-plaintext highlighter-rouge">embed.attend(x)</code> function to perform the transposed matmul. Our final embedding class is shown below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Embeddings</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">model_dimension</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embed</span><span class="p">(</span>
            <span class="n">num_embeddings</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span>
            <span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="nc">RMSNorm</span><span class="p">(</span><span class="n">model_dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">out</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">out</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="nf">is_mutable_collection</span><span class="p">(</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">):</span>
                <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">.</span><span class="nf">attend</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="feedforward">FeedForward</h2> <p>We can use the standard <code class="language-plaintext highlighter-rouge">nn.Dense</code> module to implement a Feedforward block. We wrap the <code class="language-plaintext highlighter-rouge">nn.Dense</code> for now since this will allow us to perform <code class="language-plaintext highlighter-rouge">Tensor Parallelism</code> / <code class="language-plaintext highlighter-rouge">FSDP</code> in the future as it is the building block for all future modules.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">features</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span>

    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dtype</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p>For the FeedForward layer we combine 2 <code class="language-plaintext highlighter-rouge">Dense</code> modules using a <code class="language-plaintext highlighter-rouge">4x up/down</code> projection.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">model_dimension</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">ff_dim</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span>

    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">train</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span>
            <span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">ff_dim</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">,</span>
        <span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nf">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span>
            <span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">,</span>
        <span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p>Note that since JAX is functional, for features such as <code class="language-plaintext highlighter-rouge">Dropout</code>, we pass parameters to allow the functions to remain pure, rather then relying on state.</p> <h2 id="rope">RoPE</h2> <h3 id="mathematical-intuition">Mathematical Intuition</h3> <p>Rotary Positional Embeddings allow for relative embeddings based on applying standard euclidean 2D-rotation to each $2$ dimensional subspaces of the $n$ dimensional vector. We can represent this as</p> \[\mathbf{x}_{\text{RoPE}}(m) = \underbrace{ \begin{bmatrix} \cos(m\theta_1) &amp; -\sin(m\theta_1) &amp; 0 &amp; 0&amp; \ldots &amp; 0 &amp; 0\\ \sin(m\theta_1) &amp; \cos(m\theta_1) &amp; 0 &amp; 0 &amp; \ldots &amp; 0 &amp; 0 &amp; \\ 0 &amp; 0 &amp; \cos(m\theta_2) &amp; -\sin(m\theta_2) &amp; &amp; \vdots &amp; \vdots \\ 0 &amp; 0 &amp; \sin(m\theta_2) &amp; \cos(m\theta_2) &amp; &amp; \vdots &amp; \vdots \\ \vdots &amp; \vdots &amp; &amp; &amp; \ddots &amp; \vdots &amp; \vdots \\ 0 &amp; 0&amp; \ldots &amp; \ldots &amp; &amp; \cos(m\theta_{d/2}) &amp; -\sin(m\theta*{d/2}) \\ 0 &amp; 0 &amp; \ldots &amp; \ldots &amp; &amp; \sin(m\theta*{d/2}) &amp; \cos(m\theta*{d/2}) \end{bmatrix} }* \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix}\] <p>where $m$ is the time position. We can also define $R(m \theta_k)$ block as</p> \[R(m\theta_k) = \begin{bmatrix} \cos(m\theta_k) &amp; -\sin(m\theta_k)\\[2pt] \sin(m\theta_k) &amp; \cos(m\theta_k) \end{bmatrix}.\] <p>and then in paired coordinates $(2k-1,2k)$:</p> \[\begin{bmatrix} x'_{2k-1}\\ x'_{2k} \end{bmatrix} = R(m\theta_k) \begin{bmatrix} x_{2k-1}\\ x_{2k} \end{bmatrix}, \qquad k=1,\dots,\tfrac{d}{2}.\] <p>Intuitively, this is just a 2D-rotation by different angles at each time step, applied within each subspace to provide every possible time step with a distinct relative positional encoding.</p> <p>The rotation matrix is used in the forward pass; however, instead of doing a matrix multiplication which would be $O(d^2T)$, we can perform a product-wise multiplication taking advantage of the of the sparsity of the matrix and achieve $O(dT)$. Hence for any time step and position, we only perform 2 multiplications which can be written as an element-wise operation instead.</p> \[\begin{equation} \mathbf{x}'(m) = \begin{pmatrix} x*1 \\ x_2 \\ x_3 \\ x_4 \\ \vdots \\ x*{d-1} \\ x*d \end{pmatrix} \odot \begin{pmatrix} \cos m\theta*{1} \\ \cos m\theta*{1} \\ \cos m\theta*{2} \\ \cos m\theta*{2} \\ \vdots \\ \cos m\theta*{d/2} \\ \cos m\theta\_{d/2} \end{pmatrix} - \begin{pmatrix} * x_2 \\ x_1 \\ * x_4 \\ x_3 \\ \vdots \\ * x*d \\ x*{d-1} \end{pmatrix} \odot \begin{pmatrix} \sin m\theta*{1} \\ \sin m\theta*{1} \\ \sin m\theta*{2} \\ \sin m\theta*{2} \\ \vdots \\ \sin m\theta*{d/2} \\ \sin m\theta*{d/2} \end{pmatrix} \end{equation}\] <h3 id="implementation-in-jax">Implementation in JAX</h3> <p>To implement this, we can create the cosine and sine vectors and then perform the necessary tensor ops on $x$ during the forward pass. We begin the class like a standard JAX module; however, unlike the others, this will have no params in it. We just use this class to allow for lazy initialization with the other classes that call it, such as Multi-Head Latent Attention.</p> <p>We first begin by taking in T (the length of the sequence) and the <code class="language-plaintext highlighter-rouge">model_dim</code>, ensuring that the dimension can be split into 2 subspaces.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RoPE</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">T</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">model_dim</span><span class="p">:</span> <span class="nb">int</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">model_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">model_dim must be even</span><span class="sh">"</span>
</code></pre></div></div> <p>Then, we create a frequency array <code class="language-plaintext highlighter-rouge">[1,2,3,...,T]</code> which will scale our $\theta$. To make this a<code class="language-plaintext highlighter-rouge">2D</code> array, we expand the dim to create <code class="language-plaintext highlighter-rouge">[[1], [2], [3], ..., [T]]</code>, since this will allow for broadcasting with the channel frequencies and make it one indexed.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">freq</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
</code></pre></div></div> <p>To setup the $\theta$, we first create an array of the powers of the base that yield each $\theta$. Specifically, $\theta_i = B^{-2i/d}$ , so we can represent the array as</p> \[[\theta_1, \theta_2, \ldots, \theta_{d / 2}] = B^{-2/d \cdot [1,2,\ldots, d/2]}.\] <p>Since our final array for $\sin/\cos$ is $\sin \left([\theta_1, \theta_1, \theta_2, \theta_2, \ldots, \theta_{d / 2}, \theta_{d/2}]\right)$ , we can repeat the elements along the second dim and then flatten it into one continuous array. Thus,</p> \[pos = [0,0,1,1, \ldots, d/2, d/2].\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pos</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">model_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">pos</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>We now compute our base (we use $10,000$) to create our array of $\theta$ , using log rules.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">log_theta_base</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">pos</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">model_dim</span> <span class="o">*</span> <span class="n">log_theta_base</span><span class="p">)</span>
</code></pre></div></div> <p>Finally, we create the final array of $\cos$ and $\sin$ by broadcasting each channel dim across every time step $t$ where each $\theta$ now becomes $t\theta$. The final array will therefore be</p> \[[[1 \theta_0, 1\theta_0 1\theta_1, \theta_1, \ldots, 1 \theta_{d/2} 1 \theta_{d/2}], \ldots, [T \theta_0, T \theta_0, \ldots, T \theta_{d/2}, T \theta_{d/2}]].\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RoPE</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">T</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">model_dim</span><span class="p">:</span> <span class="nb">int</span>
    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">model_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">model_dim must be even</span><span class="sh">"</span>

        <span class="n">freq</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="n">pos</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">model_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">pos</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">log_theta_base</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">pos</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">model_dim</span> <span class="o">*</span> <span class="n">log_theta_base</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">cos</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">freq</span> <span class="o">*</span> <span class="n">theta</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sin</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">freq</span> <span class="o">*</span> <span class="n">theta</span><span class="p">)</span>
</code></pre></div></div> <p>In the forward pass, $x$ is batched across $t$ time steps, allowing us to index into the 2D array and perform element-wise multiplication. Therefore, we first begin by taking in our array as well as the time step our array begins at to determine the first index. We also get the $T$ or length of our input to know how far to index into the $\sin / \cos$ arrays. We also cast the input to float32 to help with precision.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span>
    <span class="n">t_start</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">dtype</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
</code></pre></div></div> <p>Since the $\cos$ Hadamard product requires the input <code class="language-plaintext highlighter-rouge">x</code>, we can perform that operation before turning our attention to the second operand.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cos_rope</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">cos</span><span class="p">[</span><span class="n">t_start</span> <span class="p">:</span> <span class="n">t_start</span> <span class="o">+</span> <span class="n">T</span><span class="p">,</span> <span class="p">:]</span>
</code></pre></div></div> <p>We first begin by breaking the $x$ input into the <code class="language-plaintext highlighter-rouge">2</code> dim subspaces and multiplying the second component by $-1$. We then stack the second onto the first and reshape to obtain the flipped version.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_inter</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">x_inter_one</span> <span class="o">=</span> <span class="n">x_inter</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">]</span> <span class="c1"># first component
</span><span class="n">x_inter_two</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">x_inter</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># second component
</span><span class="n">x_inter</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">x_inter_two</span><span class="p">,</span> <span class="n">x_inter_one</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># stack will switch betwen on the given axis, in this case this is a flip
</span><span class="n">x_inter</span> <span class="o">=</span> <span class="n">x_inter</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
</code></pre></div></div> <p>We can then multiply this by the $\sin$ array, add it back and recast to the original type to get our output.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RoPE</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">T</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">model_dim</span><span class="p">:</span> <span class="nb">int</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">model_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">model_dim must be even</span><span class="sh">"</span>

        <span class="n">freq</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="n">pos</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">model_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">pos</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">log_theta_base</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">pos</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">model_dim</span> <span class="o">*</span> <span class="n">log_theta_base</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">cos</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">freq</span> <span class="o">*</span> <span class="n">theta</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sin</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">freq</span> <span class="o">*</span> <span class="n">theta</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span>
        <span class="n">t_start</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">dtype</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="n">cos_rope</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">cos</span><span class="p">[</span><span class="n">t_start</span> <span class="p">:</span> <span class="n">t_start</span> <span class="o">+</span> <span class="n">T</span><span class="p">,</span> <span class="p">:]</span>

        <span class="n">x_inter</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">x_inter_one</span> <span class="o">=</span> <span class="n">x_inter</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">x_inter_two</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">x_inter</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">x_inter</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">x_inter_two</span><span class="p">,</span> <span class="n">x_inter_one</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x_inter</span> <span class="o">=</span> <span class="n">x_inter</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>

        <span class="n">sin_rope</span> <span class="o">=</span> <span class="n">x_inter</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">sin</span><span class="p">[</span><span class="n">t_start</span> <span class="p">:</span> <span class="n">t_start</span> <span class="o">+</span> <span class="n">T</span><span class="p">,</span> <span class="p">:]</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">cos_rope</span> <span class="o">+</span> <span class="n">sin_rope</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="multi-latent-attention">Multi-Latent Attention</h2> <h3 id="low-rank-attention-motivation">Low-Rank Attention Motivation</h3> <p>We now write the core attention mechanism of the model. We will use multi-latent attention introduced in <a href="https://arxiv.org/pdf/2405.04434" rel="external nofollow noopener" target="_blank">DeepSeek-V2</a>. The driving idea is motivated by how to save inference-time memory. In a standard KV-cache, our transformer has to save $2LTd$ elements as for each key/value pair in a layer, we have $T$ time steps for which the dimension is $d$. Now, consider a transformer with a sequence length of $T = 128k$, dimension of $d = 7168$ and layers $L = 61$. If the KV cache is stored in <code class="language-plaintext highlighter-rouge">bfloat16</code> this leads to $\frac{2 \cdot 61 \cdot 128 \cdot 10^3 \cdot 7168 \cdot 2}{10^9} \approx 220GB$ of memory constraints. There exists solutions to optimize this such as Grouped Query Attention or Multi Query Attention (when $n_{\text{groups}}$ equals 1); however, these often lead to a decrease in quality. Multi latent attention attempts to fix this using the idea of <a href="https://en.wikipedia.org/wiki/Low-rank_approximation" rel="external nofollow noopener" target="_blank">Low-Rank decomposition</a> . Instead of using $K = W^k x$ and $V = W^v x$. we decompose the $W^k$ and $W^v$ into a matrix-matrix product $W^K = AB$ where $A \in \mathbb{R}^{d \times r}$ and $B \in \mathbb{R}^{r \times d}$ where $r Â«Â d$. This way we use $2dr$ space instead of $2d^2$ and thus grow linearly with $d$ instead of quadratically. In terms of the cache, we now store $Bx$ instead of $W^Kx = ABx$. This means our memory is now $2LTr$, which in terms of the previous example, is roughly $33$ GB.</p> <p>To further save memory, we can use the same $B$ matrix for the key and values, thus $W^K = A^KB$ and $W^V = A^V B$. This means we can get rid of the 2 factor, further cutting our memory in half to $\approx 16$GB. Note this does trade memory for compute; however, in these cases, we are memory bound which appeals to the idea of MLA.</p> <h3 id="implementation-and-caching">Implementation and Caching</h3> <p>Thus we begin by defining our module, with our hyper-parameters.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MLA</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">model_dimension</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">T</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dhR</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">model_dimension, n_head, dropout, model_dtype</code> all follow from standard MHA. Latent dim is the rank $r$ of the low-rank projection. <code class="language-plaintext highlighter-rouge">dhR</code> and <code class="language-plaintext highlighter-rouge">T</code> are for the RoPE embedding which will be discussed later.</p> <p>For our call method, we take in the array as well as the caches.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@nn.compact</span>
<span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span>
  <span class="n">self</span><span class="p">,</span>
  <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span>
  <span class="o">*</span><span class="p">,</span>
  <span class="n">cKV_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
  <span class="n">kRT_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
  <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]]]:</span>

  <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span> <span class="c1"># get dimension information
</span></code></pre></div></div> <p>Note we will return a tuple with the result of the attention and another tuple which will hold the caches for the key-value pair and the RoPE (to be discussed later on).</p> <p>We first begin by projecting the <code class="language-plaintext highlighter-rouge">x</code> to the latent dim for the key-value pair and the queries.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">kv_latent</span><span class="p">,</span> <span class="n">q_latent</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>Note that we only create one <code class="language-plaintext highlighter-rouge">Dense</code> for this operation since we can split the output along the last dim due to obtaining $x \in \mathbb{R}^{ B \times T \times 2 \cdot r}$ which is equivalent to doing the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kv_latent</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">q_latent</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p>We can then write the up-projection from the latent space to the final key, value, query pairs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span>
  <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">kv_latent</span><span class="p">),</span>
  <span class="mi">2</span><span class="p">,</span>
  <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">q_latent</span><span class="p">)</span>
</code></pre></div></div> <p>Then, we can map over the tuple <code class="language-plaintext highlighter-rouge">(q, k ,v)</code> to split them into <code class="language-plaintext highlighter-rouge">n_head</code> batches.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
  <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">B T (nh d) -&gt; B nh T d</span><span class="sh">"</span><span class="p">,</span> <span class="n">nh</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">),</span> <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div> <p>We make use of the <code class="language-plaintext highlighter-rouge">rearrange</code> op from the <code class="language-plaintext highlighter-rouge">einops</code> library for ease of reading the operation, as opposed to writing <code class="language-plaintext highlighter-rouge">x.reshape(...).permute(...)</code>. In this case, we split the last dim into <code class="language-plaintext highlighter-rouge">n_head</code> arrays and then permute it to ensure the last 2 dims will be multiplied like in MHA. We can use the <code class="language-plaintext highlighter-rouge">tree.map</code> function to map over the the tuple.</p> <p>We can now write a function to perform the normal scaled-dot product attention.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">scaledDotProd</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
  <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">dtype</span>

  <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="p">,</span><span class="n">v</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">),</span> <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
  <span class="n">dk</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

  <span class="n">w</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">B n T d, B n t d -&gt; B n T t</span><span class="sh">"</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dk</span><span class="o">**-</span><span class="mf">0.5</span><span class="p">)</span>
  <span class="n">w</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
  <span class="n">w</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">B n T t, B n t d -&gt; B n T d</span><span class="sh">"</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

  <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p>Breaking this function down, the input <code class="language-plaintext highlighter-rouge">dtype</code> is recorded to ensure that after performing the attention computation in <code class="language-plaintext highlighter-rouge">jnp.float32</code>, it can be casted back to the right precision afterwards. We then covert our <code class="language-plaintext highlighter-rouge">q,k,v</code> to <code class="language-plaintext highlighter-rouge">jnp.float32</code>. The attention operation can be expressed using <a href="https://rockt.ai/2018/04/30/einsum" rel="external nofollow noopener" target="_blank">einsum notation</a> and casted back to the original type.</p> <p>We can then create the mask and call the function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span>
  <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">k</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])),</span>
<span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="nf">scaledDotProd</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</code></pre></div></div> <p>After, we can use <code class="language-plaintext highlighter-rouge">rearrange</code> to concat all the heads back together, pass it through a projection matrix and apply Dropout.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="sh">"</span><span class="s">B nh T dk -&gt; B T (nh dk)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">output</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">)(</span><span class="n">output</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)</span>
</code></pre></div></div> <p>MLA also uses the idea of decoupled RoPE appending on the positional encoding to the absolute key-query pairs. We first project $x$ into a latent RoPE space and encode those using the previous RoPE module. We can then concat them to the keys / queries. Since we will be using interleaved blocks (some layers will not have positional embeddings), we use a check for rope if the rope latent dim <code class="language-plaintext highlighter-rouge">dhR</code> is greater than 0.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">use_rope</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dhR</span> <span class="o">&gt;</span> <span class="mi">0</span>

<span class="k">if</span> <span class="n">use_rope</span><span class="p">:</span>
  <span class="n">t_start</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># needed for KV Cache
</span>  <span class="n">x_k_r</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dhR</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x_q_r</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dhR</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p>Following the Deepseek V-3 paper, each query head will get a unique set of decoupled RoPE encodings but the key heads will all share one set (repeated across each head). Now, we can setup a RoPE module and apply them onto both latents. For the queries RoPE, we can rearrange them to split each head into itâ€™s own batch.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">use_rope</span><span class="p">:</span>
  <span class="n">t_start</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># needed for KV Cache
</span>  <span class="n">x_k_r</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dhR</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x_q_r</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dhR</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

  <span class="n">rope_k</span> <span class="o">=</span> <span class="nc">RoPE</span><span class="p">(</span>
    <span class="n">model_dim</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dhR</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">T</span>
  <span class="p">)</span>
  <span class="n">rope_q</span> <span class="o">=</span> <span class="nc">RoPE</span><span class="p">(</span>
    <span class="n">model_dim</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dhR</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span>
    <span class="n">T</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">,</span>
  <span class="p">)</span>

  <span class="n">kRt</span> <span class="o">=</span> <span class="nf">rope_k</span><span class="p">(</span><span class="n">x_k_r</span><span class="p">,</span> <span class="n">t_start</span><span class="p">)</span>

  <span class="n">qRt</span> <span class="o">=</span> <span class="nf">rope_q</span><span class="p">(</span><span class="n">x_q_r</span><span class="p">,</span> <span class="n">t_start</span><span class="p">)</span>
  <span class="n">qRt</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">qRt</span><span class="p">,</span> <span class="sh">"</span><span class="s">B T (nh d) -&gt; B nh T d</span><span class="sh">"</span><span class="p">,</span> <span class="n">nh</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">)</span>
</code></pre></div></div> <p>After constructing the key, query, and value tensors, if we are using RoPE, we can concatenate them while repeating the decoupled key embeddings across all heads.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
  <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">B T (nh d) -&gt; B nh T d</span><span class="sh">"</span><span class="p">,</span> <span class="n">nh</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">),</span>
  <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="p">)</span>

<span class="k">if</span> <span class="n">use_rope</span><span class="p">:</span>
  <span class="n">q</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">q</span><span class="p">,</span> <span class="n">qRt</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

  <span class="n">kRt</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">kRt</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># add dim for head
</span>  <span class="n">k</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">k</span><span class="p">,</span> <span class="n">kRt</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>The last step is to setup the caching. The first change is in the RoPE block to find<code class="language-plaintext highlighter-rouge"> t_start</code> since if we are using cached indices, we need to know which position to begin applying the RoPE from. To do this, we take the length of the cache as it represents our current index which we need to start from as tensor-indexing is 0-indexed.</p> <p>Thus our first line in the if statement of the rope block becomes:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t_start</span> <span class="o">=</span> <span class="n">KV_cache</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">KV_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="mi">0</span>
</code></pre></div></div> <p>Note we can use either <code class="language-plaintext highlighter-rouge">KV_cache.shape[1]</code> or <code class="language-plaintext highlighter-rouge">KR_cache.shape[1]</code>.</p> <p>Then we can build the cache if we are not training.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="ow">not</span> <span class="n">train</span><span class="p">:</span>
<span class="c1"># build cache
</span></code></pre></div></div> <p>If the past cache isnâ€™t none, we want to append along the $T$ axis which is the <code class="language-plaintext highlighter-rouge">1</code> index, otherwise we want to set it to the <code class="language-plaintext highlighter-rouge">kv_latent</code>. Thus, a simple implementation allows us to set the <code class="language-plaintext highlighter-rouge">kv_latent</code> to the concat of the <code class="language-plaintext highlighter-rouge">KV_cache</code> if it is not none and the current latent as it will be of length 1 (since we are using the past key/value from the cache). We can then set the <code class="language-plaintext highlighter-rouge">KV_cache</code> to this.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">kv_latent</span><span class="p">,</span> <span class="n">q_latent</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="bp">...</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">train</span><span class="p">:</span>
  <span class="k">if</span> <span class="n">KV_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">kv_latent</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">KV_cache</span><span class="p">,</span> <span class="n">kv_latent</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">KV_cache</span> <span class="o">=</span> <span class="n">kv_latent</span>
</code></pre></div></div> <p>The same approach can work with the <code class="language-plaintext highlighter-rouge">RoPE</code> keys.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="ow">not</span> <span class="n">train</span><span class="p">:</span>
  <span class="k">if</span> <span class="n">KV_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">kv_latent</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">KV_cache</span><span class="p">,</span> <span class="n">kv_latent</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">KV_cache</span> <span class="o">=</span> <span class="n">kv_latent</span>

  <span class="k">if</span> <span class="n">use_rope</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">KR_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">kRt</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">KR_cache</span><span class="p">,</span> <span class="n">kRt</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">KR_cache</span> <span class="o">=</span> <span class="n">kRt</span>
</code></pre></div></div> <p>The last change required is in the masking since if we have a length of 1, we donâ€™t want to mask out any element since that query pair can attend to every past one.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">T</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
  <span class="c1"># q.shape[2] is 1 as well but we are more explict
</span>  <span class="n">mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">local_n_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span> <span class="n">l</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span>
    <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">local_n_heads</span><span class="p">,</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">k</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])),</span>
  <span class="p">)</span>
</code></pre></div></div> <p>At the ending, we return following the signature of the function:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">KV_cache</span><span class="p">,</span> <span class="n">KR_cache</span><span class="p">)</span>
</code></pre></div></div> <p>To improve the readability of the signatures in the future, we can use a type alias of:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cache_type</span> <span class="o">=</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]]</span>
</code></pre></div></div> <p>Thus the final MLA class is:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MLA</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">model_dimension</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">T</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dhR</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">KV_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">KR_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="n">cache_type</span><span class="p">]:</span>
        <span class="n">use_rope</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dhR</span> <span class="o">&gt;</span> <span class="mi">0</span>

        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>

        <span class="n">x</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">kv_latent</span><span class="p">,</span> <span class="n">q_latent</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">use_rope</span><span class="p">:</span>
            <span class="n">t_start</span> <span class="o">=</span> <span class="n">KV_cache</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">KV_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="n">x_k_r</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dhR</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x_q_r</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dhR</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

            <span class="n">rope_k</span> <span class="o">=</span> <span class="nc">RoPE</span><span class="p">(</span>
                <span class="n">model_dim</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dhR</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">T</span>
            <span class="p">)</span>
            <span class="n">rope_q</span> <span class="o">=</span> <span class="nc">RoPE</span><span class="p">(</span>
                <span class="n">model_dim</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dhR</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span>
                <span class="n">T</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">kRt</span> <span class="o">=</span> <span class="nf">rope_k</span><span class="p">(</span><span class="n">x_k_r</span><span class="p">,</span> <span class="n">t_start</span><span class="p">)</span>

            <span class="n">qRt</span> <span class="o">=</span> <span class="nf">rope_q</span><span class="p">(</span><span class="n">x_q_r</span><span class="p">,</span> <span class="n">t_start</span><span class="p">)</span>
            <span class="n">qRt</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">qRt</span><span class="p">,</span> <span class="sh">"</span><span class="s">B T (nh d) -&gt; B nh T d</span><span class="sh">"</span><span class="p">,</span> <span class="n">nh</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">train</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">KV_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">kv_latent</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">KV_cache</span><span class="p">,</span> <span class="n">kv_latent</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">KV_cache</span> <span class="o">=</span> <span class="n">kv_latent</span>

            <span class="k">if</span> <span class="n">use_rope</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">KR_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">kRt</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">KR_cache</span><span class="p">,</span> <span class="n">kRt</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">KR_cache</span> <span class="o">=</span> <span class="n">kRt</span>

        <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span>
            <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">kv_latent</span><span class="p">),</span>
            <span class="mi">2</span><span class="p">,</span>
            <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">q_latent</span><span class="p">)</span>

        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">B T (nh d) -&gt; B nh T d</span><span class="sh">"</span><span class="p">,</span> <span class="n">nh</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">),</span> <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">all_to_all</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">,</span> <span class="n">split_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">concat_axis</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">tiled</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">),</span>
            <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">use_rope</span><span class="p">:</span>
            <span class="n">qRt</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">all_to_all</span><span class="p">(</span><span class="n">qRt</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">,</span> <span class="n">split_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">concat_axis</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">tiled</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">q</span><span class="p">,</span> <span class="n">qRt</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">kRt</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">kRt</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">kRt</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">all_to_all</span><span class="p">(</span><span class="n">kRt</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">,</span> <span class="n">split_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">concat_axis</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">tiled</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">k</span><span class="p">,</span> <span class="n">kRt</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">scaledDotProd</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
            <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">dtype</span>

            <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="p">,</span><span class="n">v</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">),</span> <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
            <span class="n">dk</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">w</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">B n T d, B n t d -&gt; B n T t</span><span class="sh">"</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dk</span><span class="o">**-</span><span class="mf">0.5</span><span class="p">)</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">B n T t, B n t d -&gt; B n T d</span><span class="sh">"</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">input_dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">output</span>

        <span class="n">local_n_heads</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">T</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">local_n_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span>
                <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">local_n_heads</span><span class="p">,</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">k</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])),</span>
            <span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="nf">scaledDotProd</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">all_to_all</span><span class="p">(</span>
            <span class="n">output</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">,</span> <span class="n">split_axis</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">concat_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tiled</span><span class="o">=</span><span class="bp">True</span>
        <span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="sh">"</span><span class="s">B nh T dk -&gt; B T (nh dk)</span><span class="sh">"</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">)(</span><span class="n">output</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">KV_cache</span><span class="p">,</span> <span class="n">KR_cache</span><span class="p">)</span>
</code></pre></div></div> <h2 id="interleaved-attention-layers">Interleaved Attention Layers</h2> <p>The idea of interleaved attention layers was introduced in <a href="https://cohere.com/research/papers/command-a-technical-report.pdf" rel="external nofollow noopener" target="_blank">Cohereâ€™s Command A model</a>. There they use sliding window attention with positional embeddings (RoPE) and a full attention with no positional embeddings. Here we use MLA attention for all layers in the block with decoupled RoPE but donâ€™t apply RoPE at the ending. This is relatively simple as we take a layer-per-block and on the last one, set the <code class="language-plaintext highlighter-rouge">dhR = 0</code>. The class is shown below.</p> <p>We begin with the single layer which applies the pre-norm normalization, attention and feedforward network.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">model_dimension</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">T</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dhR</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">bfloat16</span>

    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">cache_type</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="n">cache_type</span><span class="p">]:</span>
        <span class="n">x_res</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">x</span> <span class="o">=</span> <span class="nc">RMSNorm</span><span class="p">(</span><span class="n">model_dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="nc">MLA</span><span class="p">(</span>
            <span class="n">model_dimension</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span>
            <span class="n">n_heads</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span>
            <span class="n">T</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">,</span>
            <span class="n">latent_dim</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">,</span>
            <span class="n">dhR</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dhR</span><span class="p">,</span>
            <span class="n">model_dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
        <span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">KV_cache</span><span class="o">=</span><span class="n">cache</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">KR_cache</span><span class="o">=</span><span class="n">cache</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x_res</span>
        <span class="n">x_res</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">x</span> <span class="o">=</span> <span class="nc">RMSNorm</span><span class="p">(</span><span class="n">model_dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span>
            <span class="n">model_dimension</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span>
            <span class="n">dropout_rate</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
            <span class="n">model_dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">,</span>
        <span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x_res</span>

        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div></div> <p>We can now just call $n$ layers per block.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">model_dimension</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">T</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dhR</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">bfloat16</span>

    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">cache_type</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="n">cache_type</span><span class="p">]:</span>
        <span class="n">KV_cache</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">KR_cache</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="n">current_cache</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">current_cache</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">current_cache</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>

            <span class="n">x</span><span class="p">,</span> <span class="n">cache_out</span> <span class="o">=</span> <span class="nc">Layer</span><span class="p">(</span>
                <span class="n">model_dimension</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span>
                <span class="n">n_heads</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span>
                <span class="n">T</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">,</span>
                <span class="n">latent_dim</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">,</span>
                <span class="n">dhR</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dhR</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
                <span class="n">dropout_rate</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
                <span class="n">model_dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">,</span>
            <span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">current_cache</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">)</span>

            <span class="n">ckV</span><span class="p">,</span> <span class="n">kRT</span> <span class="o">=</span> <span class="n">cache_out</span>
            <span class="k">if</span> <span class="n">ckV</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">KV_cache</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">ckV</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">kRT</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">KR_cache</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">kRT</span><span class="p">)</span>

        <span class="n">KV_cache</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">KV_cache</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">KV_cache</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">None</span>
        <span class="n">KR_cache</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">KR_cache</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">KR_cache</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">None</span>

        <span class="n">out_cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">KV_cache</span><span class="p">,</span> <span class="n">KR_cache</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_cache</span>
</code></pre></div></div> <p>We keep an array for the two separate caches appending if the cache is not None and further stacking to make them into JAX Arrays.</p> <h2 id="transformer-model">Transformer Model</h2> <h3 id="forward-pass-and-caching">Forward Pass and Caching</h3> <p>We can now write the final Transformer Model essentially looping through $n$ blocks.</p> <p>The first step is to check if the cache input is not <code class="language-plaintext highlighter-rouge">None</code>, as that indicates we only want the last token. We can then reshape the input into a <code class="language-plaintext highlighter-rouge">(B, T)</code> tensor.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">model_dimension</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">blocks</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">layers_per_block</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">T</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dhR</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">bfloat16</span>

    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">cache_type</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="n">cache_type</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[...,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>

        <span class="o">*</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</code></pre></div></div> <p>We then add our embedding module and go through the <code class="language-plaintext highlighter-rouge">self.blocks</code>. In the end, we apply the embedding again but use the <code class="language-plaintext highlighter-rouge">out=True</code> to perform the weight tying. Finally, we reshape into the original size. The final transformer block is shown below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="n">embedding</span> <span class="o">=</span> <span class="nc">Embedding</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">model_dimension</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span>
    <span class="n">model_dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">,</span>
  <span class="p">)</span>

  <span class="n">x</span> <span class="o">=</span> <span class="nf">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="n">KV_cache</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">ckRT_cache</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">blocks</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">layer_cache</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">cKV</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
      <span class="n">kRT</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">cache</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">None</span>
      <span class="n">layer_cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">cKV</span><span class="p">,</span> <span class="n">kRT</span><span class="p">)</span>

    <span class="n">x</span><span class="p">,</span> <span class="n">cache_out</span> <span class="o">=</span> <span class="nc">Block</span><span class="p">(</span>
      <span class="n">layers</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">layers_per_block</span><span class="p">,</span>
      <span class="n">model_dimension</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span>
      <span class="n">n_heads</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span>
      <span class="n">T</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">,</span>
      <span class="n">latent_dim</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">,</span>
      <span class="n">dhR</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dhR</span><span class="p">,</span>
      <span class="n">dropout_rate</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
      <span class="n">model_dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">layer_cache</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">cache_out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">KV_cache</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">cache_out</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">cache_out</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">ckRT_cache</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">cache_out</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

  <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">KV_cache</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">KV_cache</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">KV_cache</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">KV_cache</span> <span class="o">=</span> <span class="bp">None</span>
  <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">ckRT_cache</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">ckRT_cache</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">ckRT_cache</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">ckRT_cache</span> <span class="o">=</span> <span class="bp">None</span>
  <span class="n">out_cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">KV_cache</span><span class="p">,</span> <span class="n">ckRT_cache</span><span class="p">)</span>

  <span class="n">x_out</span> <span class="o">=</span> <span class="nf">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="n">x_out</span> <span class="o">=</span> <span class="n">x_out</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">x_out</span><span class="p">,</span> <span class="n">out_cache</span>
</code></pre></div></div> <h3 id="configuration-and-initialization">Configuration and Initialization</h3> <p>This is not the final model we are using for training since in native JAX, it is not simple to split across <code class="language-plaintext highlighter-rouge">n-D</code> parallelism and we want to stay away from abstractions provided by Flax which operate as a blackbox over the network. To simplify construction of the transformer, we can create a data class to represent the arguments to the constructor and create a static method that will load the transformer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dtype_map</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">bfloat16</span><span class="sh">"</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">float16</span><span class="sh">"</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">int32</span><span class="sh">"</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">int64</span><span class="sh">"</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">int64</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">def</span> <span class="nf">convert_dtype</span><span class="p">(</span><span class="n">dtype_str</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dtype_str</span> <span class="ow">in</span> <span class="n">dtype_map</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dtype_map</span><span class="p">[</span><span class="n">dtype_str</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Unsupported dtype: </span><span class="si">{</span><span class="n">dtype_str</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">modelConfig</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">model config class</span><span class="sh">"""</span>

    <span class="n">model_dimension</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">blocks</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">layers_per_block</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">T</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dhR</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">bfloat16</span><span class="sh">"</span>

<span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="bp">...</span>
  <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">modelConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="sh">"</span><span class="s">Transformer</span><span class="sh">"</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">cls</span><span class="p">(</span>
            <span class="n">model_dimension</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span>
            <span class="n">vocab_size</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span>
            <span class="n">n_head</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span>
            <span class="n">blocks</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">blocks</span><span class="p">,</span>
            <span class="n">layers_per_block</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">layers_per_block</span><span class="p">,</span>
            <span class="n">T</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">T</span><span class="p">,</span>
            <span class="n">latent_dim</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">,</span>
            <span class="n">dhR</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">dhR</span><span class="p">,</span>
            <span class="n">dropout_rate</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
            <span class="n">model_dtype</span><span class="o">=</span><span class="nf">convert_dtype</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">),</span>
        <span class="p">)</span>
</code></pre></div></div> <p>We next discuss how to apply parallelism methods to scale this transformer.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/main.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'jindal013/jaxformer-website-v2',
        'data-repo-id': 'R_kgDOPoEVEA',
        'data-category': 'General',
        'data-category-id': 'DIC_kwDOPoEVEM4Cu2n4',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-loading': '1',
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> Â© Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>