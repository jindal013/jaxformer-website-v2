<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Inside the Sharded Model:<br>3-D Parallelism | Jaxformer: Scaling Modern Transformers </title> <meta name="author" content=" "> <meta name="description" content="Here we discuss the 4 main parallelism techniques used for scaling LLMs: data parallelism, fully-sharded data parallelism (FSDP), pipeline parallelism and tensor parallelism. For each, we discuss their theory and a scalable implementation."> <meta name="keywords" content="jscaling, jax, llms, transformers, tpus, google, cloud, parallelism, distributed"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/jaxformer-icon.png?7001ddef15419e25335b33b49c6ce725"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jaxformer.com/sharded/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> <script async src="https://www.googletagmanager.com/gtag/js?id=G-G878LK8JDJ"></script> <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-G878LK8JDJ');
</script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Inside the Sharded Model:<br>3-D Parallelism",
            "description": "Here we discuss the 4 main parallelism techniques used for scaling LLMs: data parallelism, fully-sharded data parallelism (FSDP), pipeline parallelism and tensor parallelism. For each, we discuss their theory and a scalable implementation.",
            "published": "September 05, 2025",
            "authors": [
              
              {
                "author": "Aditya Makkar",
                "authorURL": "https://x.com/AdityaMakkar000"
              },
              
              {
                "author": "Divya Makkar",
                "authorURL": "https://x.com/_DivyaMakkar"
              },
              
              {
                "author": "Chinmay Jindal",
                "authorURL": "https://x.com/chinmayjindal_"
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <script>
    function goToTop() {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    }

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
      // Get the button:
      let mybutton = document.getElementById("top-button");

      if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
  }
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="d-none d-sm-inline">Jaxformer: Scaling Modern Transformers</span> <span class="d-inline d-sm-none"> Jaxformer </span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="left-button section-button"><a href="../base_model"><svg viewbox="-78.5 0 512 512"><path d="M257 64L291 98 128 262 291 426 257 460 61 262 257 64Z"></path></svg></a></div> <div class="right-button section-button"><a href="../dataset"><svg viewbox="-78.5 0 512 512"><path d="M98 460L64 426 227 262 64 98 98 64 294 262 98 460Z"></path></svg></a></div> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item nav-hidden"><a class="nav-link" onclick="goToTop()" id="top-button" style="display: none;">Back to Top</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../base_model">Previous Part</a></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../dataset">Next Part</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Sections </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/">Part 0. Introduction</a> <a class="dropdown-item " href="/tokenization/">Part 1. Tokenization</a> <a class="dropdown-item " href="/base_model/">Part 2. Base Model</a> <a class="dropdown-item " href="/sharded/">Part 3. Sharded Model</a> <a class="dropdown-item " href="/dataset/">Part 4. Dataset &amp; Config</a> <a class="dropdown-item " href="/distributed_training/">Part 5. Distributed Training</a> <a class="dropdown-item " href="/moe/">Part 6. Mixture of Experts</a> <a class="dropdown-item " href="/training/">Part 7. Training Results</a> <a class="dropdown-item " href="/conclusion/">Part 8. Conclusion</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Inside the Sharded Model:<br>3-D Parallelism</h1> <p>Part 3 of <a href="">Jaxformer</a> (<a href="../base_model">Part 2: Base Model</a> | <a href="../dataset">Part 4: Dataset &amp; Config</a>)</p> <p>Here we discuss the 4 main parallelism techniques used for scaling LLMs: data parallelism, fully-sharded data parallelism (FSDP), pipeline parallelism and tensor parallelism. For each, we discuss their theory and a scalable implementation.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#foundations-of-sharding">Foundations of Sharding</a> </div> <div> <a href="#data-parallelism">Data Parallelism</a> </div> <div> <a href="#pipeline-parallelism">Pipeline Parallelism</a> </div> <div> <a href="#tensor-parallelism">Tensor Parallelism</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#rmsnorm">RMSNorm</a> </li> <li> <a href="#embedding">Embedding</a> </li> <li> <a href="#rope">RoPE</a> </li> <li> <a href="#attention">Attention</a> </li> </ul> </nav> </d-contents> <h2 id="foundations-of-sharding">Foundations of Sharding</h2> <p>When scaling models in JAX, we need to explicitly control how the data and computations are partitioned. This is where the intuition behind manual parallelism techniques in JAX comes in.</p> <p>To begin, the environment can be set-up to simulate an arbitrary number of CPU devices, here 8 are being simulated. Note that all XLA flags must come before JAX imports because the flags are parsed once. This means they need to be defined in the environment before importing JAX to ensure they are recognized.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>

<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">XLA_FLAGS</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">--xla_force_host_platform_device_count=8</span><span class="sh">'</span>

<span class="kn">from</span> <span class="n">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="kn">from</span> <span class="n">jax.sharding</span> <span class="kn">import</span> <span class="n">Mesh</span><span class="p">,</span> <span class="n">PartitionSpec</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="n">jax.debug</span> <span class="kn">import</span> <span class="n">visualize_array_sharding</span>
<span class="kn">from</span> <span class="n">jax.sharding</span> <span class="kn">import</span> <span class="n">NamedSharding</span>
<span class="kn">from</span> <span class="n">jax.experimental.shard_map</span> <span class="kn">import</span> <span class="n">shard_map</span>
</code></pre></div></div> <p>The configuration of these devices can be shown by calling <code class="language-plaintext highlighter-rouge">jax.devices()</code> which returns</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="nc">CpuDevice</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
<span class="nc">CpuDevice</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<span class="nc">CpuDevice</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="nc">CpuDevice</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
<span class="nc">CpuDevice</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
<span class="nc">CpuDevice</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
<span class="nc">CpuDevice</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">6</span><span class="p">),</span>
<span class="nc">CpuDevice</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">7</span><span class="p">)]</span>
</code></pre></div></div> <p>Before explicitly sharding tensors across devices, we can create a <code class="language-plaintext highlighter-rouge">jax.sharding.Mesh</code> to define a grid of available devices, reshaping them into custom configurations and assigning a name to their axes. This allows for multi-dimension sharding along the defined axes (for example, data parallelism along one axis and pipeline parallelism on the other). In this case, since we have simulated 8 devices, they have been split into a $2 \times 4$ configuration along the <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> axes respectively (note the names are arbitrary, <code class="language-plaintext highlighter-rouge">x</code> represents the axis with 2 devices and <code class="language-plaintext highlighter-rouge">y</code> represents the axis with 4 devices).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">make_mesh</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">))</span>
</code></pre></div></div> <p>Now, to demonstrate sharded vector addition across 8 distinct devices, we can begin allocating two vectors a and b, which are reshaped to be of the same configuration as the device grid, and an element-wise addition function to be called on each individual device.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">vec_addition</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</code></pre></div></div> <p>Then, to call the function of each of the 8 devices, we can use <code class="language-plaintext highlighter-rouge">jax.shard_map</code> which maps a function over arrays distributed across devices. <code class="language-plaintext highlighter-rouge">Shard_add</code> is defined as a wrapper around the <code class="language-plaintext highlighter-rouge">shard_map</code> that maps the function <code class="language-plaintext highlighter-rouge">vec_addition</code> defined as element-wise addition, on the 2 x 4 device mesh, where each of the arguments to <code class="language-plaintext highlighter-rouge">vec_addition(a, b)</code> are split across the <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> axes, whilst the output is also split on each of the devices. This is represented by the partition spec object <code class="language-plaintext highlighter-rouge">P()</code> which specifies how the input and outputs should be partitioned across the devices.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">shard_add</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">shard_map</span><span class="p">(</span>
    <span class="n">vec_addition</span><span class="p">,</span>
    <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">,</span>
    <span class="n">in_specs</span><span class="o">=</span><span class="p">(</span><span class="nc">P</span><span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">),</span> <span class="nc">P</span><span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">)),</span>
    <span class="n">out_specs</span><span class="o">=</span><span class="nc">P</span><span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="nf">shard_add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nf">visualize_array_sharding</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</code></pre></div></div> <p>Using <code class="language-plaintext highlighter-rouge">visualize_array_sharding(c)</code>, we can see how the sum is split-element wise on each of the devices.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharded/1-480.webp 480w,/assets/img/sharded/1-800.webp 800w,/assets/img/sharded/1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sharded/1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>When printing the values of vectors a, b and c, we see that the element wise addition worked.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">Array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int32</span><span class="p">)</span> <span class="c1">#a
</span><span class="nc">Array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int32</span><span class="p">)</span> <span class="c1">#b
</span><span class="nc">Array</span><span class="p">([[</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">14</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int32</span><span class="p">)</span> <span class="c1">#c
</span></code></pre></div></div> <p>In the case where the whole vector c should be replicated across the devices, the following changes would need to be made. In the device-wise vector addition function, each device does element wise addition on its shard. Then, the first <code class="language-plaintext highlighter-rouge">all_gather</code>, along the mesh axis <code class="language-plaintext highlighter-rouge">x</code> concatenates the results along dimension <code class="language-plaintext highlighter-rouge">0</code> of the array. This results in each device along the same column with the same data, essentially collecting all elements column-wise. Then, the same is done row-wise along the <code class="language-plaintext highlighter-rouge">y</code> axis/dimension 1. The final local result is an array of shape <code class="language-plaintext highlighter-rouge">(2,4)</code>, essentially replicated across each device. So, the <code class="language-plaintext highlighter-rouge">shard_map</code> function on the bottom, calls the <code class="language-plaintext highlighter-rouge">vec_addition</code> function on each device which does local addition, then all gathers all elements for each device in the mesh defined above. The input vectors a and b are sharded across all the devices; however, the output remains <code class="language-plaintext highlighter-rouge">P()</code> because it means the output is replicated on all devices, instead of staying sharded. Then, the argument <code class="language-plaintext highlighter-rouge">check_vma=False</code> is passed because since JAX use vma to ensure the shardings are right, but it cannot infer that the all-gather has replicated the information fully. Thus, turning it off allows us to write whatever shardings we want and ensure correctness ourself (JAX compiler doesn’t help anymore). In this case we know that what we have done replicates.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">vec_addition</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">local_result</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">local_result</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">all_gather</span><span class="p">(</span><span class="n">local_result</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">,</span> <span class="n">tiled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">local_result</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">all_gather</span><span class="p">(</span><span class="n">local_result</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">,</span> <span class="n">tiled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">local_result</span>

<span class="n">shard_add</span> <span class="o">=</span> <span class="nf">shard_map</span><span class="p">(</span>
    <span class="n">vec_addition</span><span class="p">,</span>
    <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">,</span>
    <span class="n">in_specs</span><span class="o">=</span><span class="p">(</span><span class="nc">P</span><span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">),</span> <span class="nc">P</span><span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">)),</span>
    <span class="n">out_specs</span><span class="o">=</span><span class="nc">P</span><span class="p">(),</span>
    <span class="n">check_vma</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="nf">shard_add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nf">visualize_array_sharding</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</code></pre></div></div> <p>When visualizing the output, the following is shown where c remains the same sum as above. It shows that c is replicated the same across all devices.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharded/2-480.webp 480w,/assets/img/sharded/2-800.webp 800w,/assets/img/sharded/2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sharded/2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>That concludes an introduction to distributed training in JAX. These principles are then scaled up across higher-dimensional arrays to form the basis of modern distributed techniques including data, pipeline and tensor parallelism.</p> <h2 id="data-parallelism">Data Parallelism</h2> <p>There exist numerous parallelism strategies (data, tensor, pipeline) for training large language models. Data parallelism, as the name suggests, involves replicating the model across compute whilst parallelizing the data. At its core, data parallelism splits the batch size of the input shape <code class="language-plaintext highlighter-rouge">(B, T, C)</code> into smaller batches that are distributed across <code class="language-plaintext highlighter-rouge">n</code> devices <code class="language-plaintext highlighter-rouge">(B/n, T, C)</code>. In this way, we can increase the batch size as each device processes a subset of the data independently, in parallel. After computing the forward pass and obtaining the gradients, they are averaged across all the devices, using the <code class="language-plaintext highlighter-rouge">jax.lax.pmean(x, axis_name)</code>operation and updated across every model. Since the weights are replicated (have partition spec of <code class="language-plaintext highlighter-rouge">P()</code>) JAX automatically does a gradient sync. This operation, performs an all-reduce mean on <code class="language-plaintext highlighter-rouge">x</code> along the <code class="language-plaintext highlighter-rouge">axis_name</code> in the grid mesh of devices and thus the gradients will sync when <code class="language-plaintext highlighter-rouge">jax.grad</code> is called.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(...):</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="bp">...</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">pmean</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">'</span><span class="s">dp</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># reduce across data parallel
</span>
  <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></div> <p>The advantages of data parallelism allow for large-scale training with low communication bottlenecks as there is only one communication required. One of the main disadvantages of it is that the model is required to fit on each device, this can be infeasible as the model grows, hence data parallelism is often combined with other strategies including pipeline and tensor parallelism.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharded/3-480.webp 480w,/assets/img/sharded/3-800.webp 800w,/assets/img/sharded/3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sharded/3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Data parallelism with the model replicated across all GPUs whilst the batch is split into smaller batch sizes</figcaption> </figure> <p>Pure data parallelism doesn’t require changes in our model class. However, the biggest downside of data parallelism is that the model needs to be replicated in each instance. This leads to large memory usage. A way to fix this is to use an extension of DP known as Fully-Sharded Data Parallelism, where each model keeps a subset of the parameters and performs all-gathers to ensure that only a single instance of the parameters are replicated. The same goes for the gradients and optimizer states. To implement this, we only need to ensure the parameters are sharded since our gradients and optimizer state are as computed and sharded in the same partition spec as the parameters they represent.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharded/4.svg" sizes="95vw"></source> <img src="/assets/img/sharded/4.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">The third state of the zero optimizer above is equivalent to FSDP where all the gradients, parameters and optimizer states are sharded</figcaption> </figure> <p>We implement FSDP on the weight matrix for the dense network only. Since every dense layer is wrapped in our own class, this is reasonable for parameter sharding. We begin by writing the Dense module ourselves in terms of Flax Linen parameters instead of using the class given by Flax. We initialize a kernel which is our weights matrix and a bias. Then, we cast to the desired data type, perform a matrix multiplication and add the bias.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">features</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span>

    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
    <span class="n">kernel</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">param</span><span class="p">(</span>
      <span class="sh">"</span><span class="s">kernel</span><span class="sh">"</span><span class="p">,</span>
      <span class="n">nn</span><span class="p">.</span><span class="n">initializers</span><span class="p">.</span><span class="nf">lecun_normal</span><span class="p">(),</span>
      <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">features</span><span class="p">),</span>
      <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">bias</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">param</span><span class="p">(</span>
      <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span><span class="p">,</span>
      <span class="n">nn</span><span class="p">.</span><span class="n">initializers</span><span class="p">.</span><span class="n">zeros</span><span class="p">,</span>
      <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">features</span><span class="p">,),</span>
      <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span>
    <span class="p">)</span>

    <span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
      <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">dtype</span><span class="p">),</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">...d,df-&gt;...f</span><span class="sh">"</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>

    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p>For FSDP initialization, it is acceptable to replicate parameters across each sub-axis (both pipeline and tensor), since inference would not be possible otherwise, as FSDP is not used during inference. However, after initialization, we need to all-gather the kernel. This can be done by using <code class="language-plaintext highlighter-rouge">self.is_mutable_collection("params")</code> to determine what stage we are at. If we are in the initialization (params are mutable), we can initialize the kernel normally, otherwise since Flax manages the parameters of an <code class="language-plaintext highlighter-rouge">nn.Module</code>, we can collect the current kernel in the scope of the function and all gather it. For the <code class="language-plaintext highlighter-rouge">all gather</code>, we want to do it across the data parallel axis abbreviated in our mesh as <code class="language-plaintext highlighter-rouge">dp</code> along the last dim of the matrix and we want to concat them not stack them so we pass <code class="language-plaintext highlighter-rouge">Tiled=True</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="nf">is_mutable_collection</span><span class="p">(</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">kernel</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">param</span><span class="p">(</span>
      <span class="sh">"</span><span class="s">kernel</span><span class="sh">"</span><span class="p">,</span>
      <span class="n">nn</span><span class="p">.</span><span class="n">initializers</span><span class="p">.</span><span class="nf">lecun_normal</span><span class="p">(),</span>
      <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">features</span><span class="p">),</span>
      <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span>
    <span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">kernel</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">scope</span><span class="p">.</span><span class="nf">get_variable</span><span class="p">(</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">kernel</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">kernel</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">all_gather</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="sh">"</span><span class="s">dp</span><span class="sh">"</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tiled</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p>There are a few unanswered questions left here, such as how do we split the parameters after they are made, or how do we prevent JAX from storing the activation in memory for the backward pass (and thus eliminating the benefits of FSDP). These will be answered below but, assuming we are able to spilt the parameters (each kernel) across the <code class="language-plaintext highlighter-rouge">dp</code> axis<code class="language-plaintext highlighter-rouge">(x.shape[-1], self.features / dp.size)</code>, we are able to perform the desired FSDP operation. The rest of the <code class="language-plaintext highlighter-rouge">Dense</code> remains the same for now (Tensor Parallelism requires further operations). Therefore our Dense is:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">features</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span>

    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="nf">is_mutable_collection</span><span class="p">(</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">):</span>
            <span class="n">kernel</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">param</span><span class="p">(</span>
                <span class="sh">"</span><span class="s">kernel</span><span class="sh">"</span><span class="p">,</span>
                <span class="n">nn</span><span class="p">.</span><span class="n">initializers</span><span class="p">.</span><span class="nf">lecun_normal</span><span class="p">(),</span>
                <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">features</span><span class="p">),</span>
                <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">kernel</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">scope</span><span class="p">.</span><span class="nf">get_variable</span><span class="p">(</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">kernel</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">kernel</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">all_gather</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="sh">"</span><span class="s">dp</span><span class="sh">"</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tiled</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">bias</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">param</span><span class="p">(</span>
      <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span><span class="p">,</span>
      <span class="n">nn</span><span class="p">.</span><span class="n">initializers</span><span class="p">.</span><span class="n">zeros</span><span class="p">,</span>
      <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">features</span><span class="p">,),</span>
      <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span>
    <span class="p">)</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">dtype</span><span class="p">),</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">...d,df-&gt;...f</span><span class="sh">"</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>

        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h2 id="pipeline-parallelism">Pipeline Parallelism</h2> <p>Pipeline parallelism is another parallelism technique that allows for training LLMs across distributed nodes. While data parallelism, works well for smaller to intermediate models, when the model size increases, it becomes difficult to scale as the model can no longer fit on a single device. Hence, in such cases, strategies that parallelize the model instead of the data need to be used. In pipeline parallelism, the model is split vertically. This means the layers of the model are partitioned on different devices, for example, a transformer with 16 layers and 4 homogenous devices are split evenly (4 consecutive layers per device). The input batch passes through the first device with the first <code class="language-plaintext highlighter-rouge">n</code> layers, then the output of that device is passed to the next device through the next <code class="language-plaintext highlighter-rouge">n</code> layers and etc. The backwards pass is formed in the opposite direction from the last device, computing the gradient for the last <code class="language-plaintext highlighter-rouge">n</code> layers, then computing the back propagation through the next device and etc. Pipeline Parallelism is advantageous because each device requires a portion of the model, allowing for more scaling as memory requirements are reduced. Due to the nature of this parallelism, the following computation graph can be created.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharded/5-480.webp 480w,/assets/img/sharded/5-800.webp 800w,/assets/img/sharded/5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sharded/5.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Naive Pipeline Parallelism</figcaption> </figure> <p>Looking at the figure, it is evident that the forward pass of each state is dependent on the device before it and as a result, in the image above, the devices are idle for a large amount of time. This causes an low underutilization of devices as at any time step, only one device is being used. Hence, the GPipe Algorithm was introduced to increase device efficiency by splitting the batch size into mini batches (smaller, equal-sized batches) for which the forwards and backwards pass can be computed sequentially. Now, each device can immediately start working on the next micro-batch and can be overlapped over each partition. The idle time of the device is called a bubble, which can be reduced by choosing a smaller size of micro-batches.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharded/6-480.webp 480w,/assets/img/sharded/6-800.webp 800w,/assets/img/sharded/6-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sharded/6.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">GPipe Pipeline Parallelism</figcaption> </figure> <p>When looking at the fraction of time wasted by the bubble, the formula can be derived looking at the following image for naive pipeline parallelism.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharded/7-480.webp 480w,/assets/img/sharded/7-800.webp 800w,/assets/img/sharded/7-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sharded/7.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Naive Pipeline Parallelism with bubbles displayed</figcaption> </figure> <p>To calculate the portion of time as a bubble, assume $n$ to be the number of devices. Then, the amount of idle time for the top left bubble can be calculated as the arithmetic sum between 1 and n-1.</p> \[\frac{(n-1+1)(n-1)}{2} = \frac{n^2 - n}{2} = \frac{n(n-1)}{2}\] <p>The top right bubble is calculated as twice the top left bubble as the magnitude of time the backwards pass takes is twice that of the forwards pass. Hence, the top left bubble is $n(n-1)$. It is trivial to prove that the center bubbles are equal to the sum of the top left and top right bubbles, hence the final bubbles sum can be computed as:</p> \[2\frac{n(n-1)}{2} + 2n(n-1) = n(n-1) + 2n(n-1) = 3n(n-1)\] <p>The numerator of this ratio has been computed above; however, the denominator is computed as the total amount of time taken by all the devices. This ratio can be computed as $n(n + 2n) = 3n^2$. Thus, the ratio of time wasted in the naive pipeline is:</p> \[\frac{3n(n-1)}{3n^2} = \frac{(n-1)}{n}\] <p>It is evident that as n gets larger, the fraction of time wasted approaches 1, signifying heavy inefficiencies. Computing this ratio for the GPipe Algorithm yields the following.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharded/8-480.webp 480w,/assets/img/sharded/8-800.webp 800w,/assets/img/sharded/8-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sharded/8.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Bubble Image with GPipe</figcaption> </figure> <p>To calculate the total bubble ration we can use the same procedure as above to calculate the total bubble time as:</p> \[2\frac{n(n-1)}{2} + 2n(n-1) = n(n-1) + 2n(n-1) = 3n(n-1)\] <p>The total time taken is equivalent to the total area which is $n * 3(n+m-1)$ since in each the forward pass we have to do $n+m-1$ passes and twice that in the backwards pass. When dividing the two, we get:</p> \[\frac{3n(n-1)}{3n(n +m-1)} = \frac{n-1}{n+m-1}\] <p>Note that when $m = 1$, this equation becomes the same equation above. So, increasing the size of the mini batches, results in a smaller ratio of bubble-time wasted; however, we cannot infinitely increase the mini batch size because that will result in an underutilization of the GPUs and increase in communication costs, so we must maintain a balance between the two. GPipe papers have that when $m \geq 4n$ the communication cost becomes negligible.</p> <p>There are two main challenges when implementing pipeline. The first is the actual forward/backward pass and the second is setting up the parameters. We begin by setting up the parameters.</p> <p>Currently, our parameters are represented as a JAX PyTree (any Python data structure such as a list, tuple, or dictionary whose children are JAX arrays), specifically as a dictionary where the module keys serve as paths. For example if we want the first down block for the MLA, we can do <code class="language-plaintext highlighter-rouge">params['Block_0']['Layer_0']['MLA_0']['Dense_0'] = {'kernel': Array(...), 'bias': Array(...)}</code>. Now when we have a PyTree and use sharding functions (i.e <code class="language-plaintext highlighter-rouge">jax.device_put</code>) it maps over the tree hence if <code class="language-plaintext highlighter-rouge">p</code> is some PyTree, <code class="language-plaintext highlighter-rouge">jax.device_put(p, NamedSharding(...)) = jax.tree.map(lambda x: jax.device_put(x, NamedSharding(...)), p)</code>. This leads to a problem with the current Transformer class since it’s parameters are sequential, meaning it may have keys <code class="language-plaintext highlighter-rouge">Block_0</code>, <code class="language-plaintext highlighter-rouge">Block_1</code>, … <code class="language-plaintext highlighter-rouge">Block_n</code> where we want to shard the first <code class="language-plaintext highlighter-rouge">n/pp_size</code> blocks on the first device, then the blocks from <code class="language-plaintext highlighter-rouge">n/pp_size + 1, 2n/pp_size</code> on the second device and so on. One way to fix this and make it more natural in JAX is to consider partitioning only across the Blocks. Then since each of the params in <code class="language-plaintext highlighter-rouge">Block_0</code>, <code class="language-plaintext highlighter-rouge">Block_1</code>, … ,<code class="language-plaintext highlighter-rouge">Block_n</code> are identical (they all have the layers defined), we can create the parameter dictionary as one block with all the parameters stacked. This allows the parameters to be sharded across the pipeline axis. Instead of having <code class="language-plaintext highlighter-rouge">params = {'Block_0': {...}, ... 'Block_n': {...}}</code>, we now have <code class="language-plaintext highlighter-rouge">params = {'Block_0': {...}}</code>, where each block includes a leading axis. For example, instead of a kernel having the shape <code class="language-plaintext highlighter-rouge">(4, 8)</code>, it now has the shape <code class="language-plaintext highlighter-rouge">(L, 4, 8)</code>, where <code class="language-plaintext highlighter-rouge">L</code> is the number of layers in the model.</p> <p>To begin writing this out, we can create a new class called <code class="language-plaintext highlighter-rouge">ShardedModel</code> which will be used to implement all the sharded features. In the constructor, we can split the embedding and block into two separate components since we will want to manipulate the parameters of the block independent of the embedding module.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">shardedModel</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">modelConfig</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="nf">convert_dtype</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="nc">Embedding</span><span class="p">(</span>
            <span class="n">vocab_size</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span>
            <span class="n">model_dimension</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span>
            <span class="n">model_dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">block</span> <span class="o">=</span> <span class="nc">Block</span><span class="p">(</span>
            <span class="n">layers</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">layers_per_block</span><span class="p">,</span>
            <span class="n">model_dimension</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span>
            <span class="n">n_heads</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span>
            <span class="n">T</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">T</span><span class="p">,</span>
            <span class="n">latent_dim</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">,</span>
            <span class="n">dhR</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">dhR</span><span class="p">,</span>
            <span class="n">dropout_rate</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
            <span class="n">model_dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
</code></pre></div></div> <p>Before we can write the initialization method for the weights, we need to have some function that inputs the config of the model and returns a Partition Spec of the parameters. This will allow us to write the init method under a shard map, allowing for direct creation on devices rather then transfer which would defeat the whole purpose of using parallelism methods.</p> <p>To do this efficiently, we use the <code class="language-plaintext highlighter-rouge">jax.eval_shape</code> function, which returns the shapes of a function’s outputs. Since we do not care about the actual values, only the dimensions, we can use these shapes to construct the final PyTree structure and the PartitionSpec.</p> <p>The function first takes a few variables that are needed to make the mock data such as the sequence length <code class="language-plaintext highlighter-rouge">T</code> and the number of layers and number of devices. It then sets up the mock data and a key needed for the <code class="language-plaintext highlighter-rouge">init</code> methods which generate the fake parameters (again fake because we aren’t actually going to use these parameters it just tells us the structure we are working with).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">shardedModel</span><span class="p">:</span>
  <span class="bp">...</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">get_p_spec</span><span class="p">(</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">Block</span><span class="p">],</span> <span class="n">mesh</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">Mesh</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">modelConfig</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">NamedSharding</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">NamedSharding</span><span class="p">]:</span>
        <span class="n">T</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">T</span>
        <span class="n">n_devices</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">.</span><span class="n">devices</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">n_layers</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">blocks</span>
        <span class="k">assert</span> <span class="n">n_layers</span> <span class="o">%</span> <span class="n">n_devices</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="sh">"</span><span class="s">Number of layers must be divisible by number of devices</span><span class="sh">"</span>
        <span class="p">)</span>

        <span class="n">embed</span><span class="p">,</span> <span class="n">layer</span> <span class="o">=</span> <span class="n">model</span>

        <span class="n">x_embed</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">x_layer</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">embed</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <p>Then, we write a function that <code class="language-plaintext highlighter-rouge">eval_shape</code> can call to generate the fake parameters. This function is placed under a <code class="language-plaintext highlighter-rouge">shard_map</code> since we want to replicate the stacked structure. Note that for the out spec, we replicate the embedding params on every device and the layer we concatenate on the pipeline axis. This differs from the real output of the model since some of the parameters such as the kernels of any dense layer are also split in the FSDP style. We first init the embed module normally. Then, we make <code class="language-plaintext highlighter-rouge">n_layer // n_devices</code> of the layer module and stack each array in this PyTree onto one dim. This way, when we concat on the <code class="language-plaintext highlighter-rouge">pp</code> axis, we are able to get the parameters aligned on one dimension which will be sharded in pipeline parallelism.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">get_p_spec</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="nd">@partial</span><span class="p">(</span>
    <span class="n">jax</span><span class="p">.</span><span class="n">shard_map</span><span class="p">,</span>
    <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">,</span>
    <span class="n">in_specs</span><span class="o">=</span><span class="p">(</span><span class="nc">P</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="nc">P</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)),</span>
    <span class="n">out_specs</span><span class="o">=</span><span class="p">(</span><span class="nc">P</span><span class="p">(),</span> <span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">)),</span>
  <span class="p">)</span>
  <span class="k">def</span> <span class="nf">get_var_spec_shard</span><span class="p">(</span><span class="n">x_embed</span><span class="p">,</span> <span class="n">x_layer</span><span class="p">):</span>
    <span class="n">embed_shape</span> <span class="o">=</span> <span class="n">embed</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">x_embed</span><span class="p">)[</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">layer_shape</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">//</span> <span class="n">n_devices</span><span class="p">):</span>
      <span class="n">layer_shape</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">layer</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">x_layer</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">])</span>
    <span class="n">layer_shape</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="o">*</span><span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="o">*</span><span class="n">layer_shape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">embed_shape</span><span class="p">,</span> <span class="n">layer_shape</span>

  <span class="n">eval_shape</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">eval_shape</span><span class="p">(</span>
    <span class="n">get_var_spec_shard</span><span class="p">,</span>
    <span class="n">x_embed</span><span class="p">,</span>
    <span class="n">x_layer</span><span class="p">,</span>
  <span class="p">)</span>
</code></pre></div></div> <p>We can now use <code class="language-plaintext highlighter-rouge">jax.tree.map</code> to go through the shapes and convert them to the desired <code class="language-plaintext highlighter-rouge">PartitionSpec</code>. If we are in a layer parameter, we want to split everything on the first axis across the <code class="language-plaintext highlighter-rouge">pp</code> axis but only the kernels (which are 3 dim) along the <code class="language-plaintext highlighter-rouge">dp</code> axis since we perform the all-gather to collect the params in FSDP. We keep explicit representations for <code class="language-plaintext highlighter-rouge">gamma/beta</code> since for future parallelism like tensor, we will need to revisit these rules. Embeddings will be replicated on each device for now since we only need to split the the block across the pipeline axis.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">get_p_spec</span><span class="p">(...):</span>
  <span class="n">join_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">path</span><span class="p">:</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="n">key</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">path</span><span class="p">).</span><span class="nf">lower</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">layer_partition</span><span class="p">(</span><span class="n">key</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="p">...],</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">P</span><span class="p">:</span>
    <span class="n">path</span> <span class="o">=</span> <span class="nf">join_fn</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

    <span class="k">if</span> <span class="sh">"</span><span class="s">gamma</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">path</span> <span class="ow">or</span> <span class="sh">"</span><span class="s">beta</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">path</span><span class="p">:</span>
      <span class="k">return</span> <span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
      <span class="k">return</span> <span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="sh">"</span><span class="s">dp</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

  <span class="n">embed_p_spec</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nc">P</span><span class="p">(</span>
      <span class="o">*</span><span class="p">(</span><span class="bp">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span><span class="p">)),</span>
    <span class="p">),</span>
    <span class="n">eval_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
  <span class="p">)</span>

  <span class="n">layer_p_spec</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map_with_path</span><span class="p">(</span>
    <span class="n">layer_partition</span><span class="p">,</span>
    <span class="n">eval_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
  <span class="p">)</span>

  <span class="k">return</span> <span class="n">embed_p_spec</span><span class="p">,</span> <span class="n">layer_p_spec</span>
</code></pre></div></div> <p>We can then now begin writing the <code class="language-plaintext highlighter-rouge">init_weights</code> method. It will follow in similar structure to the <code class="language-plaintext highlighter-rouge">get_p_spec</code> function. We begin by getting the <code class="language-plaintext highlighter-rouge">out_spec</code>. Then, we will replace the <code class="language-plaintext highlighter-rouge">dp</code> axes in any of the layer partition with <code class="language-plaintext highlighter-rouge">None</code> for now since in initialization, we don’t want to split the <code class="language-plaintext highlighter-rouge">Dense</code> kernel’s across the <code class="language-plaintext highlighter-rouge">dp</code> axis.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">shardedModel</span><span class="p">:</span>

  <span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
    <span class="n">out_spec</span> <span class="o">=</span> <span class="n">shardedModel</span><span class="p">.</span><span class="nf">get_p_spec</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">block</span><span class="p">],</span> <span class="n">mesh</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">replace_fsdp</span><span class="p">(</span><span class="n">p</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">PartitionSpec</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">p</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="sh">"</span><span class="s">dp</span><span class="sh">"</span><span class="p">:</span>
                <span class="n">p</span> <span class="o">=</span> <span class="nc">P</span><span class="p">(</span><span class="o">*</span><span class="n">p</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">None</span><span class="p">)</span> <span class="c1"># remove None from last position
</span>            <span class="k">return</span> <span class="n">p</span>

        <span class="n">out_spec_no_fsdp</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">replace_fsdp</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">out_spec</span><span class="p">)</span>
</code></pre></div></div> <p>We can then prepare our init variables, namely our mock data and unique keys for each layer to ensure that each layer being created is not an identical copy.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">init_weights</span><span class="p">(...):</span>
    <span class="bp">...</span>

    <span class="n">x_embed</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">T</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">x_layer</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">layer_devices</span> <span class="o">=</span> <span class="n">mesh</span><span class="p">.</span><span class="n">devices</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">blocks</span> <span class="o">//</span> <span class="n">layer_devices</span><span class="p">,</span> <span class="sh">"</span><span class="s">Number of blocks must be divisible by number of devices</span><span class="sh">"</span>
    <span class="n">layers_per_device</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">blocks</span> <span class="o">//</span> <span class="n">layer_devices</span>

    <span class="n">key</span><span class="p">,</span> <span class="n">embed_key</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">key</span><span class="p">,</span> <span class="o">*</span><span class="n">layer_keys</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">layer_devices</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">layer_keys</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">layer_keys</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">layer_devices</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># make into jax array
</span></code></pre></div></div> <p>We can now write out sub-function <code class="language-plaintext highlighter-rouge">init_params</code> identical to sub function in the <code class="language-plaintext highlighter-rouge">get_p_spec</code> only now using different keys.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(...):</span>
  <span class="bp">...</span>

  <span class="nd">@jax.jit</span>
  <span class="nd">@partial</span><span class="p">(</span>
    <span class="n">jax</span><span class="p">.</span><span class="n">shard_map</span><span class="p">,</span>
    <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">,</span>
    <span class="n">in_specs</span><span class="o">=</span><span class="p">(</span><span class="nc">P</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="nc">P</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">)),</span>
    <span class="n">out_specs</span><span class="o">=</span><span class="n">out_spec_no_fsdp</span><span class="p">,</span>
  <span class="p">)</span>
  <span class="k">def</span> <span class="nf">init_params</span><span class="p">(</span><span class="n">x_embed</span><span class="p">,</span> <span class="n">x_layer</span><span class="p">,</span> <span class="n">layer_key</span><span class="p">):</span>
    <span class="n">layer_key</span> <span class="o">=</span> <span class="n">layer_key</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">embedding_params</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span>
      <span class="n">embed_key</span><span class="p">,</span>
      <span class="n">x_embed</span><span class="p">,</span>
      <span class="n">out</span><span class="o">=</span><span class="bp">False</span>
    <span class="p">)[</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">layer_params</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">layers_per_device</span><span class="p">):</span>
      <span class="n">layer_key</span><span class="p">,</span> <span class="n">init_key</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">layer_key</span><span class="p">)</span>
      <span class="n">current_params</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">block</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">init_key</span><span class="p">,</span> <span class="n">x_layer</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span>
        <span class="sh">"</span><span class="s">params</span><span class="sh">"</span>
      <span class="p">]</span>
      <span class="n">layer_params</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">current_params</span><span class="p">)</span>
    <span class="n">layer_params</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
      <span class="k">lambda</span> <span class="o">*</span><span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
      <span class="o">*</span><span class="n">layer_params</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">embedding_params</span><span class="p">,</span> <span class="n">layer_params</span>
</code></pre></div></div> <p>We can call this to get back our variables and use <code class="language-plaintext highlighter-rouge">device_put</code> to move them to the Partition Spec with FSDP.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(...):</span>
  <span class="n">out_spec</span> <span class="o">=</span> <span class="n">shardedModel</span><span class="p">.</span><span class="nf">get_p_spec</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">block</span><span class="p">],</span> <span class="n">mesh</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">)</span>
  <span class="bp">...</span>
  <span class="n">out_spec_no_fsdp</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">replace_fsdp</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">out_spec</span><span class="p">)</span>

  <span class="bp">...</span>
  <span class="nd">@jax.jit</span>
  <span class="nd">@partial</span><span class="p">(</span>
    <span class="n">jax</span><span class="p">.</span><span class="n">shard_map</span><span class="p">,</span>
    <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">,</span>
    <span class="n">in_specs</span><span class="o">=</span><span class="p">(</span><span class="nc">P</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="nc">P</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">)),</span>
    <span class="n">out_specs</span><span class="o">=</span><span class="n">out_spec_no_fsdp</span><span class="p">,</span>
  <span class="p">)</span>
  <span class="k">def</span> <span class="nf">init_params</span><span class="p">(</span><span class="n">x_embed</span><span class="p">,</span> <span class="n">x_layer</span><span class="p">,</span> <span class="n">layer_key</span><span class="p">):</span>
    <span class="bp">...</span>
    <span class="k">return</span> <span class="n">embedding_params</span><span class="p">,</span> <span class="n">layer_params</span>

  <span class="n">params</span> <span class="o">=</span> <span class="nf">init_params</span><span class="p">(</span><span class="n">x_embed</span><span class="p">,</span> <span class="n">x_layer</span><span class="p">,</span> <span class="n">layer_keys</span><span class="p">)</span>
  <span class="n">params</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_put</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">y</span><span class="p">)),</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">out_spec</span><span class="p">,</span>
  <span class="p">)</span>

  <span class="k">return</span> <span class="n">params</span>
</code></pre></div></div> <p>Now, we can move on to the actual forward pass for the pipeline implementation. We’ll call this step <code class="language-plaintext highlighter-rouge">pipe_step</code>, and it will take the same arguments as a standard <code class="language-plaintext highlighter-rouge">model.apply(...)</code> call. We begin by unpacking the parameters (since they are provided as a tuple) and if the cache is not <code class="language-plaintext highlighter-rouge">None</code> taking the last token in <code class="language-plaintext highlighter-rouge">x</code> similar to what we did in the <code class="language-plaintext highlighter-rouge">Transformer</code> class. We can then apply the <code class="language-plaintext highlighter-rouge">self.embeddings</code> module like a normal JAX module.</p> <p>For now, we’ll comment out the pipeline implementation for the layers by treating it as a black box and assuming the embeddings output is passed through it. We can then reapply <code class="language-plaintext highlighter-rouge">self.embedding</code> with<code class="language-plaintext highlighter-rouge">out=True</code> to obtain the final logits.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipe_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="n">embedding_params</span><span class="p">,</span> <span class="n">layer_params</span> <span class="o">=</span> <span class="n">params</span>

  <span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]</span>

  <span class="n">embeddings</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">.</span><span class="nf">apply</span><span class="p">({</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">:</span> <span class="n">embedding_params</span><span class="p">},</span> <span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

  <span class="c1"># some pipeline implmentation here
</span>  <span class="c1"># embeddings become layer_out
</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">.</span><span class="nf">apply</span><span class="p">({</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">:</span> <span class="n">embedding_params</span><span class="p">},</span> <span class="n">layer_out</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">cache</span>

</code></pre></div></div> <p>So far this is identical to the transformer. We now turn our attention to the actual pipeline implementation.</p> <p>We start by writing a forward function that passes through a single batch through <code class="language-plaintext highlighter-rouge">self.block</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipe_step</span><span class="p">(...):</span>
  <span class="n">embedding_params</span><span class="p">,</span> <span class="n">layer_params</span> <span class="o">=</span> <span class="n">params</span>

  <span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]</span>

  <span class="n">embeddings</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">.</span><span class="nf">apply</span><span class="p">({</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">:</span> <span class="n">embedding_params</span><span class="p">},</span> <span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

  <span class="n">layer_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">block</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">:</span> <span class="n">params</span><span class="p">},</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">,</span>
    <span class="n">rngs</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">dropout</span><span class="sh">"</span><span class="p">:</span> <span class="n">key</span><span class="p">}</span> <span class="k">if</span> <span class="n">train</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
  <span class="p">)</span>
  <span class="bp">...</span>
  <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div></div> <p>There are a few downsides of such a simple layer function. The first is we can speed up implementation if we know we do not have to compute the gradient for some pipeline stages, namely the stages in the bubble. Below we will see that the stage is originally made with <code class="language-plaintext highlighter-rouge">nan</code> values hence we can write a wrapper on this function to choose between a stop-gradient method if there is a <code class="language-plaintext highlighter-rouge">nan</code>, otherwise call this layer function. Specially we can keep a <code class="language-plaintext highlighter-rouge">state_idx</code> which will be written below that indexes into the array for which function should be used. We can also remat (a.k.a checkpoint) this function to save memory since we are training on TPU’s whose individual HBM are quite low (&lt; 30GB).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipe_step</span><span class="p">(...):</span>

  <span class="n">layer_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">block</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span>
            <span class="p">{</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">:</span> <span class="n">params</span><span class="p">},</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
            <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">,</span>
            <span class="n">rngs</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">dropout</span><span class="sh">"</span><span class="p">:</span> <span class="n">key</span><span class="p">}</span> <span class="k">if</span> <span class="n">train</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
        <span class="p">)</span>

  <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="n">checkpoint_policies</span><span class="p">.</span><span class="n">nothing_saveable</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">fwd_fn</span><span class="p">(</span><span class="n">state_idx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="n">stop_grad</span><span class="p">):</span>
      <span class="nf">return </span><span class="p">(</span>
        <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">stop_gradient</span><span class="p">(</span><span class="nf">layer_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">stop_grad</span>
        <span class="k">else</span> <span class="nf">layer_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
      <span class="p">)</span>

    <span class="n">fns</span> <span class="o">=</span> <span class="p">[</span>
      <span class="nf">grad_fn</span><span class="p">(</span><span class="n">stop_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
      <span class="nf">grad_fn</span><span class="p">(</span><span class="n">stop_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">switch</span><span class="p">(</span>
      <span class="n">state_idx</span><span class="p">,</span>
      <span class="n">fns</span><span class="p">,</span>
      <span class="n">x</span><span class="p">,</span>
      <span class="n">params</span><span class="p">,</span>
      <span class="n">cache</span><span class="p">,</span>
      <span class="n">key</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div> <p>We can now write the function that will execute the GPipe phase, which we will call <code class="language-plaintext highlighter-rouge">pipeline.</code> This function takes the forward function to be executed at each stage (our <code class="language-plaintext highlighter-rouge">layer_fn</code> from the previous code block). The <code class="language-plaintext highlighter-rouge">stage_params</code> are the stacked parameters for the local layers on the device. For example, if we have $L$ layers and $n$ devices, the leading dimension of each parameter’s shape is $L/n$. Concretely, a kernel with input size 4 and output size 8, with $L = 10$ and $n = 2$, would have <code class="language-plaintext highlighter-rouge">stage_params</code> of shape <code class="language-plaintext highlighter-rouge">(5, 4, 8)</code>. The inputs are the local inputs arranged into microbatches per device. If $x \in \text{dataset}$ has a global shape <code class="language-plaintext highlighter-rouge">(M, B, T)</code>, where $M$ is the total number of microbatches, $B$ is the batch size per microbatch, and $T$ is the sequence length, then under the pipeline (since it runs inside a <code class="language-plaintext highlighter-rouge">shard_map</code>), the shape becomes <code class="language-plaintext highlighter-rouge">(M / pp_size, ...)</code> because each device processes an equal share of the total microbatches. The cache corresponds to the KV-cache at each stage and the key is the main JAX key for the specific device.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(</span>
  <span class="n">self</span><span class="p">,</span>
  <span class="n">fn</span><span class="p">,</span>
  <span class="n">stage_params</span><span class="p">:</span> <span class="n">PyTree</span><span class="p">,</span>
  <span class="n">inputs</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span>
  <span class="n">cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]]],</span>
  <span class="n">key</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">PRNGKey</span><span class="p">,</span>
<span class="p">):</span>

  <span class="c1"># implementation goes here
</span>
  <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">out_cache</span>

</code></pre></div></div> <p>The first step is to get all the variables needed to define our pipeline loop. That is we need</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(...):</span>
  <span class="n">device_idx</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">axis_index</span><span class="p">(</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># current device in pp axis
</span>  <span class="n">n_devices</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">axis_size</span><span class="p">(</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># total devices
</span>  <span class="n">layers_per_device</span> <span class="o">=</span> <span class="n">stage_params</span><span class="p">[</span><span class="sh">"</span><span class="s">Layer_0</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">MLA_0</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">Dense_0</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">kernel</span><span class="sh">"</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span>
    <span class="mi">0</span>
  <span class="p">]</span> <span class="c1"># layers per device
</span>  <span class="n">layers</span> <span class="o">=</span> <span class="n">layers_per_device</span> <span class="o">*</span> <span class="n">n_devices</span> <span class="c1"># total layers
</span>  <span class="n">microbatch_per_device</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1">#  microbatch per device
</span>  <span class="n">microbatches</span> <span class="o">=</span> <span class="n">n_devices</span> <span class="o">*</span> <span class="n">microbatch_per_device</span> <span class="c1"># total microbatches
</span></code></pre></div></div> <p>We can then create our outputs with the same shape as the inputs and our state, which is a buffer of the input/output for all the layers on the current device (this will be used to send data to different devices). Additionally, we create the mask matrix for states that are carrying <code class="language-plaintext highlighter-rouge">nan</code> values and the permutation that we will use a bit later. The permutation is just an array of tuples with increment values to indicate which pairs of devices will communicate (each device will communicate with its neighbour in the given arrangement). We also make the arrays for the KV-cache identical to the <code class="language-plaintext highlighter-rouge">Transformer</code> class.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="n">nan</span>

  <span class="n">state</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span>
      <span class="p">(</span>
        <span class="n">layers_per_device</span><span class="p">,</span>
        <span class="o">*</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span>
      <span class="p">)</span>
    <span class="p">)</span>
    <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="n">nan</span>
  <span class="p">)</span>

  <span class="n">state_idx</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">layers_per_device</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="n">perm</span> <span class="o">=</span> <span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">n_devices</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_devices</span><span class="p">)]</span>

  <span class="n">KV_cache</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">KR_cache</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></div> <p>As explained above, the total number of steps in the forward pass is <code class="language-plaintext highlighter-rouge">n + m - 1</code> where $n$ is the number of devices , $m$ is the total microbatches. However this is a simplification, as the true number of steps is $L + m - 1$ where $L$ is the total number of layers since we now have to consider if there is more then 1 layer per device. In each stage we have to do 3 steps. The first is to load the correct data and prepare the arguments (KV-cache, etc.), the next is to actually call the forward function and the next is to communicate the data. The first variable is <code class="language-plaintext highlighter-rouge">batch_idx</code>, which indicates the current microbatch being processed by the device. For each interval of <code class="language-plaintext highlighter-rouge">microbatch_per_device</code>, the device uses its local inputs, after which it rotates to obtain the next batch from another device. After we have gone through all the microbatches (<code class="language-plaintext highlighter-rouge">i &gt; microbatches - 1</code>), the <code class="language-plaintext highlighter-rouge">batch_idx</code> becomes meaningless (we have reached the stage where the first device no longer is providing useful outputs). Similarly the <code class="language-plaintext highlighter-rouge">layer_idx</code> tells us which index of the output we are on. It only becomes useful after $i &gt; L - 2$ since that is when the first microbatch has passed through the last layer. After we have completed <code class="language-plaintext highlighter-rouge">microbatches_per_device</code> steps, we rotate the output to start filling it for the next device’s microbatches. After we have computed both indexes, we set the state’s 0 index if we are on the first device for pipeline (essentially the device that holds the first layer) and set it equal to the <code class="language-plaintext highlighter-rouge">batch_idx</code> of the input, otherwise we keep the current state value. Similarly we set the <code class="language-plaintext highlighter-rouge">state_idx</code>’s 0 index at the 0 device to be 1 indicating it is no longer filled with <code class="language-plaintext highlighter-rouge">nan</code> values. We also make enough keys for the layers on this device for the forward computation and if the cache is not <code class="language-plaintext highlighter-rouge">None</code>, we make a tuple of the cache values.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">microbatches</span> <span class="o">+</span> <span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">batch_idx</span> <span class="o">=</span> <span class="n">i</span> <span class="o">%</span> <span class="n">microbatch_per_device</span>
    <span class="n">layer_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">layers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">microbatch_per_device</span>

    <span class="n">state</span> <span class="o">=</span> <span class="n">state</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">device_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">state_idx</span> <span class="o">=</span> <span class="n">state_idx</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">device_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">state_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

    <span class="n">key</span><span class="p">,</span> <span class="o">*</span><span class="n">layer_keys</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">layers_per_device</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">layer_keys</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">layer_keys</span><span class="p">)</span>

    <span class="n">current_cache</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">current_cache</span> <span class="o">=</span> <span class="p">[</span><span class="n">cache</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="bp">None</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">cache</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">current_cache</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
</code></pre></div></div> <p>We can now use the <code class="language-plaintext highlighter-rouge">jax.vmap</code> function to use vectorize the forward pass for the layers on this device. The function to vectorize over is the function given as a parameter and we pass in all the variables we have prepared. This now becomes our new state and cache.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">microbatches</span> <span class="o">+</span> <span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
    <span class="bp">...</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">out_cache</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">fn</span><span class="p">)(</span>
      <span class="n">state_idx</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">stage_params</span><span class="p">,</span> <span class="n">current_cache</span><span class="p">,</span> <span class="n">layer_keys</span>
    <span class="p">)</span>
</code></pre></div></div> <p>We are now on the final step which is to prepare the outputs. We append the out cache again identical to the <code class="language-plaintext highlighter-rouge">Tranformer</code> class and set the outputs at the <code class="language-plaintext highlighter-rouge">layer_idx</code> to the last state if this is the last device since that is the last layer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">microbatches</span> <span class="o">+</span> <span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
    <span class="bp">...</span>
    <span class="k">if</span> <span class="n">out_cache</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">KV_cache</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">out_cache</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">out_cache</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">KR_cache</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">out_cache</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">].</span><span class="nf">set</span><span class="p">(</span>
      <span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">device_idx</span> <span class="o">==</span> <span class="n">n_devices</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">state</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">outputs</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">])</span>
    <span class="p">)</span>
</code></pre></div></div> <p>We now need to rotate the state values across the pipeline devices. To achieve this, we use the <code class="language-plaintext highlighter-rouge">jax.lax.ppermute</code> communication operation, which sends a JAX array along a specified axis according to a given permutation. Specifically, we permute the last index of the <code class="language-plaintext highlighter-rouge">state</code> along the <code class="language-plaintext highlighter-rouge">pp</code> axis using the defined permutation and then prepend it to the front of the state. This is because we are collecting the last state from the previous device, which must now be passed into the first layer. The remaining <code class="language-plaintext highlighter-rouge">state</code> values stay the same but are shifted down by one. The same procedure is applied to <code class="language-plaintext highlighter-rouge">state_idx</code>, since it serves as a mask over the <code class="language-plaintext highlighter-rouge">state</code> values</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">microbatches</span> <span class="o">+</span> <span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
    <span class="bp">...</span>
    <span class="k">if</span> <span class="n">out_cache</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">KV_cache</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">out_cache</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">out_cache</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">KR_cache</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">out_cache</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">].</span><span class="nf">set</span><span class="p">(</span>
      <span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">device_idx</span> <span class="o">==</span> <span class="n">n_devices</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">state</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">outputs</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">])</span>
    <span class="p">)</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concat</span><span class="p">(</span>
          <span class="p">[</span><span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">ppermute</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="n">perm</span><span class="p">)[</span><span class="bp">None</span><span class="p">,</span> <span class="p">...],</span> <span class="n">state</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span>
          <span class="n">axis</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">)</span>

    <span class="n">state_idx</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concat</span><span class="p">(</span>
      <span class="p">[</span>
        <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">ppermute</span><span class="p">(</span><span class="n">state_idx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="n">perm</span><span class="p">)[</span><span class="bp">None</span><span class="p">,</span> <span class="p">...],</span>
        <span class="n">state_idx</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
      <span class="p">],</span>
      <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div> <p>The other two arrays that may need to be shifted are the inputs and the outputs. If <code class="language-plaintext highlighter-rouge">batch_idx</code> has reached the last microbatch, i.e., <code class="language-plaintext highlighter-rouge">batch_idx == microbatch_per_device - 1</code>, we must also permute the inputs to fetch a fresh batch. Similarly, for the outputs, when we reach <code class="language-plaintext highlighter-rouge">microbatch_per_device - 1</code>, we rotate to begin filling the next device buffer. For the inputs, it is important to note that once $i &gt; M - 1$, no further rotation is needed, since all inputs have already been processed. For the outputs, although we are continuously filling and permuting the array, it only becomes relevant once $i &gt; L - 2$, because at $L - 1$, the first batch reaches the final output and starts populating the output array. From $L - 1$ onward, we must step $M$ more times, which ensures that each device fills its output array exactly once.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">microbatches</span> <span class="o">+</span> <span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
    <span class="bp">...</span>
    <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">==</span> <span class="n">microbatch_per_device</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">ppermute</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="n">perm</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">layer_idx</span> <span class="o">==</span> <span class="n">microbatch_per_device</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">ppermute</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="n">perm</span><span class="p">)</span>
</code></pre></div></div> <p>With that we are done the staging loop. We permute the output array one more time since from $i = L - 1$ until $i = M + L - 1$ we have fully rotted the outputs arrays meaning the last device (device n) has the final output for device 1, device 1 has the output for device 2 and so on. We also prepare the final KV-cache</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(...):</span>
    <span class="bp">...</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">ppermute</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span>

  <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">KV_cache</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">KV_cache</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">KV_cache</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">KV_cache</span> <span class="o">=</span> <span class="bp">None</span>

  <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">KR_cache</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">KR_cache</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">KR_cache</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">KR_cache</span> <span class="o">=</span> <span class="bp">None</span>
  <span class="n">out_cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">KV_cache</span><span class="p">,</span> <span class="n">KR_cache</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">out_cache</span>
</code></pre></div></div> <p>We can now call this in our pipe_step method to complete our sharded forward pass.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipe_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">cache</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="n">embedding_params</span><span class="p">,</span> <span class="n">layer_params</span> <span class="o">=</span> <span class="n">params</span>

  <span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[...,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>

  <span class="n">embeddings</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">.</span><span class="nf">apply</span><span class="p">({</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">:</span> <span class="n">embedding_params</span><span class="p">},</span> <span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

  <span class="n">layer_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">block</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">:</span> <span class="n">params</span><span class="p">},</span>
    <span class="n">x</span><span class="p">,</span>
    <span class="n">cache</span><span class="o">=</span><span class="n">cache</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">,</span>
    <span class="n">rngs</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">dropout</span><span class="sh">"</span><span class="p">:</span> <span class="n">key</span><span class="p">}</span> <span class="k">if</span> <span class="n">train</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
  <span class="p">)</span>

  <span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="n">checkpoint_policies</span><span class="p">.</span><span class="n">nothing_saveable</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">fwd_fn</span><span class="p">(</span><span class="n">state_idx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="n">stop_grad</span><span class="p">):</span>
      <span class="nf">return </span><span class="p">(</span>
        <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">stop_gradient</span><span class="p">(</span><span class="nf">layer_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">stop_grad</span>
        <span class="k">else</span> <span class="nf">layer_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
      <span class="p">)</span>

    <span class="n">fns</span> <span class="o">=</span> <span class="p">[</span>
      <span class="nf">grad_fn</span><span class="p">(</span><span class="n">stop_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
      <span class="nf">grad_fn</span><span class="p">(</span><span class="n">stop_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">switch</span><span class="p">(</span>
      <span class="n">state_idx</span><span class="p">,</span>
      <span class="n">fns</span><span class="p">,</span>
      <span class="n">x</span><span class="p">,</span>
      <span class="n">params</span><span class="p">,</span>
      <span class="n">cache</span><span class="p">,</span>
      <span class="n">key</span><span class="p">,</span>
    <span class="p">)</span>

  <span class="n">layer_out</span><span class="p">,</span> <span class="n">out_cache</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pipeline</span><span class="p">(</span>
    <span class="n">fwd_fn</span><span class="p">,</span> <span class="n">layer_params</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">key</span>
  <span class="p">)</span>

  <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">.</span><span class="nf">apply</span><span class="p">({</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">:</span> <span class="n">embedding_params</span><span class="p">},</span> <span class="n">layer_out</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">out_cache</span>
</code></pre></div></div> <h2 id="tensor-parallelism">Tensor Parallelism</h2> <p>Another model parallelism (splits the model across devices instead of data) strategy is tensor parallelism. In this strategy the model is split across it’s feature dimension. An advantage of tensor parallelism is that it doesn’t face similar problems to pipeline parallelism’s bubble problems because all devices work on the same batch of data together. Tensor parallelism strongly relies on communication between different devices and is thus a popular strategy when training on TPUs due to the ICI connections between a large number of chips in a single pod (think nodes for GPUs). Suppose the model had a feature size of 512 and there were 4 devices, then there would exist 128 consecutive features across the different devices. Since the layers/modules have an intra-computation split, the devices must communicate features and outputs. There are two main strategies to do this however for our case we have chosen the scatter strategy.</p> <p>The scatter strategy needs to be done for every layer. Below is the scatter strategy for the dense layer.</p> <p>Suppose we are performing a matrix multiplication between $A \in \mathbb{R}^{m \times n}$ and $X \in \mathbb{R}^{n \times d}$. Using this strategy, the columns of $A$ and rows of $X$ are split across the $n$ devices, thus each device has vectors $a \in \mathbb{R}^{m \times 1}$ and $x \in \mathbb{R}^{1 \times d}$. Each device $k$, can compute $Y^k \in \mathbb{R}^{m \times d} = ax$, which contains a portion of the sum of $Y$, as $Y_{ij} = \sum_{k=1}^n Y^k_{ij}$. Hence, we need to sum the partial matrices on each device to get the final vector $y$, which can be split along the columns across the devices using the <code class="language-plaintext highlighter-rouge">psum scatter</code> strategy.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharded/9-480.webp 480w,/assets/img/sharded/9-800.webp 800w,/assets/img/sharded/9-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sharded/9.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Tensor Parallelism on a TP Dense Layer</figcaption> </figure> <h3 id="rmsnorm">RMSNorm</h3> <p>For RMSNorm since the hidden dimension is split across devices each device first computes its local sum of squares. To get the global sum we use <code class="language-plaintext highlighter-rouge">jax.lax.psum(..., axis_name)</code> which performs an all-reduce so that every device receives the total $\sum_{i=1}^{n} x_i^2$. Finally, we compute the global hidden size by all-reducing each local <code class="language-plaintext highlighter-rouge">x.shape[-1]</code> then divide the RMS by this global dimension.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharded/10-480.webp 480w,/assets/img/sharded/10-800.webp 800w,/assets/img/sharded/10-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sharded/10.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Tensor Parallelism on RMS Norm</figcaption> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RMSNorm</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span>

    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
    <span class="bp">...</span>
        <span class="n">rms</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1">#local sum computation on each device
</span>        <span class="n">rms</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">psum</span><span class="p">(</span><span class="n">rms</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># sum across devices
</span>        <span class="n">rms</span> <span class="o">=</span> <span class="n">rms</span> <span class="o">/</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">psum</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">)</span>
    <span class="bp">...</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h3 id="embedding">Embedding</h3> <p>In this case, the embedding layer is also split across the devices. At the start of the forward pass, the input values are loaded as the shape $(B, \frac{T}{\text{tp size}})$ as the sequence length dimension is sharded along the TP axis. Note the idea of <code class="language-plaintext highlighter-rouge">T</code> being sharded is called sequence parallelism but for memory-bandwidth we begin by sharding the <code class="language-plaintext highlighter-rouge">T</code> dim across the tensor axis. After the embedding is applied on the inputs, their shape becomes $(B, \frac{T}{\text{tp size}}, C)$. Then, since the tensor should be split along the hidden dimension axis, the function <code class="language-plaintext highlighter-rouge">jax.lax.all_to_all(x, axis_name, split axis, concat_axis, tiled)</code> is applied on the inputs after the embedding layer <code class="language-plaintext highlighter-rouge">x</code>. The axis_name is along the tensor parallelism axis (<code class="language-plaintext highlighter-rouge">tp</code>), the split_axis denotes along which axis the TP sharding should occur - in this case it is the hidden dim. Since <code class="language-plaintext highlighter-rouge">all_to_all</code> syntax doesn’t allow for negative numbers <code class="language-plaintext highlighter-rouge">split_axis=x.ndim-1</code>, which is equivalent to the -1 dim. The <code class="language-plaintext highlighter-rouge">concat_axis=x.ndim-2</code>, or the -2 dimension which indicates that all T across the devices should be concatenated as denoted by <code class="language-plaintext highlighter-rouge">tiled=True</code>. Hence the final shape now becomes $(B, T, \frac{C}{\text{tp size}})$ as intended. Similarly after the <code class="language-plaintext highlighter-rouge">self.norm</code> is applied we do the inverse all-to-all to obtain $(B, \frac{T}{\text{tp size}}, C)$ and then use the normal weight tying to obtain $(B, \frac{T}{\text{tp size}}, V)$ . Then in the loss function, we can <code class="language-plaintext highlighter-rouge">pmean</code> across the <code class="language-plaintext highlighter-rouge">tp</code> axis since tokens on one <code class="language-plaintext highlighter-rouge">tp</code> axis device’s are compute with weights on another <code class="language-plaintext highlighter-rouge">tp</code> axis device (this will be implemented later in the main training script).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="bp">...</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">out</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">out</span><span class="p">:</span>
            <span class="o">*</span><span class="n">_</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">all_to_all</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">,</span> <span class="n">split_axis</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">concat_axis</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">tiled</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">)</span>
      <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="nf">is_mutable_collection</span><span class="p">(</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">):</span>
                <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">all_to_all</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">,</span> <span class="n">split_axis</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">concat_axis</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tiled</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">.</span><span class="nf">attend</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h3 id="rope">RoPE</h3> <p>The next module that needs to change is the RoPE logic since the cos/sin matrices need to spilt for the channels that are on the device. Thus we only need to make changes in the setup method to slice the matrices. To do this, we find the current index in the <code class="language-plaintext highlighter-rouge">tp</code> axis and the size to find how many channels will be on each device, we call this the <code class="language-plaintext highlighter-rouge">slice_factor</code>. Then we use <code class="language-plaintext highlighter-rouge">jax.lax.dynamic_slice_in_dim</code> which is essentially <code class="language-plaintext highlighter-rouge">arr[..., start_idx: start_idx + length</code> but works under a <code class="language-plaintext highlighter-rouge">jit</code> context with dynamic values (values not known at compile time). We find the <code class="language-plaintext highlighter-rouge">start_idx</code> by multiplying the <code class="language-plaintext highlighter-rouge">idx * slice_factor</code> since that adds up the slices for the previous devices. This is done on the <code class="language-plaintext highlighter-rouge">axis=-1</code> since that is the channel axis.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharded/11-480.webp 480w,/assets/img/sharded/11-800.webp 800w,/assets/img/sharded/11-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/sharded/11.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">RoPE under Tensor Parallelism</figcaption> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RoPE</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="bp">...</span>
  <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">axis_index</span><span class="p">(</span><span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">tensor_size</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">psum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">slice_factor</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">model_dim</span> <span class="o">//</span> <span class="n">tensor_size</span>

    <span class="n">self</span><span class="p">.</span><span class="n">cos</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">dynamic_slice_in_dim</span><span class="p">(</span>
            <span class="n">cos</span><span class="p">,</span> <span class="n">slice_factor</span> <span class="o">*</span> <span class="n">idx</span><span class="p">,</span> <span class="n">slice_factor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sin</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">dynamic_slice_in_dim</span><span class="p">(</span>
            <span class="n">sin</span><span class="p">,</span> <span class="n">slice_factor</span> <span class="o">*</span> <span class="n">idx</span><span class="p">,</span> <span class="n">slice_factor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
</code></pre></div></div> <h3 id="attention">Attention</h3> <p>When applying tensor parallelism to <code class="language-plaintext highlighter-rouge">MLA</code>, we have to consider how sharding will work when performing scaled-dot product attention. The easiest approach is to shard the heads along the tensor axis since they are independent of each other when performing the attention operation. After splitting the local <code class="language-plaintext highlighter-rouge">q,k,v</code> across heads, the current interpretation is that for all heads we have a fraction of the keys, queries and values (split across <code class="language-plaintext highlighter-rouge">tp</code>). Thus we can perform an all-to-all to accumulate all the <code class="language-plaintext highlighter-rouge">qkv</code> across all heads and then split the heads across the <code class="language-plaintext highlighter-rouge">tp</code> axis.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MLA</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="bp">...</span>

    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">KV_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">KR_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]]]:</span>
      <span class="bp">...</span>

        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">B T (nh d) -&gt; B nh T d</span><span class="sh">"</span><span class="p">,</span> <span class="n">nh</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">),</span>
          <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">all_to_all</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">,</span> <span class="n">split_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">concat_axis</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">tiled</span><span class="o">=</span><span class="bp">True</span>
            <span class="p">),</span>
            <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="bp">...</span>
</code></pre></div></div> <p>We can then perform attention as normally applied. Then we want the output to be sharded across the channels of output so we first regather all heads and spilt back the output along the channels. Then we are able to reshape to concat the heads with the dimension as normal.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MLA</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="bp">...</span>

    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">KV_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">KR_cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]]]:</span>

    <span class="bp">...</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nf">scaledDotProd</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">all_to_all</span><span class="p">(</span>
            <span class="n">output</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">,</span> <span class="n">split_axis</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">concat_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tiled</span><span class="o">=</span><span class="bp">True</span>
        <span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="sh">"</span><span class="s">B nh T dk -&gt; B T (nh dk)</span><span class="sh">"</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">model_dtype</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">)(</span><span class="n">output</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">KV_cache</span><span class="p">,</span> <span class="n">KR_cache</span><span class="p">)</span>
</code></pre></div></div> <p>We now define the rules for the partition spec since certain features need to sharded along another axis as well. We shard the <code class="language-plaintext highlighter-rouge">RMSNorm</code> params in both the embedding and layer blocks. We shard the first axis of the the kernels (first axis ignoring pipeline since that will get split) in all <code class="language-plaintext highlighter-rouge">Dense</code> blocks in the layer as well, otherwise for biases the sharding is only for the pipeline dim.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">shardedModel</span><span class="p">:</span>
  <span class="bp">...</span>
  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">get_p_spec</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">Block</span><span class="p">],</span> <span class="n">mesh</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">Mesh</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">modelConfig</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">NamedSharding</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">NamedSharding</span><span class="p">]:</span>
    <span class="bp">...</span>
     <span class="k">def</span> <span class="nf">layer_partition</span><span class="p">(</span><span class="n">key</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="p">...],</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">P</span><span class="p">:</span>
            <span class="n">path</span> <span class="o">=</span> <span class="nf">join_fn</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
            <span class="k">if</span> <span class="sh">"</span><span class="s">gamma</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">path</span> <span class="ow">or</span> <span class="sh">"</span><span class="s">beta</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">path</span><span class="p">:</span>
                <span class="k">return</span> <span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="k">return</span> <span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">dp</span><span class="sh">"</span><span class="p">)</span>

            <span class="k">return</span> <span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">embedding_partition</span><span class="p">(</span><span class="n">key</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="p">...],</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">P</span><span class="p">:</span>
            <span class="n">path</span> <span class="o">=</span> <span class="nf">join_fn</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
            <span class="k">if</span> <span class="sh">"</span><span class="s">gamma</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">path</span> <span class="ow">or</span> <span class="sh">"</span><span class="s">beta</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">path</span><span class="p">:</span>
                <span class="k">return</span> <span class="nc">P</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">)</span>
            <span class="k">return</span> <span class="nc">P</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="bp">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span><span class="p">)))</span>
</code></pre></div></div> <p>Combining these all together we have a strategy for 3-D parallelism. Note that each of these strategies can be further improved and may be explored in the future. For example, better pipelining algorithms exists such as <a href="https://arxiv.org/abs/1806.03377" rel="external nofollow noopener" target="_blank">1F1B</a> or <a href="https://arxiv.org/pdf/2412.19437" rel="external nofollow noopener" target="_blank">DualPipe</a> which seek to reduce the bubble time while maintaining better FLOPs. For Tensor Parallelism, we can explore gather strategies that allow for async communication operations. However, the process of integrating these advanced strategies into n-D from scratch in JAX is very similar to how we have done it here. We will now look at the configs and main training loop used to run the model.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/main.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'jindal013/jaxformer-website-v2',
        'data-repo-id': 'R_kgDOPoEVEA',
        'data-category': 'General',
        'data-category-id': 'DIC_kwDOPoEVEM4Cu2n4',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-loading': '1',
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>