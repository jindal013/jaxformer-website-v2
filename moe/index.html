<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Implementing Mixture of Experts Layers | Jaxformer: Scaling Modern Transformers </title> <meta name="author" content=" "> <meta name="description" content="Mixture of Experts (MoE) layers scale LLMs by routing tokens to a small subset of feedforward experts, reducing memory use while enabling larger models. Here we show an implementation as well as address the main training challenges such as stability, expert collapse, and accelerator efficiency."> <meta name="keywords" content="jscaling, jax, llms, transformers, tpus, google, cloud, parallelism, distributed"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/jaxformer-icon.png?7001ddef15419e25335b33b49c6ce725"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jaxformer.com/moe/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> <script async src="https://www.googletagmanager.com/gtag/js?id=G-G878LK8JDJ"></script> <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-G878LK8JDJ');
</script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Implementing Mixture of Experts Layers",
            "description": "Mixture of Experts (MoE) layers scale LLMs by routing tokens to a small subset of feedforward experts, reducing memory use while enabling larger models. Here we show an implementation as well as address the main training challenges such as stability, expert collapse, and accelerator efficiency.",
            "published": "September 05, 2025",
            "authors": [
              
              {
                "author": "Aditya Makkar",
                "authorURL": "https://x.com/AdityaMakkar000"
              },
              
              {
                "author": "Divya Makkar",
                "authorURL": "https://x.com/_DivyaMakkar"
              },
              
              {
                "author": "Chinmay Jindal",
                "authorURL": "https://x.com/chinmayjindal_"
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <script>
    function goToTop() {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    }

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
      // Get the button:
      let mybutton = document.getElementById("top-button");

      if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
  }
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="d-none d-sm-inline">Jaxformer: Scaling Modern Transformers</span> <span class="d-inline d-sm-none"> Jaxformer </span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="left-button section-button"><a href="../distributed_training"><svg viewbox="-78.5 0 512 512"><path d="M257 64L291 98 128 262 291 426 257 460 61 262 257 64Z"></path></svg></a></div> <div class="right-button section-button"><a href="../training"><svg viewbox="-78.5 0 512 512"><path d="M98 460L64 426 227 262 64 98 98 64 294 262 98 460Z"></path></svg></a></div> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item nav-hidden"><a class="nav-link" onclick="goToTop()" id="top-button" style="display: none;">Back to Top</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../distributed_training">Previous Part</a></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../training">Next Part</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Sections </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/">Part 0. Introduction</a> <a class="dropdown-item " href="/tokenization/">Part 1. Tokenization</a> <a class="dropdown-item " href="/base_model/">Part 2. Base Model</a> <a class="dropdown-item " href="/sharded/">Part 3. Sharded Model</a> <a class="dropdown-item " href="/dataset/">Part 4. Dataset &amp; Config</a> <a class="dropdown-item " href="/distributed_training/">Part 5. Distributed Training</a> <a class="dropdown-item " href="/moe/">Part 6. Mixture of Experts</a> <a class="dropdown-item " href="/training/">Part 7. Training Results</a> <a class="dropdown-item " href="/conclusion/">Part 8. Conclusion</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Implementing Mixture of Experts Layers</h1> <p>Part 6 of <a href="">Jaxformer</a> (<a href="../distributed_training">Part 5: Distributed Training</a> | <a href="../training">Part 7: Training Results</a>)</p> <p>Mixture of Experts (MoE) layers scale LLMs by routing tokens to a small subset of feedforward experts, reducing memory use while enabling larger models. Here we show an implementation as well as address the main training challenges such as stability, expert collapse, and accelerator efficiency.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#motivation-and-challenges">Motivation and Challenges</a> </div> <div> <a href="#router-design">Router Design</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#scoring-and-all-gather">Scoring and All-Gather</a> </li> <li> <a href="#top-k-selection">Top-k Selection</a> </li> </ul> <div> <a href="#moe-implementation">MoE Implementation</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#shared-experts-and-routing">Shared Experts and Routing</a> </li> <li> <a href="#scatter-and-expert-inputs">Scatter and Expert Inputs</a> </li> <li> <a href="#expert-execution-and-aggregation">Expert Execution and Aggregation</a> </li> <li> <a href="#auxiliary-loss">Auxiliary Loss</a> </li> </ul> <div> <a href="#integration-into-the-transformer">Integration into the Transformer</a> </div> <div> <a href="#training-integration">Training Integration</a> </div> </nav> </d-contents> <h2 id="motivation-and-challenges">Motivation and Challenges</h2> <p>A recent advancement in scaling LLMs to larger networks has been through the introduction of Mixture of Experts (MoE) modules in the decoder. Training MoE models are primarily difficult for 2 reasons. The first, is due to training stability. This involves ensuring each expert gets roughly the same number of tokens otherwise, it can lead to expert collapse. Luckily for us, the hard work is done and lots of open-source models provide strong training recipes that we can use. The second is training efficiency, as you want to ensure that you are utilizing your accelerators to their max, and they aren’t idle while training. We will be writing MoE similar to the Deepseek V3 paper. Additionally, distributed techniques such as expert parallelism are not being included, but we provide a general overview on how to incorporate it.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/moe/1-480.webp 480w,/assets/img/moe/1-800.webp 800w,/assets/img/moe/1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/moe/1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">MoE layer from DeepSeek V3</figcaption> </figure> <h2 id="router-design">Router Design</h2> <p>We first begin by writing the router and layer module whilst integrating it into the current program by writing the distributed setup and training configs.</p> <h3 id="scoring-and-all-gather">Scoring and All-Gather</h3> <p>The router decides how to send each token by computing a score. In the DeepSeek-V3 paper, they describe the score of the $i$ expert for the $t$ token as \(s_{i,t} = \sigma \left(u_t^Te_i\right)\) where $\sigma$ is the Sigmoid activation function. This can essentially be written as a dense layer with no bias since our tokens are transposed by definition due to being row-wise vectors. For simplicity, we also include the bias in our score function even though it has been shown at larger scales $\sim &gt;100B$ params that they tend to cause training instabilities. The vectors for each expert $e$ are known as the centroids, so the<code class="language-plaintext highlighter-rouge">Dense</code> network can be described as one as well.</p> <p>The term centroids comes from the idea that we take the dot product with the center of mass of each expert and then select the top-$k$ tokens by similarity.</p> <p>We begin the router by defining the parameters and centroids network.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">NoisyKGate</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">n_experts</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">centroids</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_experts</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">)</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">__call__</code> method, accumulates the scores and all-gathers them for two reasons. The first is that when we apply a top-k selection to chose the top experts, we need all the results along the channel-dim. The second reason and also why we do not perform a traditional all-to-all, is that when determining the routing, we want to compute it across all time steps. It is therefore easier to have the scores replicated along the tensor axis.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">NoisyKGate</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="bp">...</span>
  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="n">Array</span><span class="p">,</span> <span class="n">Array</span><span class="p">]:</span>
        <span class="n">local_scores</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">centroids</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">all_gather</span><span class="p">(</span>
            <span class="n">local_scores</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">axis</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">tiled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span> <span class="c1"># ( B, T, C) fully collected
</span></code></pre></div></div> <h3 id="top-k-selection">Top-k Selection</h3> <p>Now, we can write the function to select the <code class="language-plaintext highlighter-rouge">top_k</code> experts. We select the top k of the given array <code class="language-plaintext highlighter-rouge">x</code>, receiving indices and scores of those values, defining them as <code class="language-plaintext highlighter-rouge">g</code> values. For the indices chosen, we normalize the scores and return back normalized scores alongside the indices.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">NoisyKGate</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="bp">...</span>
  <span class="k">def</span> <span class="nf">top</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="n">Array</span><span class="p">]:</span>
        <span class="n">g_i</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">top_k</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">)</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">g_i</span> <span class="o">/</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">g_i</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">g</span><span class="p">,</span> <span class="n">i</span>
</code></pre></div></div> <p>We can then apply this function using <code class="language-plaintext highlighter-rouge">jax.lax.apply_along_axis</code> to vmap over the first 2 axes and obtain the <code class="language-plaintext highlighter-rouge">g_scores, indices</code> arrays for the batch (size will be <code class="language-plaintext highlighter-rouge">(B, T, self.k)</code> for both).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">NoisyKGate</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="bp">...</span>
  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="n">Array</span><span class="p">,</span> <span class="n">Array</span><span class="p">]:</span>
    <span class="n">g_scores</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">apply_along_axis</span><span class="p">(</span><span class="n">func1d</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">top</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">arr</span><span class="o">=</span><span class="n">scores</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">g_scores</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">scores</span>
</code></pre></div></div> <h2 id="moe-implementation">MoE Implementation</h2> <p>Now, we proceed to the main MoE implementation. A standard implementation of MoE isn’t too difficult; however in JAX, there are caveats such as trying to avoid using for loops making an MoE implementation more difficult. We provide a simpler implementation based on <a href="https://github.com/google/flaxformer/tree/main/flaxformer/architectures/moe" rel="external nofollow noopener" target="_blank">Google large-scale implementation</a> , combined with the idea from DeepSeek-V3.</p> <h3 id="shared-experts-and-routing">Shared Experts and Routing</h3> <p>We start by defining our class with standard parameters.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MoE</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">model_dimension</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">n_shared</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">n_experts</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">capacity_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span>

    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
      <span class="bp">...</span>
</code></pre></div></div> <p>We first pass <code class="language-plaintext highlighter-rouge">x</code> through the shared experts described in the DeepSeek-V3 script, which are essentially <code class="language-plaintext highlighter-rouge">n</code> feed-forward dimensions. Thus, we can create a <code class="language-plaintext highlighter-rouge">Dense</code> layer of size <code class="language-plaintext highlighter-rouge">n_shared × model_dimension</code>, then split and sum it across the <code class="language-plaintext highlighter-rouge">n_shared</code> dimension.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MoE</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="bp">...</span>
    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>

        <span class="n">shared</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span>
            <span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">n_shared</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">res_shared</span> <span class="o">=</span> <span class="nf">shared</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">res_shared</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">res_shared</span><span class="p">,</span> <span class="sh">"</span><span class="s">B T (n d) -&gt; B T n d</span><span class="sh">"</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_shared</span><span class="p">)</span>
        <span class="n">res_shared</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">res_shared</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># (B, T, n, d) -&gt; (B, T, d)
</span>       
        <span class="bp">...</span>
</code></pre></div></div> <p>Then, we setup the router and get the scores and auxiliary values from the router’s forward pass.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MoE</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="bp">...</span>
  <span class="nd">@nn.compact</span>
  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="bp">...</span>
        <span class="n">router</span> <span class="o">=</span> <span class="nc">NoisyKGate</span><span class="p">(</span>
            <span class="n">n_experts</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_experts</span><span class="p">,</span>
            <span class="n">k</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">,</span>
            <span class="n">model_dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">g_scores</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="nf">router</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, T, k), (B, T, k), (B, T, n_experts)
</span></code></pre></div></div> <p>Note that <code class="language-plaintext highlighter-rouge">x</code> is still split across the tensor dim, but the scores are not. We now compute the capacity per expert. Ideally when training, we want the tokens to be evenly spilt between all experts however that is quite rare and hence we allow some extra space through the capacity factor of the tokens.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
  <span class="bp">...</span>
  <span class="n">capacity</span> <span class="o">=</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span>
  <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
    <span class="n">capacity</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">capacity</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">capacity_factor</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">n_experts</span><span class="p">)</span>
</code></pre></div></div> <h3 id="scatter-and-expert-inputs">Scatter and Expert Inputs</h3> <p>Now, we need to build the expert inputs which will be in the shape of<code class="language-plaintext highlighter-rouge">(n_experts, capacity, C)</code> describing the input to each expert. mFor this we write the scatter function. This is inspired by the routing logic in the Google MoE layers linked above and provides a pure-JAX way to determine and create the expert routing. We begin by reshaping the inputs into 2D Tensor: one for channels and the rest is treated like a batch dim.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MoE</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">indices</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">capacity</span><span class="p">:</span> <span class="nb">int</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="n">Array</span><span class="p">]:</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">)</span>
</code></pre></div></div> <p>Since we are trying to essentially determine for each expert the first <code class="language-plaintext highlighter-rouge">N</code> tokens where <code class="language-plaintext highlighter-rouge">N</code> is the capacity, we first sort the tokens by the highest score of the batch. There are other techniques to determine the priority but this is the simplest one for now. Note we don’t sort for each position since we want every batch to remain in row with its top-k.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">scatter</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="c1"># sort to arrange in order of expert scores for each batch by
</span>  <span class="c1"># the highest scored expert
</span>  <span class="n">sorted_token_idx</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="o">-</span><span class="n">scores</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">take_along_axis</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">sorted_token_idx</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">sorted_scores</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">take_along_axis</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">sorted_token_idx</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <p>Now we swap the axes and flatten to essentially get our order in terms of priority across all batches, that is all tokens for the first position, then second position all the way until the $k$ position.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">scatter</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="c1"># swapping gives you the highest highest score across the batch
</span>  <span class="c1"># expert_1: [b_1, b_2, .. b_{B * T }], expert_2: [b_1, b_2, .. b_{B * T }], ...
</span>  <span class="c1"># flatten then to get expert indices in order
</span>  <span class="n">flat_indices</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">swapaxes</span><span class="p">(</span><span class="n">sorted_indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">flat_scores</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">swapaxes</span><span class="p">(</span><span class="n">sorted_scores</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>We now convert to one hot encodings to let us know for each position which expert it is from and multiply with the scores to get a score map</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">scatter</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="c1"># convert to one hot encoding
</span>  <span class="c1"># then multiply to get the score for each instead of 1
</span>  <span class="n">expert_onehot</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">flat_indices</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_experts</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span> <span class="c1"># (B*T*k, n_experts)
</span>  <span class="n">expert_scores</span> <span class="o">=</span> <span class="n">flat_scores</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">expert_onehot</span>  <span class="c1"># (B*T*k, n_experts)
</span></code></pre></div></div> <p>Now, we perform a cumulative sum. Instead of having a <code class="language-plaintext highlighter-rouge">1</code> for expert <code class="language-plaintext highlighter-rouge">i</code>, it now says which token number it is en-queue for that expert. We can also take the max across the experts to get how many tokens are going to each expert. This is a useful statistic to determine if we avoided expert-collapse.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">scatter</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="n">position_in_expert</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">expert_onehot</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">expert_onehot</span> <span class="c1"># get which position it is in the expert
</span>  <span class="c1"># find max position across all batches since that is the total sum from cumsum
</span>  <span class="n">tokens_per_expert</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">position_in_expert</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">)</span> <span class="c1"># take average across batch
</span></code></pre></div></div> <p>Now that we have the position in expert, we perform the inverse operations to get back our original <code class="language-plaintext highlighter-rouge">(B x T, k)</code> input with a new axis that represents the position in the expert.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">scatter</span><span class="p">(..):</span>
  <span class="bp">...</span>
  <span class="c1"># reshape it back to get for
</span>  <span class="c1"># expert_i: [b_1, b_2, .. b_{B * T }] where b_i is the one hot for which position it is in
</span>  <span class="c1"># same for expert scores
</span>  <span class="n">position_in_expert</span> <span class="o">=</span> <span class="n">position_in_expert</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">,</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_experts</span><span class="p">)</span>
  <span class="n">expert_scores</span> <span class="o">=</span> <span class="n">expert_scores</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">,</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_experts</span><span class="p">)</span>

  <span class="c1"># go back to orginal shape
</span>  <span class="n">position_in_expert</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">swapaxes</span><span class="p">(</span><span class="n">position_in_expert</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B*T, k, n_experts)
</span>  <span class="n">expert_scores</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">swapaxes</span><span class="p">(</span><span class="n">expert_scores</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># (B*T, k, n_experts)
</span></code></pre></div></div> <p>Since for each <code class="language-plaintext highlighter-rouge">k</code> only one field in <code class="language-plaintext highlighter-rouge">n_experts</code> is non-zero (as it was originally a one-hot encoding), we can take the max across <code class="language-plaintext highlighter-rouge">k</code> to determine, for every batch, which expert it is routed to and at what position. We subtract 1 to zero-index it. This is done by taking <code class="language-plaintext highlighter-rouge">jnp.max(position_in_expert, axis=1)</code>, where <code class="language-plaintext highlighter-rouge">axis=1</code> corresponds to the top-k routed experts. We can then apply <code class="language-plaintext highlighter-rouge">argsort</code> on <code class="language-plaintext highlighter-rouge">sorted_token_idx</code>, which performs an inverse permutation and restores the original arrangement of our batches.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">scatter</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="c1"># for every batch in each expert find the non-zero expert position
</span>  <span class="c1"># as for every expert we only have one non-zero value
</span>  <span class="n">final_pos</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">position_in_expert</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># make it 0 indexed
</span>  <span class="n">final_scores</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">expert_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># do the same for the score
</span>  
  <span class="c1"># unsort the indices
</span>  <span class="n">unsorted_indices</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">sorted_token_idx</span><span class="p">)</span>
  <span class="n">final_pos</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">take_along_axis</span><span class="p">(</span><span class="n">final_pos</span><span class="p">,</span> <span class="n">unsorted_indices</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">final_scores</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">take_along_axis</span><span class="p">(</span>
    <span class="n">final_scores</span><span class="p">,</span> <span class="n">unsorted_indices</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span>
  <span class="p">)</span>
</code></pre></div></div> <p>We can now create a dispatch mask which will one hot encode the position of the capacity each token is in. Subtracting 1 from the max is helpful here because the tokens are zero-indexed. This ensures that tokens which were originally 0, before subtracting 1, (eg. not routed to any expert) will not be one-hot encoded. Same goes for the tokens that are greater than or equal to the capacity (row will be all 0).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">scatter</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="c1"># final pos is now the orginal order where each index is the position in the expert
</span>  <span class="c1"># if it is greater than or less than the capcity / 0 (hence -1) the row will be 0 in the capcity
</span>  <span class="c1"># hence we have for each positoin and expert the one hot tells us which position it is in
</span>  <span class="c1"># if it is in
</span>  <span class="n">dispatch_mask</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span>
    <span class="n">final_pos</span><span class="p">,</span> <span class="n">capacity</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">int32</span>
  <span class="p">)</span>  <span class="c1"># (B*T, n_experts, capacity)
</span>  <span class="c1"># multiply out all the values in the capcity by final score
</span>  <span class="c1"># we can replicate since at most 1 value will be non zero
</span>  <span class="n">scores_mask</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">dispatch_mask</span> <span class="o">*</span> <span class="n">final_scores</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">]</span>
  <span class="p">)</span>  <span class="c1"># (B*T, n_experts, capacity)
</span></code></pre></div></div> <p>For every expert at position <code class="language-plaintext highlighter-rouge">c</code> in the capacity, we can sum the input vector for every batch since at most, only 1 value across the batch is at that position. For this we use einsum, summing over the <code class="language-plaintext highlighter-rouge">b</code> dim.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">scatter</span><span class="p">(...):</span>
  <span class="c1"># since only one expert at every position in capactiy at most
</span>  <span class="c1"># we can sum to get rid of batch dim and get the exepect capacity dimension indicies
</span>  <span class="n">expert_inputs</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bd,bec-&gt;ecd</span><span class="sh">"</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dispatch_mask</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">expert_inputs</span><span class="p">,</span> <span class="n">scores_mask</span><span class="p">,</span> <span class="n">tokens_per_expert</span>
</code></pre></div></div> <h3 id="expert-execution-and-aggregation">Expert Execution and Aggregation</h3> <p>The expert inputs now have shape <code class="language-plaintext highlighter-rouge">(experts, capacity, dimension)</code>, allowing us to proceed with the main call. Note that the scores correspond to <code class="language-plaintext highlighter-rouge">g_scores</code>, since these are what we want in the score mask (the coefficients for the weighted sum of the experts). We then create a single expert using a <code class="language-plaintext highlighter-rouge">FeedForward</code> module. Then we apply a <code class="language-plaintext highlighter-rouge">linen</code> lifted transformation, <code class="language-plaintext highlighter-rouge">nn.vmap</code>, which is equivalent to <code class="language-plaintext highlighter-rouge">vmap</code> but operates directly over a module (in this case, the expert). While we have generally avoided using lifted transformations from Flax, in this case it is simpler to rely on Flax rather than writing a separate module with parameters and manually applying <code class="language-plaintext highlighter-rouge">jax.vmap</code> over them.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@nn.compact</span>
<span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
  <span class="bp">...</span>
  <span class="n">expert_inputs</span><span class="p">,</span> <span class="n">score_mask</span><span class="p">,</span> <span class="n">tokens_per_expert</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">g_scores</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">capacity</span>
  <span class="p">)</span> <span class="c1"># (e, c, d) , (B * T, e, c), (e,)
</span>  
  <span class="n">expert</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span>
    <span class="n">model_dimension</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span>
    <span class="n">dropout_rate</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
    <span class="n">model_dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">,</span>
  <span class="p">)</span>
  
  <span class="n">expert_outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">expert</span><span class="p">,</span> <span class="n">inp</span><span class="p">:</span> <span class="nf">expert</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">),</span>
    <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">out_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">variable_axes</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span>
    <span class="n">split_rngs</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">params</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> <span class="sh">"</span><span class="s">dropout</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span>
  <span class="p">)(</span><span class="n">expert</span><span class="p">,</span> <span class="n">expert_inputs</span><span class="p">)</span> <span class="c1"># (n_experts, capacity, d)
</span></code></pre></div></div> <p>We sum the outputs with the score mask across all experts and capacity, since only one position in the capacity of each expert can be non-zero. Our goal is to aggregate the outputs for every batch position, weighted by the corresponding score. That is, at time step $i$, the final expert output $x_i$ is</p> \[x_i = \sum_{j}^{N} g_{i,j}e_{j}\] <p>where $e_j$ is output for the $j$ expert when the input was routed to expert $j$.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@nn.compact</span> 
<span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
  <span class="bp">...</span>
  <span class="n">expert_outputs</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">ecd,tec-&gt;td</span><span class="sh">"</span><span class="p">,</span> <span class="n">expert_outputs</span><span class="p">,</span> <span class="n">score_mask</span><span class="p">)</span>
  <span class="n">expert_outputs</span> <span class="o">=</span> <span class="n">expert_outputs</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</code></pre></div></div> <h3 id="auxiliary-loss">Auxiliary Loss</h3> <p>In order to prevent expert collapse, DeepSeek-V3 adds an auxiliary loss to the main CE loss in order to penalize routing tokens to the same expert. The auxiliary loss is defined as \(L_e = \alpha \sum_{i=1}^{N} f_i P_i\) where $\alpha$ is a hyperparameter and $N$ is the number of experts. Then, $f_i, P_i$ are defined as follows</p> <p>\(f_i = \frac{N}{kT} \sum_{t=1}^T \mathbb{1} (s_{i,t} \in \text{TopK}(\{ s_{j,t} \mid 1 \leq j \leq N \}, K))\) where $s_{i,t}$ is the unnormalized score, essentially counting how many times for that expert the score is in the top $k$ and $P_i$ is the sum of the normalized scores,</p> <p>$$ P_i = \frac{1}{T} \sum_{t=1}^T S_{i,t}</p> <p>$$ where $S_{i,t} = \frac{s_{i,t}}{\sum_{j=1}^{N} s_{j,t}}$. Thus we can compute $f, P$ as a function and return it as a metric. This promotes a uniform distribution over experts since it penalizes each expert for being used more times ($f_i$ becomes larger).</p> <p>We first begin by computing $P$. We normalize the scores to get $S$, and then reshape into 2D arrays since we can treat <code class="language-plaintext highlighter-rouge">B</code> as a time dim. Then we need to sum over this batch/time dim (axis=0) and normalize by <code class="language-plaintext highlighter-rouge">T = B x T_batch</code>. Thus, $P$ will then be of shape <code class="language-plaintext highlighter-rouge">(n_experts, )</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MoE</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="bp">...</span>
  <span class="k">def</span> <span class="nf">auxiliary_loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">indices</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">n_experts</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="n">shape</span>
  
    <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">/</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">,</span> <span class="n">n_experts</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">)</span>
    
    <span class="bp">...</span>
</code></pre></div></div> <p>We now calculate <code class="language-plaintext highlighter-rouge">f</code> by first flattening it into a one-dimensional array, since we want to count across the entire batch and time steps, the number of indices that were routed to each head. We do this by applying <code class="language-plaintext highlighter-rouge">jax.nn.one_hot</code>, giving a tensor of shape <code class="language-plaintext highlighter-rouge">(B × T × k, n_experts)</code>. Then, we sum over the first dim and normalize across the batch. We apply the other factor of $\frac{N}{k}$ when computing the loss in the <code class="language-plaintext highlighter-rouge">main.py</code> script. Note the <code class="language-plaintext highlighter-rouge">tokens_per_expert</code> we previously computed is the same statistic, but for clarity we will explicitly compute it for now.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MoE</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">auxiliary_loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="p">...):</span> 
    <span class="bp">...</span>
    <span class="n">total_batch</span> <span class="o">=</span> <span class="n">B</span> <span class="o">*</span> <span class="n">T</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">k</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">total_batch</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">n_experts</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">f</span><span class="p">,</span><span class="n">p</span>
</code></pre></div></div> <p>Once we have the auxiliary loss, we can now structure our final output as the output and statistics for the <code class="language-plaintext highlighter-rouge">MoE</code> layer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nd">@nn.compact</span>
<span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>

  <span class="n">f</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">auxiliary_loss</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
  <span class="n">aux</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">tokens_per_expert</span><span class="sh">"</span><span class="p">:</span> <span class="n">tokens_per_expert</span><span class="p">,</span> <span class="sh">"</span><span class="s">f</span><span class="sh">"</span><span class="p">:</span> <span class="n">f</span><span class="p">,</span> <span class="sh">"</span><span class="s">p</span><span class="sh">"</span><span class="p">:</span> <span class="n">p</span><span class="p">}</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">res_shared</span> <span class="o">+</span> <span class="n">expert_outputs</span>
  
  <span class="k">return</span> <span class="n">expert_outputs</span><span class="p">,</span> <span class="n">aux</span>
</code></pre></div></div> <h2 id="integration-into-the-transformer">Integration into the Transformer</h2> <p>Now, we modify our layer block, encoder block and sharded model to incorporate the moe <code class="language-plaintext highlighter-rouge">aux</code>. For ease in training, we only make it so that the last layer in the interleaved RoPE blocks use the MoE outputs. In this layer, we add the necessary params as well as a flag for <code class="language-plaintext highlighter-rouge">use_moe</code> defaulted to <code class="language-plaintext highlighter-rouge">False</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">model_dimension</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">T</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dhR</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">n_experts</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">n_shared</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">capacity_factor</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">use_moe</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">bfloat16</span>
</code></pre></div></div> <p>In the call, we use the flag to determine which block type is needed and return the auxiliary value along with the cache. Then, in the call, we can use the flag to determine which block type is needed and return the auxiliary value along with the cache.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="bp">...</span>
  <span class="nd">@nn.compact</span>
  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">cache_type</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="n">cache_type</span><span class="p">]:</span>
    <span class="bp">...</span>
    <span class="n">x_res</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">use_moe</span><span class="p">:</span>
      <span class="n">x</span><span class="p">,</span> <span class="n">aux</span> <span class="o">=</span> <span class="nc">MoE</span><span class="p">(</span>
        <span class="n">model_dimension</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span>
        <span class="n">n_experts</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_experts</span><span class="p">,</span>
        <span class="n">k</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">,</span>
        <span class="n">n_shared</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_shared</span><span class="p">,</span>
        <span class="n">capacity_factor</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">capacity_factor</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
        <span class="n">model_dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">,</span>
      <span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">x</span><span class="p">,</span> <span class="n">aux</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span>
        <span class="n">model_dimension</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
        <span class="n">model_dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">,</span>
      <span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">),</span> <span class="bp">None</span>
      
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x_res</span>

        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">cache</span><span class="p">,</span> <span class="n">aux</span><span class="p">)</span>
</code></pre></div></div> <p>In the block, we can take the MoE parameters and pass them into every layer, where the last sets the parameter<code class="language-plaintext highlighter-rouge">use_moe=True</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">model_dimension</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">T</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dhR</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">n_experts</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">n_shared</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">capacity_factor</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">bfloat16</span>

    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">cache</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">cache_type</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="n">cache_type</span><span class="p">]:</span>
      <span class="bp">...</span>
        <span class="n">moe_stat</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">):</span>
          <span class="c1"># build cache 
</span>          <span class="bp">...</span>

            <span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">cache_out</span><span class="p">,</span> <span class="n">aux</span><span class="p">)</span> <span class="o">=</span> <span class="nc">Layer</span><span class="p">(</span>
                <span class="n">model_dimension</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dimension</span><span class="p">,</span>
                <span class="n">n_heads</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span>
                <span class="n">T</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">T</span><span class="p">,</span>
                <span class="n">latent_dim</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">,</span>
                <span class="n">dhR</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dhR</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
                <span class="n">n_experts</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_experts</span><span class="p">,</span>
                <span class="n">k</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">,</span>
                <span class="n">n_shared</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">n_shared</span><span class="p">,</span>
                <span class="n">capacity_factor</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">capacity_factor</span><span class="p">,</span>
                <span class="n">use_moe</span><span class="o">=</span><span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="c1"># moe on last layer
</span>                <span class="n">dropout_rate</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
                <span class="n">model_dtype</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model_dtype</span><span class="p">,</span>
            <span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">current_cache</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">aux</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">moe_stat</span> <span class="o">=</span> <span class="n">aux</span>
            <span class="bp">...</span>
        <span class="bp">...</span>

        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">out_cache</span><span class="p">,</span> <span class="n">moe_stat</span><span class="p">)</span>
</code></pre></div></div> <p>We now have to store and return <code class="language-plaintext highlighter-rouge">moe_stat</code> inside the pipeline stage of the <code class="language-plaintext highlighter-rouge">shardedModel</code>. Note we do not implement <code class="language-plaintext highlighter-rouge">MoE</code> into the base transformer although it is simpler then the implementation we show in the <code class="language-plaintext highlighter-rouge">shardedModel</code>.</p> <p>We can also keep an array inside the pipeline function to store the <code class="language-plaintext highlighter-rouge">moe_stats</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="n">moe_stat</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">microbatches</span> <span class="o">+</span> <span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
    <span class="bp">...</span>
    
    <span class="n">state</span><span class="p">,</span> <span class="p">(</span><span class="n">out_cache</span><span class="p">,</span> <span class="n">out_moe_stat</span><span class="p">)</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">fn</span><span class="p">)(</span>
      <span class="n">state_idx</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">stage_params</span><span class="p">,</span> <span class="n">current_cache</span><span class="p">,</span> <span class="n">layer_keys</span>
    <span class="p">)</span>
    
    <span class="bp">...</span>
    <span class="n">moe_stat</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">out_moe_stat</span><span class="p">)</span>
</code></pre></div></div> <p>Now we can stack all the layers to obtain a shape of <code class="language-plaintext highlighter-rouge">(M + L - 1, layers_per_device, n_experts)</code> for each metric. Since every MoE layer returns <code class="language-plaintext highlighter-rouge">n_experts</code>, and <code class="language-plaintext highlighter-rouge">jax.vmap</code> stacks them across <code class="language-plaintext highlighter-rouge">layers_per_device</code>, this results in <code class="language-plaintext highlighter-rouge">M + L - 1</code> calls yielding this shape.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="n">moe_stat</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="o">*</span><span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
    <span class="o">*</span><span class="n">moe_stat</span>
  <span class="p">)</span>
</code></pre></div></div> <p>Unlike the cache, we cannot simply return this because the cache is used as a placeholder. In cases where the computation is not in the main pipeline, the results don’t matter which implies that we never have to consider slicing it. However, for <code class="language-plaintext highlighter-rouge">moe_stat</code>, we don’t actually want the full <code class="language-plaintext highlighter-rouge">M + L - 1</code> statistics since this will be used in the loss and we only care about the <code class="language-plaintext highlighter-rouge">M</code> microbatches that are passed in. Moreover, these are not simply the first <code class="language-plaintext highlighter-rouge">M</code> calls, they are offset for each layer. For the first layer, we use the first <code class="language-plaintext highlighter-rouge">M</code> arrays, for the second layer, we use the <code class="language-plaintext highlighter-rouge">M</code> arrays from indices <code class="language-plaintext highlighter-rouge">1</code> to <code class="language-plaintext highlighter-rouge">M + 1</code>, and so on similar to pipeline parallelism where each layer is offset according to its position. To do this, we can make a function called <code class="language-plaintext highlighter-rouge">slice_moe</code> which is applied with a map over the <code class="language-plaintext highlighter-rouge">moe_stat</code> dict. Then, for each array, we can use a function <code class="language-plaintext highlighter-rouge">each_layer</code> which maps over the <code class="language-plaintext highlighter-rouge">layer_per_device</code> axis (axis=1) and applies a dynamic slice based on the layer index. For each layer we can use the <code class="language-plaintext highlighter-rouge">layer_idx</code> to apply <code class="language-plaintext highlighter-rouge">jax.lax.dynamic_slice_in_dim</code> which as mentioned in the <code class="language-plaintext highlighter-rouge">RoPE</code> module, is essentially equivalent to <code class="language-plaintext highlighter-rouge">arr[..., start_idx: start_idx + length]</code> on some axis. Our start index is the sum of all the past layers and the layer index of the current layer. The length is microbatches long since each layer processes <code class="language-plaintext highlighter-rouge">M</code> microbatches continuously. We can then mean over the microbatches since they are equivalent to time dimensions like before (only we have spilt it up instead of having one large batch).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(...):</span>
  <span class="bp">...</span>
  
  <span class="k">def</span> <span class="nf">slice_moe</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">each_layer</span><span class="p">(</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">dynamic_slice_in_dim</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">layers_per_device</span> <span class="o">*</span> <span class="n">device_idx</span> <span class="o">+</span> <span class="n">layer_idx</span><span class="p">,</span>
        <span class="n">microbatches</span><span class="p">,</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">0</span>
      <span class="p">)</span>
    <span class="n">sliced_x</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">each_layer</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">out_axes</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">))(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">layers_per_device</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sliced_x</span>

  <span class="n">moe_stat</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">slice_moe</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># mean across microbatches
</span>    <span class="n">moe_stat</span>
  <span class="p">)</span>
  <span class="bp">...</span>
</code></pre></div></div> <p>We can then multiply out the <code class="language-plaintext highlighter-rouge">f</code> and <code class="language-plaintext highlighter-rouge">p</code> stats since we only need them for the loss. Additionally we can sum the tokens per expert across the different layers.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(...):</span>

  <span class="n">moe_stat</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">tokens_per_expert</span><span class="sh">"</span><span class="p">:</span> <span class="n">moe_stat</span><span class="p">[</span><span class="sh">"</span><span class="s">tokens_per_expert</span><span class="sh">"</span><span class="p">].</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># (experts,)
</span>    <span class="sh">"</span><span class="s">aux_loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">moe_stat</span><span class="p">[</span><span class="sh">'</span><span class="s">f</span><span class="sh">'</span><span class="p">]</span> <span class="o">*</span> <span class="n">moe_stat</span><span class="p">[</span><span class="sh">'</span><span class="s">p</span><span class="sh">'</span><span class="p">],</span> <span class="c1"># (layers_per_device, experts)
</span>  <span class="p">}</span>

  <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="p">(</span><span class="n">out_cache</span><span class="p">,</span> <span class="n">moe_stat</span><span class="p">)</span>
</code></pre></div></div> <p>The last place in <code class="language-plaintext highlighter-rouge">model.py</code> where we need to change from MoE is in the partition spec. We also need to shard the kernel in the MoE expert layer since the <code class="language-plaintext highlighter-rouge">linen.vmap</code> will add an extra dim which our rules don’t account for. We only have to update the <code class="language-plaintext highlighter-rouge">layer_partition</code> since that contains the MoE layer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pipeline</span><span class="p">(...):</span>
  <span class="bp">...</span>
  <span class="n">join_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">path</span><span class="p">:</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="n">key</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">path</span><span class="p">).</span><span class="nf">lower</span><span class="p">()</span>
  <span class="k">def</span> <span class="nf">layer_partition</span><span class="p">(</span><span class="n">key</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="p">...],</span> <span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">P</span><span class="p">:</span>
    <span class="n">path</span> <span class="o">=</span> <span class="nf">join_fn</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="k">if</span> <span class="sh">"</span><span class="s">moe</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">path</span> <span class="ow">and</span> <span class="sh">"</span><span class="s">feedforward</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">path</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">return</span> <span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">dp</span><span class="sh">"</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">return</span> <span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="sh">"</span><span class="s">gamma</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">path</span> <span class="ow">or</span> <span class="sh">"</span><span class="s">beta</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">path</span><span class="p">:</span>
      <span class="k">return</span> <span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
      <span class="k">return</span> <span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">dp</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
    
  <span class="bp">...</span>
  <span class="n">layer_p_spec</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map_with_path</span><span class="p">(</span>
    <span class="n">layer_partition</span><span class="p">,</span>
    <span class="n">eval_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
  <span class="p">)</span>
  
  <span class="k">return</span> <span class="n">embed_p_spec</span><span class="p">,</span> <span class="n">layer_p_spec</span>
</code></pre></div></div> <p>We can also add a helper function to count the total and active parameters. We count the total parameters similar to the main script function. Then we use the <code class="language-plaintext highlighter-rouge">join_fn</code> to check whether the layer is an expert layer, and if so, we count only k experts instead of all <code class="language-plaintext highlighter-rouge">n_experts</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">param_count</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>

  <span class="n">total_params</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">reduce</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">,</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="mi">0</span><span class="p">,</span>
  <span class="p">)</span>

  <span class="n">join_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">path</span><span class="p">:</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="n">key</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">path</span><span class="p">).</span><span class="nf">lower</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">count_active_params</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">path</span> <span class="o">=</span> <span class="nf">join_fn</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">n_elements</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span>

    <span class="n">is_expert</span> <span class="o">=</span> <span class="sh">"</span><span class="s">moe</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">path</span> <span class="ow">and</span> <span class="sh">"</span><span class="s">feedforward</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">path</span>
    <span class="k">if</span> <span class="n">is_expert</span><span class="p">:</span>
      <span class="n">n_elements</span> <span class="o">=</span> <span class="n">n_elements</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">n_experts</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">k</span>

    <span class="k">return</span> <span class="n">n_elements</span>

  <span class="n">active_params_map</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map_with_path</span><span class="p">(</span><span class="n">count_active_params</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">active_params</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">reduce</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">active_params_map</span><span class="p">,</span> <span class="mi">0</span>
  <span class="p">)</span>

  <span class="k">return</span> <span class="n">total_params</span><span class="p">,</span> <span class="n">active_params</span>
</code></pre></div></div> <h2 id="training-integration">Training Integration</h2> <p>We can then also update the modelConfig in <code class="language-plaintext highlighter-rouge">utils.py</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">modelConfig</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">model config class</span><span class="sh">"""</span>

    <span class="n">model_dimension</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">blocks</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">layers_per_block</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">T</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dhR</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">model_dtype</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">bfloat16</span><span class="sh">"</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">n_experts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">n_shared</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">capacity_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
</code></pre></div></div> <p>Now we make the changes to our main training script. The first change is using the correct parameter count.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">cfg</span><span class="p">):</span>
  <span class="bp">...</span>
  <span class="n">param_count</span><span class="p">,</span> <span class="n">active_param_count</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">param_count</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
  <span class="nf">log</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total parameters: </span><span class="si">{</span><span class="n">param_count</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> with </span><span class="si">{</span><span class="n">active_param_count</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> active</span><span class="sh">"</span><span class="p">)</span>
  <span class="bp">...</span>
</code></pre></div></div> <p>Inside of our <code class="language-plaintext highlighter-rouge">loss_fn</code> in <code class="language-plaintext highlighter-rouge">step</code> we unpack with an extra state.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">train</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="n">logits</span><span class="p">,</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">moe_stat</span><span class="p">)</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">pipe_step</span><span class="p">(</span>
      <span class="n">params</span><span class="p">,</span>
      <span class="n">x</span><span class="p">,</span>
      <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span>
      <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div> <p>We can then sum across the different device axes and add it to our loss.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">step</span><span class="p">(...):</span>
  <span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(...):</span>
    <span class="bp">...</span>
    <span class="n">loss_balance</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">moe_stat</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">psum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">"</span><span class="s">dp</span><span class="sh">"</span><span class="p">),</span> <span class="n">moe_stat</span><span class="p">)</span>
    <span class="n">moe_stat</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">psum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">"</span><span class="s">tp</span><span class="sh">"</span><span class="p">),</span> <span class="n">moe_stat</span><span class="p">)</span>
    <span class="n">moe_stat</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">psum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">"</span><span class="s">pp</span><span class="sh">"</span><span class="p">),</span> <span class="n">moe_stat</span><span class="p">)</span>

    <span class="n">loss_balance</span> <span class="o">=</span> <span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_config</span><span class="p">.</span><span class="n">n_experts</span> <span class="o">/</span> <span class="n">cfg</span><span class="p">.</span><span class="n">model_config</span><span class="p">.</span><span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="n">moe_stat</span><span class="p">[</span><span class="sh">"</span><span class="s">aux_loss</span><span class="sh">"</span><span class="p">].</span><span class="nf">sum</span><span class="p">()</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_cross</span> <span class="o">+</span> <span class="n">cfg</span><span class="p">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">loss_balance</span>

    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
      <span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">loss_cross</span><span class="sh">"</span><span class="p">:</span> <span class="n">loss_cross</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">loss_balance</span><span class="sh">"</span><span class="p">:</span> <span class="n">loss_balance</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">load_expert</span><span class="sh">"</span><span class="p">:</span> <span class="n">moe_stat</span><span class="p">[</span><span class="sh">"</span><span class="s">tokens_per_expert</span><span class="sh">"</span><span class="p">]</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">wandb</code> logging can be updated.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">current_step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">init_step</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">):</span>
  <span class="bp">...</span>
  <span class="k">if</span> <span class="n">use_wandb</span><span class="p">:</span>
    <span class="n">wandb_log</span> <span class="o">=</span> <span class="p">{</span>
      <span class="sh">"</span><span class="s">step</span><span class="sh">"</span><span class="p">:</span> <span class="n">current_step</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">loss/train_loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">],</span>
      <span class="sh">"</span><span class="s">loss/train_cross_entropy_loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss_cross</span><span class="sh">"</span><span class="p">],</span>
      <span class="sh">"</span><span class="s">lr</span><span class="sh">"</span><span class="p">:</span> <span class="n">opt_state</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">hyperparams</span><span class="p">[</span><span class="sh">"</span><span class="s">learning_rate</span><span class="sh">"</span><span class="p">],</span>
    <span class="p">}</span>
    <span class="n">wandb_log</span><span class="p">[</span><span class="sh">"</span><span class="s">loss/load_loss</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss_balance</span><span class="sh">"</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_config</span><span class="p">.</span><span class="n">n_experts</span><span class="p">):</span>
      <span class="n">wandb_log</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">load/head_</span><span class="si">{</span><span class="n">h</span><span class="si">}</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_get</span><span class="p">(</span><span class="n">metrics</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">load_expert</span><span class="sh">"</span><span class="p">])[</span><span class="n">h</span><span class="p">]</span>

  <span class="k">if</span> <span class="n">current_step</span> <span class="o">%</span> <span class="n">cfg</span><span class="p">.</span><span class="n">checkpoint_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="bp">...</span>

    <span class="k">if</span> <span class="n">use_wandb</span><span class="p">:</span>
      <span class="n">wandb_log</span><span class="p">[</span><span class="sh">"</span><span class="s">loss/val_loss</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">val_metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">]</span>
      <span class="n">wandb_log</span><span class="p">[</span><span class="sh">"</span><span class="s">loss/val_cross_entropy_loss</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">val_metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss_cross</span><span class="sh">"</span><span class="p">]</span>
      <span class="n">wandb_log</span><span class="p">[</span><span class="sh">"</span><span class="s">loss/val_load_loss</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">val_metrics</span><span class="p">[</span><span class="sh">"</span><span class="s">loss_balance</span><span class="sh">"</span><span class="p">]</span>
      <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">cfg</span><span class="p">.</span><span class="n">model_config</span><span class="p">.</span><span class="n">n_experts</span><span class="p">):</span>
        <span class="n">wandb_log</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">load/head_</span><span class="si">{</span><span class="n">h</span><span class="si">}</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_get</span><span class="p">(</span><span class="n">val_metrics</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">load_expert</span><span class="sh">"</span><span class="p">])[</span><span class="n">h</span><span class="p">]</span>
      <span class="bp">...</span>
</code></pre></div></div> <p>Now, we move to launch the final training run across 32 TPU-v4s.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/main.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'jindal013/jaxformer-website-v2',
        'data-repo-id': 'R_kgDOPoEVEA',
        'data-category': 'General',
        'data-category-id': 'DIC_kwDOPoEVEM4Cu2n4',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-loading': '1',
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>