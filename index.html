<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Jaxformer: Scaling Modern Transformers </title> <meta name="author" content=" "> <meta name="description" content="This is a zero-to-one guide on scaling modern transformers with n-dimensional parallelism. Transformers have driven much of the deep learning revolution, yet no practical guide reflects SOTA architectures and the complexities of large-scale language modelling. While excellent resources such as DeepMind’s 'How to Scale Your Model' and HuggingFace’s 'Ultra Scale Playbook' exist, a gap remains between theory and end-to-end implementation. We aim to bridge that gap by showing you how to scale a model from scratch (in Jax, with code) to current standards."> <meta name="keywords" content="jscaling, jax, llms, transformers, tpus, google, cloud, parallelism, distributed"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/jaxformer-icon.png?7001ddef15419e25335b33b49c6ce725"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jaxformer.com/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> <script async src="https://www.googletagmanager.com/gtag/js?id=G-G878LK8JDJ"></script> <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-G878LK8JDJ');
</script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Jaxformer",
            "description": "This is a zero-to-one guide on scaling modern transformers with n-dimensional parallelism. Transformers have driven much of the deep learning revolution, yet no practical guide reflects SOTA architectures and the complexities of large-scale language modelling. While excellent resources such as DeepMind’s 'How to Scale Your Model' and HuggingFace’s 'Ultra Scale Playbook' exist, a gap remains between theory and end-to-end implementation. We aim to bridge that gap by showing you how to scale a model from scratch (in Jax, with code) to current standards.",
            "published": "September 05, 2025",
            "authors": [
              
              {
                "author": "Aditya Makkar",
                "authorURL": "https://x.com/AdityaMakkar000"
              },
              
              {
                "author": "Divya Makkar",
                "authorURL": "https://x.com/_DivyaMakkar"
              },
              
              {
                "author": "Chinmay Jindal",
                "authorURL": "https://x.com/chinmayjindal_"
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <script>
    function goToTop() {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    }

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
      // Get the button:
      let mybutton = document.getElementById("top-button");

      if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
  }
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="d-none d-sm-inline">Jaxformer: Scaling Modern Transformers</span> <span class="d-inline d-sm-none"> Jaxformer </span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="left-button section-button"><a href=""><svg viewbox="-78.5 0 512 512"><path d="M257 64L291 98 128 262 291 426 257 460 61 262 257 64Z"></path></svg></a></div> <div class="right-button section-button"><a href="../tokenization"><svg viewbox="-78.5 0 512 512"><path d="M98 460L64 426 227 262 64 98 98 64 294 262 98 460Z"></path></svg></a></div> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item nav-hidden"><a class="nav-link" onclick="goToTop()" id="top-button" style="display: none;">Back to Top</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item nav-hidden"><a class="nav-link" href="">Previous Part</a></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../tokenization">Next Part</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Sections </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/">Part 0. Introduction</a> <a class="dropdown-item " href="/tokenization/">Part 1. Tokenization</a> <a class="dropdown-item " href="/base_model/">Part 2. Base Model</a> <a class="dropdown-item " href="/sharded/">Part 3. Sharded Model</a> <a class="dropdown-item " href="/dataset/">Part 4. Dataset &amp; Config</a> <a class="dropdown-item " href="/distributed_training/">Part 5. Distributed Training</a> <a class="dropdown-item " href="/moe/">Part 6. Mixture of Experts</a> <a class="dropdown-item " href="/training/">Part 7. Training Results</a> <a class="dropdown-item " href="/conclusion/">Part 8. Conclusion</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Jaxformer</h1> <p><span class="subtitle">Scaling Modern Transformers</span> <span class="subtitle-links">(Part 0: Intro | <a href="../tokenization">Part 1: Tokenization</a>)</span></p> <p>This is a zero-to-one guide on scaling modern transformers with n-dimensional parallelism. Transformers have driven much of the deep learning revolution, yet no practical guide reflects SOTA architectures and the complexities of large-scale language modelling. While excellent resources such as DeepMind’s 'How to Scale Your Model' and HuggingFace’s 'Ultra Scale Playbook' exist, a gap remains between theory and end-to-end implementation. We aim to bridge that gap by showing you how to scale a model from scratch (in Jax, with code) to current standards.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#code-contact">Code &amp; Contact</a> </div> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#prerequisites">Prerequisites</a> </div> <div> <a href="#goals">Goals</a> </div> <div> <a href="#overview">Overview</a> </div> </nav> </d-contents> <p><img id="banner" class="img-fluid" alt="Banner" src="/assets/img/banner-light.png" data-light-src="/assets/img/banner.png" data-dark-src="/assets/img/banner-light.png"></p> <script>
  document.addEventListener('DOMContentLoaded', () => {
    const btn = document.getElementById('light-toggle');
    const banner = document.getElementById('banner');

    function swapBanner() {
      const isDark = document.documentElement.getAttribute('data-theme') === 'dark';
      banner.src = isDark ? banner.dataset.darkSrc : banner.dataset.lightSrc;
    }

    swapBanner();

    btn.addEventListener('click', () => requestAnimationFrame(swapBanner));
  });
</script> <h2 id="code--contact">Code &amp; Contact</h2> <p>Find the complete code for this guide on our <a href="https://github.com/divyamakkar0/Jaxformer" rel="external nofollow noopener" target="_blank">GitHub repository</a>.<br> More information about the authors can be found on the <a href="https://jaxformer.com/conclusion">Conclusions page</a>.</p> <h2 id="introduction">Introduction</h2> <p>Modern transformers are at the heart of today’s deep learning systems, but taking them from a single-GPU prototype to a multi-node cluster is not straightforward. Scaling efficiently requires understanding how data moves through the hardware, how models can be split across devices, and how training infrastructure ties everything together.</p> <p>This guide is a practical, code-first walkthrough of scaling modern transformers in JAX. Our goal is to bridge the gap between high-level scaling theory and hands-on implementation. By the end, you should feel comfortable building a SOTA transformer model that runs on TPUs/GPUs, sharding it across devices, and training it at scale with techniques used in state-of-the-art systems.</p> <h2 id="prerequisites">Prerequisites</h2> <p>Prior to reading this guide, we assume you are famiilar with the following topics and resources (or equivalent material):</p> <ul> <li>Basic Transformer implementations</li> <li>Familiarity with Distributed Training ideas</li> <li>JAX basics: we recommend that you start reading through their <a href="https://docs.jax.dev/en/latest/" rel="external nofollow noopener" target="_blank">docs</a> </li> <li>Andrej Karpathy’s <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" rel="external nofollow noopener" target="_blank">Zero-to-Hero Neural Network</a> series</li> </ul> <h2 id="goals">Goals</h2> <p>By the end of this guide, you should be able to:</p> <ul> <li>Understand how to tokenize and stream large datasets efficiently for training.</li> <li>Estimate the compute, memory, and communication costs of running a transformer model.</li> <li>Select and combine parallelism schemes (data, tensor, pipeline, FSDP, MoE) for a given hardware setup.</li> <li>Confidently configure and launch distributed training runs on multi-host TPU or GPU clusters.</li> <li>Recognize bottlenecks that prevent strong scaling and know how to address them.</li> </ul> <p>This is v1.0. We aim to update the guide sporadically as we implement more complex ideas and architectures in the future.</p> <h2 id="overview">Overview</h2> <p>Here’s how the guide is structured:</p> <ul> <li> <strong><a href="tokenization">Part 1: Tokenization at Scale</a></strong> — how to preprocess massive datasets, shard them, and checkpoint safely for distributed training.</li> <li> <strong><a href="base_model">Part 2: Base Model</a></strong> — building a transformer in JAX with modules like RMSNorm, RoPE, and Multi-latent Attention.</li> <li> <strong><a href="sharded">Part 3: Sharded Model</a></strong> — introducing parallelism strategies (data, tensor, pipeline, FSDP) and applying them to transformer layers.</li> <li> <strong><a href="distributed_training">Part 4: Distributed Training</a></strong> — how to set up TPU/GPU clusters, manage checkpoints, and synchronize training loops.</li> <li> <strong><a href="dataset_class">Part 5: Dataset &amp; Configs</a></strong> — structured configs for datasets, hyperparameters, and runtime options.</li> <li> <strong><a href="moe">Part 6: Mixture of Experts</a></strong> — implementing and training MoE layers, covering routing, stability, and efficiency challenges.</li> <li> <strong><a href="final_run">Part 7: Final Run</a></strong> — putting it all together: multi-host scripts, launching large runs across TPU pods, and analyzing results.</li> <li> <strong><a href="conclusion">Part 8: Conclusions</a></strong> — lessons learned, future directions like DualPipe and expert parallelism, and additional resources.</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/main.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'jindal013/jaxformer-website-v2',
        'data-repo-id': 'R_kgDOPoEVEA',
        'data-category': 'General',
        'data-category-id': 'DIC_kwDOPoEVEM4Cu2n4',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-loading': '1',
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>