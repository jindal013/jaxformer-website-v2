<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Final Run and Training Results | Jaxformer: Scaling Modern Transformers </title> <meta name="author" content=" "> <meta name="description" content="We now write the launch scripts and launch the final run, showcasing how to use multi-controller JAX to conduct large scale, multi-host training runs."> <meta name="keywords" content="jscaling, jax, llms, transformers, tpus, google, cloud, parallelism, distributed"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/jaxformer-icon.png?7001ddef15419e25335b33b49c6ce725"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jaxformer.com/training/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> <script async src="https://www.googletagmanager.com/gtag/js?id=G-G878LK8JDJ"></script> <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-G878LK8JDJ');
</script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Final Run and Training Results",
            "description": "We now write the launch scripts and launch the final run, showcasing how to use multi-controller JAX to conduct large scale, multi-host training runs.",
            "published": "September 05, 2025",
            "authors": [
              
              {
                "author": "Aditya Makkar",
                "authorURL": "https://x.com/AdityaMakkar000"
              },
              
              {
                "author": "Divya Makkar",
                "authorURL": "https://x.com/_DivyaMakkar"
              },
              
              {
                "author": "Chinmay Jindal",
                "authorURL": "https://x.com/chinmayjindal_"
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <script>
    function goToTop() {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    }

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
      // Get the button:
      let mybutton = document.getElementById("top-button");

      if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
  }
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Jaxformer: Scaling Modern Transformers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="left-button section-button"><a href="../moe"><svg viewbox="-78.5 0 512 512"><path d="M257 64L291 98 128 262 291 426 257 460 61 262 257 64Z"></path></svg></a></div> <div class="right-button section-button"><a href="../conclusion"><svg viewbox="-78.5 0 512 512"><path d="M98 460L64 426 227 262 64 98 98 64 294 262 98 460Z"></path></svg></a></div> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item nav-hidden"><a class="nav-link" onclick="goToTop()" id="top-button" style="display: none;">Back to Top</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../moe">Previous Part</a></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../conclusion">Next Part</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Sections </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/">Part 0. Introduction</a> <a class="dropdown-item " href="/tokenization/">Part 1. Tokenization</a> <a class="dropdown-item " href="/base_model/">Part 2. Base Model</a> <a class="dropdown-item " href="/sharded/">Part 3. Sharded Model</a> <a class="dropdown-item " href="/dataset/">Part 4. Dataset &amp; Config</a> <a class="dropdown-item " href="/distributed_training/">Part 5. Distributed Training</a> <a class="dropdown-item " href="/moe/">Part 6. Mixture of Experts</a> <a class="dropdown-item " href="/training/">Part 7. Training Results</a> <a class="dropdown-item " href="/conclusion/">Part 8. Conclusion</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Final Run and Training Results</h1> <p>Part 7 of <a href="">Jaxformer</a> (<a href="../moe">Part 6: Mixture of Experts</a> | <a href="../conclusion">Part 8: Conclusion</a>)</p> <p>We now write the launch scripts and launch the final run, showcasing how to use multi-controller JAX to conduct large scale, multi-host training runs.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#final-run-scripts">Final Run Scripts</a> </div> <div> <a href="#training-results">Training Results</a> </div> </nav> </d-contents> <h2 id="final-run-scripts">Final Run Scripts</h2> <p>There are two main scripts that are significant for launching a training run. The first is found in the <code class="language-plaintext highlighter-rouge">launcher.sh</code> script which contains the IP addresses for all the TPUs as well as a command that launches a training run on each TPU. The command <code class="language-plaintext highlighter-rouge">printf "%s\n" "${IPS[@]}" | xargs -n 1 -P 0 -I {} bash run.sh {}</code> does the following:</p> <ul> <li>`printf “%s\n” “${IPS[@]}” prints each address in the IPS variable on a seperate line</li> <li> <code class="language-plaintext highlighter-rouge">| xargs</code> takes the argument from the ip and runs the command on all distributed devices at once</li> <li> <code class="language-plaintext highlighter-rouge">-n 1</code> runs the command once per input item (each IP gets its own <code class="language-plaintext highlighter-rouge">bash run.sh {}</code> call)</li> <li> <code class="language-plaintext highlighter-rouge">-P 0</code> runs as many process in parallel where each IP will be processed on a distinct device</li> <li> <code class="language-plaintext highlighter-rouge">-I {}</code> placeholder for the IP argument</li> <li> <code class="language-plaintext highlighter-rouge">bash run.sh {}</code> calls the <code class="language-plaintext highlighter-rouge">run.sh</code> script passing the IP as an argument</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#!/bin/bash
</span><span class="n">source</span> <span class="p">.</span><span class="n">env</span>

<span class="n">IPS</span><span class="o">=</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">35.186.25.28</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">35.186.39.76</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">107.167.173.215</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">35.186.132.44</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">35.186.24.134</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">35.186.58.69</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">35.186.134.160</span><span class="sh">"</span>
    <span class="sh">"</span><span class="s">35.186.107.62</span><span class="sh">"</span>
<span class="p">)</span>

<span class="n">printf</span> <span class="sh">"</span><span class="s">%s</span><span class="se">\n</span><span class="sh">"</span> <span class="sh">"</span><span class="s">${IPS[@]}</span><span class="sh">"</span> <span class="o">|</span> <span class="n">xargs</span> <span class="o">-</span><span class="n">n</span> <span class="mi">1</span> <span class="o">-</span><span class="n">P</span> <span class="mi">0</span> <span class="o">-</span><span class="n">I</span> <span class="p">{}</span> <span class="n">bash</span> <span class="n">run</span><span class="p">.</span><span class="n">sh</span> <span class="p">{}</span>
</code></pre></div></div> <p>Essentially the purpose of this script is to execute <code class="language-plaintext highlighter-rouge">run.sh</code> with each individual IP as an argument to the script, on parallel devices. The purpose of <code class="language-plaintext highlighter-rouge">run.sh</code> is to:</p> <ol> <li>SSH into the IP given as an argument using the command: <code class="language-plaintext highlighter-rouge">ssh $USER@$IP</code>.</li> <li>Kills any current tmux sessions <code class="language-plaintext highlighter-rouge">tmux kill-session -t $SESSION</code>, by telling tmux to kill a session with the name matching the <code class="language-plaintext highlighter-rouge">$SESSION</code> variable.</li> <li>A new tmux session is started <code class="language-plaintext highlighter-rouge">tmux new-session -d -s $SESSION</code> with flag <code class="language-plaintext highlighter-rouge">-s $SESSION</code>, naming the session with the variable name and the flag <code class="language-plaintext highlighter-rouge">-d</code> creating the session in the background without attaching immediately.</li> <li>Moving to the correct directory and resetting the samples in the folder. This is done with the <code class="language-plaintext highlighter-rouge">tmux send-keys -t $SESSION:0 'cd ~/Jaxformer &amp;&amp; rm -rf samples &amp;&amp; mkdir samples' C-m</code> command. <code class="language-plaintext highlighter-rouge">tmux send-keys</code> tells tmux the keystrokes to execute in the<code class="language-plaintext highlighter-rouge">-t $SESSION:0</code> in the target session in the first window specified by <code class="language-plaintext highlighter-rouge">:0</code>. Following that is the actual command to be typed in the session which essentially moves to the Jaxformer directory, removed the folder with samples and then recreates it, essentially resetting the samples. Then <code class="language-plaintext highlighter-rouge">C-m</code> is executed, which enters the command that was previously typed into the tmux session to run.</li> <li>General setup. The same command as the one above is repeated, except with different internal commands to be executed in the tmux sessions. Specifically, inside the Jaxformer directory, the file is refetched from the git origin and reset to get the latest version. Then, the<code class="language-plaintext highlighter-rouge">setupTPU.sh</code> script is ran to install the correct dependencies on the TPU, and finally the model is ran as seen in the <code class="language-plaintext highlighter-rouge">$command</code> variable.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#!/bin/bash
</span>
<span class="n">IP</span><span class="o">=</span><span class="err">$</span><span class="mi">1</span>
<span class="n">SESSION</span><span class="o">=</span><span class="sh">"</span><span class="s">trainingRun</span><span class="sh">"</span>
<span class="n">command</span><span class="o">=</span><span class="sh">"</span><span class="s">python test.py --checkpoint_steps=75 --n_device_axis 8 2 2 --name moe1B --train_batch_size 32 --use_cache --wandb --eval_steps 10</span><span class="sh">"</span>

<span class="n">echo</span> <span class="sh">"</span><span class="s">Running on $IP</span><span class="sh">"</span>

<span class="n">ssh</span> <span class="err">$</span><span class="n">USER</span><span class="o">@</span><span class="err">$</span><span class="n">IP</span> <span class="sh">"</span><span class="s">

    tmux kill-session -t $SESSION
    tmux new-session -d -s $SESSION

    tmux send-keys -t $SESSION:0 </span><span class="sh">'</span><span class="s">cd ~/Jaxformer &amp;&amp; rm -rf samples &amp;&amp; mkdir samples</span><span class="sh">'</span><span class="s"> C-m
    tmux send-keys -t $SESSION:0 </span><span class="sh">'</span><span class="s">git fetch origin &amp;&amp; git reset --hard origin/main</span><span class="sh">'</span><span class="s"> C-m
    tmux send-keys -t $SESSION:0 </span><span class="sh">'</span><span class="s">bash setupTpu.sh</span><span class="sh">'</span><span class="s"> C-m
    tmux send-keys -t $SESSION:0 </span><span class="sh">'</span><span class="s">$command</span><span class="sh">'</span><span class="s"> C-m
</span><span class="sh">"</span>
<span class="n">echo</span> <span class="sh">"</span><span class="s">done commands</span><span class="sh">"</span>
</code></pre></div></div> <p>For demonstration of the final training, we use the command below which was run across a cluster of 32 TPU-v4 devices across 8 controllers. (8 IPs for ssh).</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python test.py <span class="nt">--checkpoint_steps</span><span class="o">=</span>75 <span class="nt">--n_device_axis</span> 8 2 2 <span class="nt">--name</span> moe1B <span class="nt">--train_batch_size</span> 32 <span class="nt">--use_cache</span> <span class="nt">--wandb</span> <span class="nt">--eval_steps</span> 10<span class="s2">"
</span></code></pre></div></div> <p>We are using 8 devices for FSDP, 2 for pipeline and 2 for tensor. Here is the final config.</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"model_config"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"model_dimension"</span><span class="p">:</span><span class="w"> </span><span class="mi">768</span><span class="p">,</span><span class="w">
    </span><span class="nl">"vocab_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">100277</span><span class="p">,</span><span class="w">
    </span><span class="nl">"n_head"</span><span class="p">:</span><span class="w"> </span><span class="mi">12</span><span class="p">,</span><span class="w">
    </span><span class="nl">"blocks"</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w">
    </span><span class="nl">"layers_per_block"</span><span class="p">:</span><span class="w"> </span><span class="mi">6</span><span class="p">,</span><span class="w">
    </span><span class="nl">"T"</span><span class="p">:</span><span class="w"> </span><span class="mi">1024</span><span class="p">,</span><span class="w">
    </span><span class="nl">"latent_dim"</span><span class="p">:</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w">
    </span><span class="nl">"dhR"</span><span class="p">:</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w">
    </span><span class="nl">"dropout_rate"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.2</span><span class="p">,</span><span class="w">
    </span><span class="nl">"model_dtype"</span><span class="p">:</span><span class="w"> </span><span class="s2">"bfloat16"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"k"</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w">
    </span><span class="nl">"n_experts"</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w">
    </span><span class="nl">"n_shared"</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w">
    </span><span class="nl">"capacity_factor"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.5</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"data_config"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"bucket_name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"350bt_gpt4"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"process_path"</span><span class="p">:</span><span class="w"> </span><span class="s2">"./bucket_downloads/processShard"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"train_folder_name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"train"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"val_folder_name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"val"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"T"</span><span class="p">:</span><span class="w"> </span><span class="mi">1024</span><span class="p">,</span><span class="w">
    </span><span class="nl">"train_batch_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w">
    </span><span class="nl">"val_batch_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w">
    </span><span class="nl">"micro_batch_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"lr"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"max_lr"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0006</span><span class="p">,</span><span class="w">
    </span><span class="nl">"min_lr"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
    </span><span class="nl">"end_lr"</span><span class="p">:</span><span class="w"> </span><span class="mi">6e-5</span><span class="p">,</span><span class="w">
    </span><span class="nl">"warmup_steps"</span><span class="p">:</span><span class="w"> </span><span class="mi">5000</span><span class="p">,</span><span class="w">
    </span><span class="nl">"end_steps"</span><span class="p">:</span><span class="w"> </span><span class="mi">75000</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"device_config"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"n_device_axis"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">]</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"inference_config"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"prompt"</span><span class="p">:</span><span class="w"> </span><span class="s2">"hello world"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"batch_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
    </span><span class="nl">"top_k"</span><span class="p">:</span><span class="w"> </span><span class="mi">10000</span><span class="p">,</span><span class="w">
    </span><span class="nl">"temperature"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w">
    </span><span class="nl">"n_devices"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
    </span><span class="nl">"max_tokens"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w">
    </span><span class="nl">"use_cache"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"output_dir"</span><span class="p">:</span><span class="w"> </span><span class="s2">"gs://results_jaxformer/"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"training_steps"</span><span class="p">:</span><span class="w"> </span><span class="mi">100000</span><span class="p">,</span><span class="w">
  </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"moe1B"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"grad_step"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
  </span><span class="nl">"alpha"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0001</span><span class="p">,</span><span class="w">
  </span><span class="nl">"checkpoint_steps"</span><span class="p">:</span><span class="w"> </span><span class="mi">75</span><span class="p">,</span><span class="w">
  </span><span class="nl">"eval_steps"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w">
  </span><span class="nl">"seed"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
  </span><span class="nl">"wandb"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span><span class="w">
  </span><span class="nl">"grad_clip_norm"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p>In total this config yields 949,248,384 parameters with 343,321,728 active parameters.</p> <h2 id="training-results">Training Results</h2> <p>We only train until we hit 3.28 validation loss (inspired by nanoGPT speedrun) due to TRC compute limits. This was achieved after (26,100 steps) and in total $\sim 6.5$ billion tokens; however, with better compute and more time this could continue decreasing.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/final_run/1-480.webp 480w,/assets/img/final_run/1-800.webp 800w,/assets/img/final_run/1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/final_run/1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Notably we avoid expert collapse as seen by the tokens per head and the auxiliary loss curves.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/final_run/2-480.webp 480w,/assets/img/final_run/2-800.webp 800w,/assets/img/final_run/2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/final_run/2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/final_run/3-480.webp 480w,/assets/img/final_run/3-800.webp 800w,/assets/img/final_run/3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/final_run/3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/main.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'jindal013/jaxformer-website-v2',
        'data-repo-id': 'R_kgDOPoEVEA',
        'data-category': 'General',
        'data-category-id': 'DIC_kwDOPoEVEM4Cu2n4',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-loading': '1',
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>